[{"content":"Background To obtain more relevant metrics and a comprehensive understanding of traces when external API requests access our systems, We opted to enable tracing for the network mesh components situated above the Kubernetes service. This specifically covers the entire request flow from its initiation: the ingress gateway receives the request and routes it according to defined rules to a particular K8s service; then, the Istio sidecar proxy containers, running alongside our application containers in the same pod, receive and proxy the request to main containers, where our Node.js server process ultimately handle it and generate the appropriate response. Before enabling tracing for istio mesh, tracing for a HTTP request began when the request reached our application container at the pod level, with HTTP auto-instrumentation serving as the trace root.\nFlow of an external request:\nsequenceDiagram autonumber participant Client participant Ingress as Istio Ingress Gateway participant Sidecar as Istio Sidecar (Envoy) participant App as Node.js App Client -\u0026gt;\u0026gt; Ingress: HTTP request Note right of Ingress: Trace root Ingress -\u0026gt;\u0026gt; Sidecar: Route to service (mTLS) Note right of Sidecar: Envoy inbound span Sidecar -\u0026gt;\u0026gt; App: Forward request Note right of App: HTTP server span App --\u0026gt;\u0026gt; Sidecar: Response Sidecar --\u0026gt;\u0026gt; Ingress: Response Ingress --\u0026gt;\u0026gt; Client: Response Following this change, we gained additional spans, with the trace root now identified as the Istio ingress gateway, followed by the sidecar containers of the K8s service pod to which the request was routed, and finally the main Node.js container’s HTTP span. The primary advantage of this approach is significantly improved visibility into request failures occurring before the application layer. Earlier, when issues arose within the network mesh, such as routing failures, mTLS handshakes, or upstream connectivity problems, we would receive alerts from the frontend indicating 5XX errors on API calls, yet observe no corresponding errors in our backend HTTP metrics. This mismatch often caused confusion, as the requests never reached our application containers, and therefore application-level instrumentation could neither record the request nor emit 5XX responses. As a result, our backend alerting was effectively blind to these failure scenarios. This closes the observability gap by clearly identifying failures occurring before the application layer. Let’s look at one such issue that occurred at seemingly random intervals, making it difficult to diagnose, and how the additional observability data from the network mesh helped us identify the root cause.\n503 UC Error Upon configuring Grafana dashboards for several Istio metrics (sourced from here), we observed numerous 503 errors reported by the ingress gateway across many Kubernetes services, without any specific HTTP API endpoint being consistently implicated. With tracing in place, we searched Grafana Tempo (our datasource for traces) using the following TraceQL query:\n{span.http.status_code=\u0026#34;503\u0026#34; \u0026amp;\u0026amp; resource.service.name=\u0026#34;istio-ingressgateway.istio-system\u0026#34;} This allowed us to quickly identify traces corresponding to all such 5XX error cases. As shown in the trace above, the request fails at the Istio ingress gateway and sidecar proxy layers, and no span corresponding to the main application container (Node.js HTTP server) is present. This indicates that the request never reached the application pod itself. This also explains the alert mismatch observed earlier: since the failure occurred entirely within the network mesh, application-level HTTP instrumentation never recorded the request and therefore could not emit 5XX metrics or trigger backend alerts. With mesh-level tracing enabled, these previously invisible failures are now captured and can be correlated directly with frontend-reported errors. Having identified the pod where the request failed, we can now query Grafana Loki, our log data source, to check whether the istio-proxy sidecar container recorded any logs. Since the sidecar represents the terminal point for this request within the system, its logs are the most relevant for understanding the failure. Indeed, the following log entry confirms the issue:\n[2026-01-27T06:08:22.422Z] \u0026#34;GET ********* HTTP/1.1\u0026#34; 503 UC upstream_reset_before_response_started{connection_termination} - \u0026#34;-\u0026#34; 0 88 50 - \u0026#34;2401:4900:bce9:6781:a4ab:23ff:fec4:3ef4,172.69.152.151\u0026#34; \u0026#34;Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Mobile Safari/537.36\u0026#34; \u0026#34;c1e87fc8-e7d0-9f67-8bfe-42be707c9b25\u0026#34; \u0026#34;api.xyz.com\u0026#34; \u0026#34;10.88.4.251:3000\u0026#34; inbound|3000|| 127.0.0.6:33183 10.88.4.251:3000 XX.XX.XX.XX:0 - default The key part of the log entry is 503 UC upstream_reset_before_response_started{connection_termination}. This indicates that when the sidecar proxy forwarded the request to the upstream service (the main application container), the upstream closed or reset the connection before any response could be sent. Envoy interprets this as an upstream connection termination, and consequently returns a 503 error to the downstream client. After investigating the issue and discussing it extensively, including exploring insights with LLMs, we arrived at an interesting finding that ultimately pointed to the root cause: the connection was being terminated by the upstream application container. Before diving into that, it is important to briefly outline the layered nature of a network request in our setup. From the very start, a request passes through multiple entities: The cloud load balancer, the ingress gateway inside the Kubernetes cluster, the Istio sidecar proxy, and finally the main application container. Each of these components maintains its own set of connections and, in most cases, its own connection pools. When a new request arrives, it may reuse an existing connection or establish a new one for forwarding the request upstream. As a result, a single logical request is often carried across a chain of independent network connections before the packet reaches its final destination. In an Istio service mesh, communication between proxies is secured using mTLS and typically happens over HTTP/2, while traffic forwarded from the sidecar to the main application container is usually downgraded to HTTP/1.1. Each end of these connections enforces its own keep-alive and idle timeout policies, and once a connection remains idle beyond the configured threshold, it may be closed by that endpoint. The exact network signal used to close the connection, such as a graceful close or a reset, depends on the protocol in use. This layered connection model and protocol behaviour form an important context for understanding what we observe next.\nsequenceDiagram autonumber participant Envoy as Istio Sidecar (Envoy) participant Node as Node.js App participant OS as Kernel TCP Stack Note over Envoy,Node: HTTP/1.1 keep-alive connection (idle) Node -\u0026gt;\u0026gt; OS: Keep-alive timeout expires OS -\u0026gt;\u0026gt; Envoy: TCP FIN (graceful close) Note over Envoy: Connection still appears reusable Envoy -\u0026gt;\u0026gt; OS: Send new HTTP request on same socket OS -\u0026gt;\u0026gt; Envoy: TCP RST (socket already closed) Envoy -\u0026gt;\u0026gt; Envoy: Mark upstream reset Note right of Envoy: 503 UC\\nupstream_reset_before_response_started By default, a Node.js HTTP server enforces a keep-alive timeout of around 5 seconds. If the connection between the Istio sidecar and the Node.js application container remains idle beyond this threshold, the Node.js process initiates a graceful shutdown of the HTTP/1.1 connection by sending a TCP FIN. However, a race condition can occur if, around the same time, the sidecar forwards a new downstream request and attempts to reuse this connection. In such cases, the request is written to a socket that has already been closed by the application. The operating system’s TCP stack then responds with a TCP RST, indicating that the connection is no longer valid. From Envoy’s perspective, this manifests as an upstream connection reset before any response headers are received. As a result, the sidecar treats this as a network-level failure and returns a 503 UC (upstream_reset_before_response_started) error to the downstream client.\nThis failure occurs in the narrow window where the upstream has already initiated a graceful connection close, but the sidecar has not yet observed the FIN and therefore attempts to reuse the same connection.\nThe fix for this issue is relatively straightforward: we need to ensure that idle connections are closed by downstream components before the Node.js process in the main container initiates the close. This prevents Envoy from reusing a connection that the application has already decided to terminate. By default, the Istio sidecar proxy and the ingress gateway maintain upstream connections with idle timeouts of around one hour in most configurations. Reducing these timeouts to values lower than Node.js’s default 5 seconds is not a viable option, as it would lead to excessive connection churn, increased TLS handshake overhead, and degraded performance. On the other hand, increasing the Node.js HTTP server’s keep-alive timeout to match Envoy’s one-hour timeout is also undesirable, since it would cause the application to retain a large number of idle connections, potentially resulting in unnecessary memory usage and file descriptor pressure. In our setup, there is an additional network component upstream of the Istio ingress gateway, A cloud provider load balancer, with an idle connection timeout of 30 seconds. This effectively places a hard upper bound on how long idle downstream connections can exist, as they will be terminated by the load balancer regardless of Envoy or application-level settings. Given this constraint, the optimal solution is to configure the Node.js HTTP server’s keep-alive timeout to be slightly higher than the load balancer’s idle timeout (for example, just above 30 seconds). This ensures that idle connections are consistently closed by downstream entities first, eliminating the race condition where Envoy might attempt to reuse a connection that the application has already closed.\n1const server = http.createServer(app).listen(port); 2server.keepAliveTimeout = 45 * 1000; // 45 seconds HTTP/2 If you read the issue comment linked below, which discusses the same problem, you will notice a recurring mention of HTTP/2, which is used internally by the Istio service mesh. This is also evident from the trace shown earlier, where the communication between the ingress gateway and the sidecar proxy uses HTTP/2, while the failed connection to the main application container occurs over HTTP/1.1.\nhttps://github.com/istio/istio/issues/55138#issuecomment-2666855044\nNo HTTP server runtime that I have ever seen has such a ridiculously high HTTP persistent connection keep-alive timeout of one hour or longer.\nNote that this high keep-alive timeout in Envoy/Istio is only an issue for the inbound cluster of the sidecar of a particular server workload. That 1h keep-alive timeout default is also used for the HTTP Connection Manager (HCM) of the downstream listener and the upstream cluster for the HTTP/2 mTLS istio↔istio connections, where high client-side HTTP keep-alive timeouts are not an issue. This is because HTTP/2—used for Istio↔Istio communication—provides additional mechanisms (such as the GOAWAY frame) to gracefully close connections without abruptly terminating the underlying TCP socket.\nThe issue lies specifically with HTTP/1.1 connections from the Istio/Envoy sidecar’s inbound cluster to the application server running inside the main container.\nIn short, high keep-alive timeouts are far less problematic for HTTP/2 connections because connection lifecycle is managed at the stream level rather than solely at the TCP socket level. HTTP/2 allows a server to signal intent to stop accepting new streams using a GOAWAY frame, while still keeping the underlying connection open long enough for in-flight requests to complete. This avoids the abrupt connection termination patterns commonly seen with HTTP/1.1, where closing an idle connection often translates directly into a TCP FIN and can lead to race conditions during connection reuse.\nAs a result, the one-hour keep-alive default in Istio is safe for HTTP/2-based Istio-to-Istio communication, but becomes problematic only when applied to HTTP/1.1 connections between the sidecar proxy and the application container.\nA simplistic sequence diagram of HTTP/2 connection\u0026rsquo;s graceful shutdown.\nsequenceDiagram autonumber participant Client as Envoy Client participant Server as Envoy Server %% Connection setup Client -\u0026gt;\u0026gt; Server: TCP handshake Client -\u0026gt;\u0026gt; Server: TLS handshake (mTLS) Client -\u0026gt;\u0026gt; Server: HTTP/2 connection preface Client -\u0026gt;\u0026gt; Server: SETTINGS frame Server -\u0026gt;\u0026gt; Client: SETTINGS frame (ACK) Note over Client,Server: Single TCP connection - Multiple concurrent streams %% Normal request/response Client -\u0026gt;\u0026gt; Server: HEADERS (Stream 1) Server -\u0026gt;\u0026gt; Client: HEADERS + DATA (Stream 1) %% Graceful shutdown Server -\u0026gt;\u0026gt; Client: GOAWAY (last_stream_id = N) Note right of Client: No new streams allowed - Existing streams may continue Client -\u0026gt;\u0026gt; Server: DATA (Streams ≤ N) Server -\u0026gt;\u0026gt; Client: DATA (Streams ≤ N) %% Final close Server -\u0026gt;\u0026gt; Client: TCP FIN Client -\u0026gt;\u0026gt; Server: TCP ACK References https://github.com/istio/istio/issues/55138#issuecomment-2666855044 ","permalink":"https://harshrai654.github.io/blogs/debugging-http-503-uc-errors-in-istio-service-mesh/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eTo obtain more relevant metrics and a comprehensive understanding of traces when external API requests access our systems, We opted to enable tracing for the network mesh components situated above the Kubernetes service. This specifically covers the entire request flow from its initiation: the ingress gateway receives the request and routes it according to defined rules to a particular K8s service; then, the Istio sidecar proxy containers, running alongside our application containers in the same pod, receive and proxy the request to main containers, where our Node.js server process ultimately handle it and generate the appropriate response.\nBefore enabling tracing for istio mesh, tracing for a HTTP request began when the request reached our application container at the pod level, with HTTP auto-instrumentation serving as the trace root.\u003c/p\u003e","title":"Debugging HTTP 503 UC Errors in Istio Service Mesh"},{"content":"In this blog I will try to explain basics of networking concepts and how these concepts are used to create simple to complex networking topologies to transfer data between physical machines and with the use of network namespaces on Linux we will try to simulate various scenarios as we discuss the theory behind them.\nNetwork Segments A network segment refers to a distinct part of a computer network that is isolated from the remainder of the network by a specific device, such as a repeater, hub, bridge, switch, or router. Within each segment, one or more computers or other hosts may reside. Depending on how these devices are connected, the network forms L1, L2 or L3 segments, Here the segment are mainly named on the basis of the layer of OSI networking model in which a segment mainly communicates.\nAn L1 segment is formed by a physical connection between networked devices, where each node on a single segment have a common physical layer When multiple L1 segments are connected by a shared switch (or bridge), an L2 network segment is formed. A L2 segment can also be formed recursively, i.e. merging multiple L2 segments via an upper layer switch to form a tree like topology.\nAt its fundamental level, each L2 (Layer 2) segment operates as an independent broadcast domain. This means that every device connected within that specific segment has the capability to send a message that will be received by all other devices on the same segment. This form of \u0026ldquo;one-to-all\u0026rdquo; communication, known as broadcasting, takes place at Layer 2 of the OSI Model (the Data Link Layer), where devices primarily use MAC addresses and Ethernet frames to communicate. When several individual L2 segments are interconnected or \u0026ldquo;merged\u0026rdquo;: For example, by a network switch, They effectively combine to form a single, larger L2 segment. The significant implication of this merging is the creation of a much bigger broadcast domain. Within this expanded domain, a broadcast message initiated by any device will now reach every other device across the entire consolidated segment, allowing for broader Layer 2 communication across the network.\nNote: VLAN can be used to split broadcast domains at the data link layer, Later on we will see how to use VLAN to partition a broadcast domain within a single L2 segment\nThe accompanying diagram illustrates the structure of a Layer 2 Ethernet frame. Communication between devices within a Layer 2 segment occurs through the generation of binary payloads conforming to this structure. Our goal is to develop custom Go code for writing raw Ethernet frames to a designated network interface. Trying Broadcast Domain With Linux network namespaces we can simulate a network topology as shown in the diagram above and try to broadcast Ethernet frames with destination MAC address of (ff:ff:ff:ff:ff:ff) and try to send frame to a specific node with its MAC address and see how the whole communication works, But before we jump into lets understand what network namespaces are.\nNetwork Namespaces Linux network namespaces offer a method to simulate a dedicated network environment for a Linux container. This is precisely why containers created by Docker are equipped with their own distinct network interfaces, IP stacks, routing tables, and firewall rules. One can conceptualize network namespaces as operating on a similar principle to C++ namespaces: they establish a scoped environment with its own set of \u0026ldquo;devices\u0026rdquo; that do not conflict with the host machine\u0026rsquo;s hardware, thereby creating a virtual device (a container) within the host system. We will be taking a slight detour to first understand how we can create a working network topology like above with network namespace on Linux.\nLet\u0026rsquo;s first create a network namespace ns1 with ip netns\n1laborant@ubuntu-01:~$ sudo ip netns add ns1 2laborant@ubuntu-01:~$ ip netns list 3ns1 The ip link command is used to display all devices available on the host machine. This command can also be run within a newly created network namespace, such as ns1, by utilizing nsenter. A file is mounted for each new network namespace at /run/netns/\u0026lt;namespace_name\u0026gt;, and this mounted file can be used with nsenter to make the specified network namespace active for a current shell session. As shown below, the ns1 namespace contains only a loop back device.\n1# ON HOST MACHINE 2laborant@ubuntu-01:~$ ip link list 31: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 4 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 52: bond0: \u0026lt;BROADCAST,MULTICAST,MASTER\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 6 link/ether 3a:4c:de:8c:05:55 brd ff:ff:ff:ff:ff:ff 73: dummy0: \u0026lt;BROADCAST,NOARP\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 8 link/ether 7a:7e:34:4b:fb:cd brd ff:ff:ff:ff:ff:ff 94: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 10 link/ether ba:58:08:f7:5c:14 brd ff:ff:ff:ff:ff:ff 11 12# ON NS1 namespace 13laborant@ubuntu-01:~$ ls -la /run/netns/ 14total 0 15drwxr-xr-x 2 root root 60 Jan 10 16:19 . 16drwxr-xr-x 16 root root 440 Jan 10 16:19 .. 17-r--r--r-- 1 root root 0 Jan 10 16:19 ns1 18 19laborant@ubuntu-01:~$ sudo nsenter --net=/run/netns/ns1 20root@ubuntu-01:laborant# ip link list 211: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 22 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 The initial step is to establish a L1 segment by creating a virtual Ethernet pair device. One interface of this pair will be positioned within the ns1 network namespace, while the other will reside on the host machine. A description of the veth device, available in its manual pages:\n1The veth devices are virtual Ethernet devices. They can act as 2tunnels between network namespaces to create a bridge to a 3physical network device in another namespace, but can also be used 4as standalone network devices. 5 6veth devices are always created in interconnected pairs. A pair 7can be created using the command: 8 9 # ip link add \u0026lt;p1-name\u0026gt; type veth peer name \u0026lt;p2-name\u0026gt; 10 11In the above, p1-name and p2-name are the names assigned to the 12two connected end points. 13 14Packets transmitted on one device in the pair are immediately 15received on the other device. When either device is down, the 16link state of the pair is down. Let\u0026rsquo;s create veth1 ⇿ ceth1 pair\n1laborant@ubuntu-01:~$ sudo ip link add veth1 type veth peer ceth1 2laborant@ubuntu-01:~$ sudo ip link list | grep \u0026#34;veth1\u0026#34; 36: veth1@ceth1: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 4 link/ether d6:d3:07:0b:a1:ce brd ff:ff:ff:ff:ff:ff Now we need to place one end of the pair inside the ns1 namespace and toggle the state of both the ends to UP from DOWN mode\n1laborant@ubuntu-01:~$ sudo ip link set ceth1 netns ns1 2laborant@ubuntu-01:~$ ip link ls | grep veth1 36: veth1@if5: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 4 link/ether d6:d3:07:0b:a1:ce brd ff:ff:ff:ff:ff:ff link-netns ns1 5# Here veth1 is in DOWN mode 6 7laborant@ubuntu-01:~$ sudo ip link set veth1 up 8laborant@ubuntu-01:~$ ip link ls | grep veth1 96: veth1@if5: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN mode DEFAULT group default qlen 1000 10 link/ether d6:d3:07:0b:a1:ce brd ff:ff:ff:ff:ff:ff link-netns ns1 11# Now it\u0026#39;s in LOWERLAYERDOWN since the other end is still DOWN 12 13 14laborant@ubuntu-01:~$ sudo ip netns exec ns1 ip link set ceth1 up 15laborant@ubuntu-01:~$ sudo ip netns exec ns1 ip link ls 161: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 17 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 185: ceth1@if6: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 19 link/ether d2:e0:87:f7:79:84 brd ff:ff:ff:ff:ff:ff link-netnsid 0 20 21laborant@ubuntu-01:~$ ip link ls | grep veth1 226: veth1@if5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 23 24# Now both ends are in UP mode I have used ip netns exec to run the commands inside ns1 namespace, which is an alternative to nsenter .\nWith the above setup now we have a L1 segment with two devices veth1 and ceth1 Now to have a L2 segment we need to repeat the above steps to create multiple such veth pairs and then connect the host end of the pairs to a device of type bridge.\nI have created two more namespaces ns2 and ns3 with veth2 ↔ ceth2 and veth3 ↔ ceth3 virtual ETH pairs respectively\n1laborant@ubuntu-01:~$ sudo ip netns add ns2 2laborant@ubuntu-01:~$ sudo ip netns add ns3 3laborant@ubuntu-01:~$ sudo ip link add veth2 type veth peer ceth2 4laborant@ubuntu-01:~$ sudo ip link add veth3 type veth peer ceth3 5laborant@ubuntu-01:~$ sudo ip link set ceth2 netns ns2 6laborant@ubuntu-01:~$ sudo ip link set ceth3 netns ns3 7laborant@ubuntu-01:~$ sudo ip link set veth2 up 8laborant@ubuntu-01:~$ sudo ip link set veth3 up 9laborant@ubuntu-01:~$ sudo ip netns exec ns2 ip link set ceth2 up 10laborant@ubuntu-01:~$ sudo ip netns exec ns3 ip link set ceth3 up 11laborant@ubuntu-01:~$ ip link list 121: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 13 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 142: bond0: \u0026lt;BROADCAST,MULTICAST,MASTER\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 15 link/ether 3a:4c:de:8c:05:55 brd ff:ff:ff:ff:ff:ff 163: dummy0: \u0026lt;BROADCAST,NOARP\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 17 link/ether 7a:7e:34:4b:fb:cd brd ff:ff:ff:ff:ff:ff 184: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 19 link/ether ba:58:08:f7:5c:14 brd ff:ff:ff:ff:ff:ff 206: veth1@if5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 21 link/ether d6:d3:07:0b:a1:ce brd ff:ff:ff:ff:ff:ff link-netns ns1 228: veth2@if7: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 23 link/ether 4a:f7:79:6e:5c:e6 brd ff:ff:ff:ff:ff:ff link-netns ns2 2410: veth3@if9: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 25 link/ether 2a:b9:c0:6d:ed:05 brd ff:ff:ff:ff:ff:ff link-netns ns3 26laborant@ubuntu-01:~$ sudo ip netns exec ns2 ip link list 271: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 28 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 297: ceth2@if8: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 30 link/ether ba:b6:2f:9a:96:71 brd ff:ff:ff:ff:ff:ff link-netnsid 0 31laborant@ubuntu-01:~$ sudo ip netns exec ns3 ip link list 321: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 33 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 349: ceth3@if10: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 35 link/ether c6:ab:66:25:73:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 Now we have all the veth pair in place, Let\u0026rsquo;s create a bridge device on host machine and connect all host end of newly created veth pairs to the bridge\n1laborant@ubuntu-01:~$ sudo ip link add br0 type bridge 2laborant@ubuntu-01:~$ sudo ip link set veth1 master br0 3laborant@ubuntu-01:~$ sudo ip link set veth2 master br0 4laborant@ubuntu-01:~$ sudo ip link set veth3 master br0 5laborant@ubuntu-01:~$ sudo ip link set br0 up 6laborant@ubuntu-01:~$ ip link list 71: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 8 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 92: bond0: \u0026lt;BROADCAST,MULTICAST,MASTER\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 10 link/ether 3a:4c:de:8c:05:55 brd ff:ff:ff:ff:ff:ff 113: dummy0: \u0026lt;BROADCAST,NOARP\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 12 link/ether 7a:7e:34:4b:fb:cd brd ff:ff:ff:ff:ff:ff 134: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 14 link/ether ba:58:08:f7:5c:14 brd ff:ff:ff:ff:ff:ff 156: veth1@if5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br0 state UP mode DEFAULT group default qlen 1000 16 link/ether d6:d3:07:0b:a1:ce brd ff:ff:ff:ff:ff:ff link-netns ns1 178: veth2@if7: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br0 state UP mode DEFAULT group default qlen 1000 18 link/ether 4a:f7:79:6e:5c:e6 brd ff:ff:ff:ff:ff:ff link-netns ns2 1910: veth3@if9: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br0 state UP mode DEFAULT group default qlen 1000 20 link/ether 2a:b9:c0:6d:ed:05 brd ff:ff:ff:ff:ff:ff link-netns ns3 2111: br0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 22 link/ether c6:e2:a8:ee:aa:be brd ff:ff:ff:ff:ff:ff Bridge br0 has been designated as the master for veth1, veth2, and veth3. The ip link list output verifies this, indicating br0 as the master for each interface, alongside their corresponding pair endpoints (ceth) located within their respective network namespaces. This configuration effectively establishes a Layer 2 segment, interconnecting three devices through the bridge. The subsequent step involves testing both the broadcast domain and individual device connectivity through the transmission of Ethernet frames.\nTesting the setup This Go utility constructs raw Ethernet frames by sequentially concatenating the necessary components. It begins by appending the 6-byte destination MAC address (obtained from user input) to the frame. Next, it adds the 6-byte source MAC address (read from the specified network interface). Following this, a 2-byte EtherType (0x88B5 in this case) is appended to indicate the protocol carried in the payload. Finally, the actual payload data (\u0026ldquo;L2-test-from-go\u0026rdquo;) is added. This ordered byte sequence forms the complete Ethernet frame, which is then sent directly over the network interface using a raw socket (AF_PACKET).\nWe have started tcpdump on the host machine to monitor traffic on the br0 bridge interface. Correspondingly, tcpdump was also started on the ceth1, ceth2, and ceth3 interfaces within the ns1, ns2, and ns3 namespaces. Host machine:\n1# Building the go snippet into executable binary 2laborant@ubuntu-01:~$ go build -o l2_ping ./l2_ping.go 3 4# Listen for the traffic on network interfaces ceth1, ceth2 and ceth3 in ns1, ns2 and ns3 network namespaces respectively 5laborant@ubuntu-01:~$ ip link list | grep br0 66: veth1@if5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br0 state UP mode DEFAULT group default qlen 1000 78: veth2@if7: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br0 state UP mode DEFAULT group default qlen 1000 810: veth3@if9: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br0 state UP mode DEFAULT group default qlen 1000 911: br0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 10laborant@ubuntu-01:~$ sudo tcpdump -i br0 -e -n 11tcpdump: verbose output suppressed, use -v[v]... for full protocol decode 12listening on br0, link-type EN10MB (Ethernet), snapshot length 262144 bytes 13 1408:51:05.394642 26:d7:11:b3:c8:e7 \u0026gt; 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::24d7:11ff:feb3:c8e7 \u0026gt; ff02::2: ICMP6, router solicitation, length 16 1508:51:06.521846 be:46:f3:5a:2b:27 \u0026gt; 02:77:f4:ad:6d:55, ethertype Unknown (0x88b5), length 29: 16 0x0000: 4c32 2d74 6573 742d 6672 6f6d 2d67 6f L2-test-from-go ns1 namespace:\n1# Sending frame from ceth1 (ns1) to ceth2 (ns2) 2# Here `02:77:f4:ad:6d:55` is the MAC address of ceth2 interface in ns2 namespace as can be seen from the output of `ip link list` below 3root@ubuntu-01:laborant# ./networking/l2_ping ceth1 02:77:f4:ad:6d:55 4Sent Ethernet frame via ceth1 → 02:77:f4:ad:6d:55 ns2 namespace:\n1laborant@ubuntu-01:~$ sudo nsenter --net=/run/netns/ns2 bash 2root@ubuntu-01:laborant# ip link list 31: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 4 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 57: ceth2@if8: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 6 link/ether 02:77:f4:ad:6d:55 brd ff:ff:ff:ff:ff:ff link-netnsid 0 7root@ubuntu-01:laborant# tcpdump -i ceth2 -e -n 8tcpdump: verbose output suppressed, use -v[v]... for full protocol decode 9listening on ceth2, link-type EN10MB (Ethernet), snapshot length 262144 bytes 10 1108:51:05.395138 26:d7:11:b3:c8:e7 \u0026gt; 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::24d7:11ff:feb3:c8e7 \u0026gt; ff02::2: ICMP6, router solicitation, length 16 1208:51:06.521868 be:46:f3:5a:2b:27 \u0026gt; 02:77:f4:ad:6d:55, ethertype Unknown (0x88b5), length 29: 13 0x0000: 4c32 2d74 6573 742d 6672 6f6d 2d67 6f L2-test-from-go ns3 namespace:\n1laborant@ubuntu-01:~$ sudo nsenter --net=/run/netns/ns3 bash 2root@ubuntu-01:laborant# ip link list 31: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 4 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 59: ceth3@if10: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 6 link/ether ce:3c:f4:76:12:a3 brd ff:ff:ff:ff:ff:ff link-netnsid 0 7root@ubuntu-01:laborant# tcpdump -i ceth3 -e -n 8tcpdump: verbose output suppressed, use -v[v]... for full protocol decode 9listening on ceth3, link-type EN10MB (Ethernet), snapshot length 262144 bytes 10 1108:51:05.394921 26:d7:11:b3:c8:e7 \u0026gt; 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::24d7:11ff:feb3:c8e7 \u0026gt; ff02::2: ICMP6, router solicitation, length 16 1208:51:06.521858 be:46:f3:5a:2b:27 \u0026gt; 02:77:f4:ad:6d:55, ethertype Unknown (0x88b5), length 29: 13 0x0000: 4c32 2d74 6573 742d 6672 6f6d 2d67 6f L2-test-from-go Upon an initial transmission of a frame from ceth1 to ceth2, it first arrives at the bridge through veth1, which is the corresponding host end of the ceth1 interface. Because the bridge has no prior record of the Ethernet frame\u0026rsquo;s destination MAC address, it consequently floods this frame to all connected ports. This action clarifies why the Ethernet frame is observable in ceth3\u0026rsquo;s tcpdump. Nevertheless, following this initial transfer, the bridge learns and stores the source MAC address (corresponding to the ceth1 ↔ veth1 connection) and its associated port within its Forwarding Database (FDB). Consequently, when a frame is subsequently sent in the opposite direction, from ceth2 to ceth1, it will not be flooded to ceth2. This is because of the bridge now possessing knowledge of the precise port for the destination MAC address linked to the veth1 ↔ ceth1 pair.\nns1 namespace:\n1# For reverse transmission from ceth2 -\u0026gt; ceth1, listening for traffic on ceth1 interface 2root@ubuntu-01:laborant# tcpdump -i ceth1 -e -n 3tcpdump: verbose output suppressed, use -v[v]... for full protocol decode 4listening on ceth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes 509:24:00.347717 02:77:f4:ad:6d:55 \u0026gt; be:46:f3:5a:2b:27, ethertype Unknown (0x88b5), length 29: 6 0x0000: 4c32 2d74 6573 742d 6672 6f6d 2d67 6f L2-test-from-go ns2 namespace:\n1# Sending frame from ceth2 2root@ubuntu-01:laborant# ./networking/l2_ping ceth2 be:46:f3:5a:2b:27 3Sent Ethernet frame via ceth2 → be:46:f3:5a:2b:27 ns3 namespace:\n1laborant@ubuntu-01:~$ sudo nsenter --net=/run/netns/ns3 bash 2root@ubuntu-01:laborant# tcpdump -i ceth3 -e -n 3tcpdump: verbose output suppressed, use -v[v]... for full protocol decode 4listening on ceth3, link-type EN10MB (Ethernet), snapshot length 262144 bytes 5 6# No traffic visible on ceth3 The preceding tests have illustrated the functionality of L2 segments and broadcast domains. We have not yet addressed L3 segments or IP packets, as the current setup\u0026rsquo;s interfaces lack IP assignments, with communication happening at the L2 layer via MAC addresses. Moving forward, we will investigate how Layer 2 and Layer 3 cooperate to facilitate packet transmission using IP addresses.\nL3 Segment To enable communication using IP addresses, it\u0026rsquo;s necessary to first assign an IP address to every machine (or, in this context, to each ceth network interface of a network namespace). The ip addr add command facilitates this process.\n1laborant@ubuntu-01:~$ sudo nsenter --net=/run/netns/ns1 bash 2root@ubuntu-01:laborant# ip addr add dev ceth1 192.168.0.1/24 3 4root@ubuntu-01:laborant# ip addr list | grep ceth1 55: ceth1@if6: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 6 inet 192.168.0.1/24 scope global ceth1 Upon assigning an IP address and its corresponding subnet mask to an interface, the kernel automatically adds a route derived from the subnet mask\u0026rsquo;s information.\n1root@ubuntu-01:laborant# ip route list 2192.168.0.0/24 dev ceth1 proto kernel scope link src 192.168.0.1 Consequently, all traffic destined for the 192.168.0.0/24 subnet will be directed through the ceth1 interface. Initially, all nodes (namespaces) will be assigned IP addresses, ensuring they are all part of the same subnet and reside within a single broadcast domain.\n1root@ubuntu-01:laborant# ip addr list | grep ceth3 29: ceth3@if10: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 3 inet 192.168.0.3/24 scope global ceth3 4 5root@ubuntu-01:laborant# ip addr list | grep ceth2 67: ceth2@if8: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 7 inet 192.168.0.2/24 scope global ceth2 An IP packet also has source and destination IP addresses in its header and the whole IP packet is actually the payload of the Ethernet frame that we have seen above. Any L2 device will only deal with Ethernet frame headers and will treat anything inside the payload as it is, we have already seen this by sending raw bytes of string L2-test-from-go as the payload of the Ethernet frame while building the frame with l2_send go utility binary. If we send a ping (ICMP packet) from ceth1 to 192.168.0.3 i.e. node 3 with ceth3 interface then the sender needs to figure out the destination MAC address as ping will only populate the source and destination IP address. ARP (Address resolution protocol) comes into play to resolve the MAC address for a given IP address, Lets see it in action by capturing the packets coming to bridge br0 and all interfaces ceth0, ceth1 and ceth3 when we send the ping message.\n1root@ubuntu-01:laborant# ping -c 1 192.168.0.3 2PING 192.168.0.3 (192.168.0.3) 56(84) bytes of data. 364 bytes from 192.168.0.3: icmp_seq=1 ttl=64 time=0.624 ms 4 5--- 192.168.0.3 ping statistics --- 61 packets transmitted, 1 received, 0% packet loss, time 0ms 7rtt min/avg/max/mdev = 0.624/0.624/0.624/0.000 ms Step 1: Ping triggers routing decision in ns1 In ns1, the routing table is:\n1root@ubuntu-01:laborant# ip route 2192.168.0.0/24 dev ceth1 proto kernel scope link src 192.168.0.1 So when I run:\n1root@ubuntu-01:laborant# ping -c 1 192.168.0.3 the kernel in ns1 decides:\nThe destination IP is in the same subnet The packet must go out via ceth1 No router or gateway is involved At this point:\nAn ICMP Echo Request is created at L3 (IP layer) No Ethernet frame can be built yet, because the destination MAC for 192.168.0.3 is unknown This missing MAC address is what triggers ARP.\nStep 2: First ARP – broadcast from ceth1 Because ns1 does not know the MAC for 192.168.0.3, it sends an ARP request. On br0, tcpdump -i br0 -e -n -vv shows:\n1be:46:f3:5a:2b:27 \u0026gt; ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), 2Request who-has 192.168.0.3 tell 192.168.0.1 Important details here:\nSource MAC: be:46:f3:5a:2b:27 (MAC of ceth1) Destination MAC: ff:ff:ff:ff:ff:ff (broadcast) This ARP is generated by the IP stack in ns1, not by the bridge Bridge behaviour at this stage When this frame enters br0 via veth1:\nThe bridge learns the source MAC: be:46:f3:5a:2b:27 → veth1 Because the destination MAC is broadcast, the bridge floods the frame to all other ports (broadcast domain) This explains why: ceth3 sees the ARP request (expected, it owns 192.168.0.3) ceth2 also sees the ARP request, even though it is not involved: be:46:f3:5a:2b:27 \u0026gt; ff:ff:ff:ff:ff:ff, ARP who-has 192.168.0.3 tell 192.168.0.1 Step 3: ARP reply from ceth3 – unicast Since ns3 owns 192.168.0.3, it replies with an ARP reply. On br0:\n1ce:3c:f4:76:12:a3 \u0026gt; be:46:f3:5a:2b:27, ethertype ARP (0x0806), 2Reply 192.168.0.3 is-at ce:3c:f4:76:12:a3 Key points:\nSource MAC: ce:3c:f4:76:12:a3 (MAC of ceth3) Destination MAC: be:46:f3:5a:2b:27 (MAC of ceth1) This is a unicast ARP reply - targeted for ceth1 Bridge behaviour here When this frame enters br0 via veth3:\nThe bridge learns another MAC: ce:3c:f4:76:12:a3 → veth3 The destination MAC (be:46:f3:5a:2b:27) is already known The frame is forwarded only to veth1 Because of this: ceth1 sees the ARP reply ceth2 does not see it This is the first point where flooding stops and unicast forwarding begins. Step 4: ICMP echo request and reply (pure unicast) After receiving the ARP reply, ns1 now has an ARP cache entry:\n1root@ubuntu-01:laborant# ip neighbour show 2192.168.0.3 dev ceth1 lladdr ce:3c:f4:76:12:a3 STALE Now ns1 can finally construct the Ethernet frame for the ICMP packet. On br0, the ICMP Echo Request appears as:\n1be:46:f3:5a:2b:27 \u0026gt; ce:3c:f4:76:12:a3, ethertype IPv4 (0x0800), 2192.168.0.1 \u0026gt; 192.168.0.3: ICMP echo request The reply from ns3:\n1ce:3c:f4:76:12:a3 \u0026gt; be:46:f3:5a:2b:27, ethertype IPv4 (0x0800), 2192.168.0.3 \u0026gt; 192.168.0.1: ICMP echo **reply** Bridge behaviour during ICMP At this point:\nThe bridge FDB (Forwarding database) already contains both MACs Every ICMP frame is forwarded only between veth1 and veth3 No flooding occurs ceth2 sees no ICMP traffic at all This confirms that the bridge is now operating in fully learned unicast mode. Step 5: Why a second ARP is sent by ceth3 A few seconds later, this appears on br0:\n1ce:3c:f4:76:12:a3 \u0026gt; be:46:f3:5a:2b:27, ethertype ARP (0x0806), 2Request who-has 192.168.0.1 tell 192.168.0.3 This often causes confusion. Important clarifications:\nThis is not reverse ARP (RARP) This is a normal ARP request initiated by ns3 Why ns3 needs this ARP In ns3, the ARP entry for 192.168.0.1 is in STALE state: Linux neighbour entries:\nAge out over time ICMP traffic alone does not guarantee permanent freshness When the entry is stale and traffic is needed, Linux probes using ARP So ns3 asks again: “Who has 192.168.0.1?”\nStep 6: Why this second ARP does NOT reach ceth2 This is the most subtle and important point. Look closely at the Ethernet header:\n1ce:3c:f4:76:12:a3 \u0026gt; be:46:f3:5a:2b:27, ethertype ARP The destination MAC is not broadcast. This means:\nns3 already knows the MAC address of 192.168.0.1 Therefore it sends a unicast ARP request Bridge behaviour for this ARP When the frame enters br0 via veth3:\nThe bridge already has this FDB entry: 1be:46:f3:5a:2b:27 → veth1 The destination MAC is known The bridge forwards the frame only to veth1 No flooding occurs As a result:\nceth1 sees the ARP request and replies ceth2 sees nothing, even though this is an ARP request Final summary (key points) A ping first triggers ARP because the destination MAC is unknown. The first ARP is broadcast and flooded across the bridge. The ARP reply allows the bridge to learn MAC locations. ICMP traffic then flows purely as unicast. ARP cache entries can become STALE, triggering additional ARP probes. These later ARPs can be unicast, not broadcast. Because the bridge already knows the destination MAC, it forwards such ARPs only to the correct port. This is why ceth2 saw the first ARP but did not see the second one. Overall, this trace cleanly demonstrates:\nARP is always generated by hosts (namespaces), not by the bridge The Linux bridge is strictly Layer 2, using only its FDB to forward frames Flooding happens only until MAC learning converges ARP traffic can be seen in your home router as well by running tcpdump on the Wi-Fii interface.\nVLAN Lets modify the setup to add 3 more network namespaces representing 3 more nodes connected to the same bridge but on a different subnet - 192.168.1.0/24 Now if we have following new node configurations:\nInterface Namespace IP ceth4 ns4 192.168.1.1/24 ceth5 ns5 192.168.1.2/24 ceth6 ns6 192.168.1.3/24 When br0 is configured as the master on their host ends, all six nodes share a single broadcast domain. Since a bridge operates as a Layer 2 device, an ARP flood message will reach every node within this broadcast domain. Consequently, an ARP request sent during a ping from 192.168.1.2 to 192.168.1.3 will be heard by all nodes in the 192.168.0.0/24 subnet. This creates an undesirable overlap between the network traffic of two subnets.\nping 192.168.1.2 -\u0026gt; 192.168.1.3 (192.168.1.0/24 subnet traffic)\n1root@ubuntu-01:laborant# ip addr list 21: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 3 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 412: ceth4@if13: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 5 link/ether 1a:cc:a5:0b:b3:52 brd ff:ff:ff:ff:ff:ff link-netnsid 0 6 inet 192.168.1.2/24 scope global ceth4 7 valid_lft forever preferred_lft forever 8 inet6 fe80::18cc:a5ff:fe0b:b352/64 scope link 9 valid_lft forever preferred_lft forever 10root@ubuntu-01:laborant# ping -c 1 192.168.1.3 11PING 192.168.1.3 (192.168.1.3) 56(84) bytes of data. 1264 bytes from 192.168.1.3: icmp_seq=1 ttl=64 time=0.098 ms 13 14--- 192.168.1.3 ping statistics --- 151 packets transmitted, 1 received, 0% packet loss, time 0ms 16rtt min/avg/max/mdev = 0.098/0.098/0.098/0.000 ms tcpdump on 192.168.1.3 node\n1root@ubuntu-01:laborant# ip addr list 21: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 3 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 416: ceth6@if17: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 5 link/ether 4a:e8:06:19:53:68 brd ff:ff:ff:ff:ff:ff link-netnsid 0 6 inet 192.168.1.3/24 scope global ceth6 7 valid_lft forever preferred_lft forever 8 inet6 fe80::48e8:6ff:fe19:5368/64 scope link 9 valid_lft forever preferred_lft forever 10root@ubuntu-01:laborant# tcpdump -i ceth6 -e -n 11tcpdump: verbose output suppressed, use -v[v]... for full protocol decode 12listening on ceth6, link-type EN10MB (Ethernet), snapshot length 262144 bytes 1318:06:38.439217 1a:cc:a5:0b:b3:52 \u0026gt; ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Request who-has 192.168.1.3 tell 192.168.1.2, length 28 1418:06:38.439239 4a:e8:06:19:53:68 \u0026gt; 1a:cc:a5:0b:b3:52, ethertype ARP (0x0806), length 42: Reply 192.168.1.3 is-at 4a:e8:06:19:53:68, length 28 1518:06:38.439257 1a:cc:a5:0b:b3:52 \u0026gt; 4a:e8:06:19:53:68, ethertype IPv4 (0x0800), length 98: 192.168.1.2 \u0026gt; 192.168.1.3: ICMP echo request, id 2348, seq 1, length 64 1618:06:38.439269 4a:e8:06:19:53:68 \u0026gt; 1a:cc:a5:0b:b3:52, ethertype IPv4 (0x0800), length 98: 192.168.1.3 \u0026gt; 192.168.1.2: ICMP echo reply, id 2348, seq 1, length 64 1718:06:43.593966 4a:e8:06:19:53:68 \u0026gt; 1a:cc:a5:0b:b3:52, ethertype ARP (0x0806), length 42: Request who-has 192.168.1.2 tell 192.168.1.3, length 28 1818:06:43.593994 1a:cc:a5:0b:b3:52 \u0026gt; 4a:e8:06:19:53:68, ethertype ARP (0x0806), length 42: Reply 192.168.1.2 is-at 1a:cc:a5:0b:b3:52, length 28 ARP floods are also visible in tcpdump of ceth3 interface with IP 192.168.0.3 of 192.168.0.0/24 subnet\n1oot@ubuntu-01:laborant# ip link list 21: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 3 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 49: ceth3@if10: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 5 link/ether 52:f2:9f:46:e4:69 brd ff:ff:ff:ff:ff:ff link-netnsid 0 6root@ubuntu-01:laborant# ip addr list 71: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 8 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 99: ceth3@if10: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 10 link/ether 52:f2:9f:46:e4:69 brd ff:ff:ff:ff:ff:ff link-netnsid 0 11 inet 192.168.0.3/24 scope global ceth3 12 valid_lft forever preferred_lft forever 13 inet6 fe80::50f2:9fff:fe46:e469/64 scope link 14 valid_lft forever preferred_lft forever 15root@ubuntu-01:laborant# tcpdump -i ceth3 -e -n 16tcpdump: verbose output suppressed, use -v[v]... for full protocol decode 17listening on ceth3, link-type EN10MB (Ethernet), snapshot length 262144 bytes 1818:06:38.439221 1a:cc:a5:0b:b3:52 \u0026gt; ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Request who-has 192.168.1.3 tell 192.168.1.2, length 28 And the reason is simple: the destination MAC address for ARP request \u0026ldquo;who-has 192.168.1.3 tell 192.168.1.2\u0026rdquo; is ff:ff:ff:ff:ff:ff, which means the bridge will broadcast the message to all connected ports. Hence, all interfaces of the other subnet will also receive the ARP request. This is undesirable, as subnets are mainly created to group traffic between different types of entities in a single network.\nVLAN tagging can be used to tag nodes of one subnet with a id which helps in partitioning the broadcast domain. We can configure VLAN at the end hosts or at the bridge itself, where bridge will store the mapping for a port and corresponding vlan-id. If we look at the currently configured vlan-id at the bridge:\n1laborant@ubuntu-01:~$ bridge vlan 2port vlan-id 3veth1 1 PVID Egress Untagged 4veth2 1 PVID Egress Untagged 5veth3 1 PVID Egress Untagged 6veth4 1 PVID Egress Untagged 7veth5 1 PVID Egress Untagged 8veth6 1 PVID Egress Untagged 9br0 1 PVID Egress Untagged Each of the port is configured with vlan-id 1 and is configured Untagged for egress traffic, That means bridge will not tag the outgoing traffic with the VLAN ID = 1 in ethernet frame (as shown above) which is fine since currently all the ports have same vlan-id.\nLets configure the bridge to have vlan-id = 10 for 192.168.0.0/24 subnet and vlan-id = 20 for 192.168.1.0/24 subnet.\n1# Removing exisiting vid 1 from all ports 2laborant@ubuntu-01:~$ sudo bridge vlan del dev veth1 vid 1 3laborant@ubuntu-01:~$ sudo bridge vlan del dev veth2 vid 1 4laborant@ubuntu-01:~$ sudo bridge vlan del dev veth3 vid 1 5laborant@ubuntu-01:~$ sudo bridge vlan del dev veth4 vid 1 6laborant@ubuntu-01:~$ sudo bridge vlan del dev veth5 vid 1 7laborant@ubuntu-01:~$ sudo bridge vlan del dev veth6 vid 1 8 9# Adding new VIDs per subnet 10laborant@ubuntu-01:~$ sudo bridge vlan add dev veth1 vid 10 pvid 10 11laborant@ubuntu-01:~$ sudo bridge vlan add dev veth2 vid 10 pvid 10 12laborant@ubuntu-01:~$ sudo bridge vlan add dev veth3 vid 10 pvid 10 13laborant@ubuntu-01:~$ sudo bridge vlan add dev veth4 vid 20 pvid 20 14laborant@ubuntu-01:~$ sudo bridge vlan add dev veth5 vid 20 pvid 20 15laborant@ubuntu-01:~$ sudo bridge vlan add dev veth6 vid 20 pvid 20 16 17# Enable VLAN filtering on bridge br0 18laborant@ubuntu-01:networking$ sudo ip link set br0 type bridge vlan_filtering 1 19 20# New bridge config 21laborant@ubuntu-01:~$ bridge vlan 22port vlan-id 23veth1 10 PVID 24veth2 10 PVID 25veth3 10 PVID 26veth4 20 PVID 27veth5 20 PVID 28veth6 20 PVID 29br0 1 PVID Egress Untagged Now lets try to send a broadcast message from ceth4\n1laborant@ubuntu-01:networking$ sudo ip netns exec ns4 /home/laborant/networking/l2_send ceth4 ff:ff:ff:ff:ff:ff 2Sent Ethernet frame via ceth4 → ff:ff:ff:ff:ff:ff All interfaces of vlan=20 heard the broadcast whereas no traffic reached interfaces ceth1, ceth2 and ceth3 of vlan=10.\n1root@ubuntu-01:laborant# tcpdump -i ceth5 -e -n 216:15:09.042913 1a:cc:a5:0b:b3:52 \u0026gt; ff:ff:ff:ff:ff:ff, ethertype Unknown (0x88b5), length 29: 3 0x0000: 4c32 2d74 6573 742d 6672 6f6d 2d67 6f L2-test-from-go 4 5 6root@ubuntu-01:laborant# tcpdump -i ceth6 -e -n 716:15:09.042913 1a:cc:a5:0b:b3:52 \u0026gt; ff:ff:ff:ff:ff:ff, ethertype Unknown (0x88b5), length 29: 8 0x0000: 4c32 2d74 6573 742d 6672 6f6d 2d67 6f L2-test-from-go Likewise, when 192.168.1.1 pings 192.168.1.3, the ARP request to resolve 192.168.1.3\u0026rsquo;s MAC address is exclusively broadcast to interfaces within the 192.168.1.1 subnet that belong to VLAN 20. This illustrates how VLANs contribute to separating traffic between subnets.\n1root@ubuntu-01:laborant# tcpdump -i ceth5 -e -n 216:25:25.134826 1a:cc:a5:0b:b3:52 \u0026gt; ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Request who-has 192.168.1.3 tell 192.168.1.1, length 28 3 4 5root@ubuntu-01:laborant# tcpdump -i ceth6 -e -n 616:25:25.134838 1a:cc:a5:0b:b3:52 \u0026gt; ff:ff:ff:ff:ff:ff, ethertype 802.1Q (0x8100), length 46: vlan 20, p 0, ethertype ARP (0x0806), Request who-has 192.168.1.3 tell 192.168.1.1, length 28 Following diagram shows how VLAN separates traffic of two subnets by enabling VLAN filtering at the bridge level. Conclusion In this post, we explored how L1, L2, and L3 network segments are formed and how the different layers of the OSI model work together in practice using Linux network namespaces. We began with pure Layer 2 communication using MAC addresses, examined broadcast domains and MAC learning behavior in a bridge, and then built on top of that to understand Layer 3 communication using IP and ARP. Through packet captures, we clearly saw how ARP bridges the gap between IP addressing and Ethernet, and how broadcast domains define the scope of Layer 2 visibility. Finally, by introducing VLANs, we demonstrated how broadcast domains can be cleanly partitioned within a single Layer 2 infrastructure, reinforcing the separation between Layer 2 forwarding and Layer 3 addressing while still allowing scalable and structured network designs.\nReferences https://labs.iximiuz.com/courses/computer-networking-fundamentals - A great place to experiment with contents of this blog. This blog post is inspired from the course itself - Highly recommended.\n","permalink":"https://harshrai654.github.io/blogs/networking-basics/","summary":"\u003cp\u003eIn this blog I will try to explain basics of networking concepts and how these concepts are used to create simple to complex networking topologies to transfer data between physical machines and with the use of network namespaces on Linux we will try to simulate various scenarios as we discuss the theory behind them.\u003c/p\u003e\n\u003ch2 id=\"network-segments\"\u003eNetwork Segments\u003c/h2\u003e\n\u003cp\u003eA \u003cem\u003enetwork segment\u003c/em\u003e refers to a distinct part of a computer \u003ca href=\"https://www.linfo.org/network.html\"\u003enetwork\u003c/a\u003e that is isolated from the remainder of the network by a specific device, such as a \u003ca href=\"https://www.linfo.org/repeater.html\"\u003erepeater\u003c/a\u003e, hub, \u003ca href=\"https://www.linfo.org/bridge.html\"\u003ebridge\u003c/a\u003e, switch, or router. Within each segment, one or more computers or other hosts may reside.\nDepending on how these devices are connected, the network forms \u003cstrong\u003eL1, L2 or L3 segments\u003c/strong\u003e, Here the segment are mainly named on the basis of the layer of OSI networking model in which a segment mainly communicates.\u003c/p\u003e","title":"Networking Basics"},{"content":"I was recently investigating ways to improve the efficiency of file uploads to a Node.js server. This need arose after encountering a production bug where the absence of a maximum file size limit for uploads led to an out-of-memory crash due to file buffers consuming excessive heap memory. In this Node.js server, I was using Express and express-openapi-validator to document the server\u0026rsquo;s API with an OpenAPI specification. express-openapi-validator utilizes multer for file uploads. I had previously encountered this library whenever file uploads from forms needed to be handled in Node.js, but I never questioned why a separate library was necessary for file uploads. This time, I decided to go deeper to understand if a dedicated package for file uploads is truly needed, and if so, what specific benefits Multer or similar libraries provide. I initially needed to find a configuration option in express-openapi-validator to set a request-wide limit on the maximum size (in bytes) of data allowed in a request, including all file attachments. The express-openapi-validator package offers a fileUploader configuration (fileUploader documentation) that passes options directly to multer.\nTherefore, I checked the multer documentation and discovered the limits configuration option (multer limits documentation), which perfectly suited the needs. multer uses busboy to handle the parsing of multipart form data in a stream fashion. Consequently, the limits options defined in multer are actually passed directly to busboy, and represent busboy's limit configurations.\n1- **limits** - _object_ - Various limits on incoming data. Valid properties are: 2 3- **fieldNameSize** - _integer_ - Max field name size (in bytes). **Default:** `100`. 4\t5- **fieldSize** - _integer_ - Max field value size (in bytes). **Default:** `1048576` (1MB). 6\t7- **fields** - _integer_ - Max number of non-file fields. **Default:** `Infinity`. 8\t9- **fileSize** - _integer_ - For multipart forms, the max file size (in bytes). **Default:** `Infinity`. 10\t11- **files** - _integer_ - For multipart forms, the max number of file fields. **Default:** `Infinity`. 12\t13- **parts** - _integer_ - For multipart forms, the max number of parts (fields + files). **Default:** `Infinity`. 14\t15- **headerPairs** - _integer_ - For multipart forms, the max number of header key-value pairs to parse. **Default:** `2000` (same as node\u0026#39;s http module). So, based on the available options, I can have a per-file-based size limit along with a limit on the number of files. However, that wasn\u0026rsquo;t quite what I needed, since the requirement wasn\u0026rsquo;t just to cap the number of files, but to limit the overall request upload size. Eventually, I stumbled upon this feature request in the busboy repository, which addressed exactly what I wanted. This discovery kicked off a intresting exploration of how busboy handles multipart form data. It led to uncovering many details regarding back pressure in streams, how TCP control flow windows help in maintaining back pressure, and all this in an effort to contribute a request-wide total size limit configuration option to busboy. Streams in NodeJS Let\u0026rsquo;s first look at an example usage of busboy with plain old HTTP server in Node.js\n1const http = require(\u0026#39;http\u0026#39;); 2 3const busboy = require(\u0026#39;busboy\u0026#39;); 4 5http.createServer((req, res) =\u0026gt; { 6 if (req.method === \u0026#39;POST\u0026#39;) { 7 console.log(\u0026#39;POST request\u0026#39;); 8 const bb = busboy({ headers: req.headers }); 9 bb.on(\u0026#39;file\u0026#39;, (name, file, info) =\u0026gt; { 10 const { filename, encoding, mimeType } = info; 11 console.log( 12 `File [${name}]: filename: %j, encoding: %j, mimeType: %j`, 13 filename, 14 encoding, 15 mimeType 16 ); 17 file.on(\u0026#39;data\u0026#39;, (data) =\u0026gt; { 18 console.log(`File [${name}] got ${data.length} bytes`); 19 }).on(\u0026#39;close\u0026#39;, () =\u0026gt; { 20 console.log(`File [${name}] done`); 21 }); 22 }); 23 bb.on(\u0026#39;field\u0026#39;, (name, val, info) =\u0026gt; { 24 console.log(`Field [${name}]: value: %j`, val); 25 }); 26 bb.on(\u0026#39;close\u0026#39;, () =\u0026gt; { 27 console.log(\u0026#39;Done parsing form!\u0026#39;); 28 res.writeHead(303, { Connection: \u0026#39;close\u0026#39;, Location: \u0026#39;/\u0026#39; }); 29 res.end(); 30 }); 31 req.pipe(bb); 32 } 33}).listen(8000, () =\u0026gt; { 34 console.log(\u0026#39;Listening for requests\u0026#39;); 35}); Looking at the code, the req object (a Readable stream) is piped into the busboy instance (bb). This bb instance returns a MultiPart class object (specifically, an instance of the class found here), which inherits from Writable.\nThe Readable and Writable streams have several methods that are invoked at different times during operations such as read, write, and pause. Because each stream relies on the consumer downstream and the producer upstream, Node.js buffers the generated data by default if there\u0026rsquo;s backpressure in the stream pipeline. You can find more information about this in the Node.js documentation:\nBoth Writable and Readable streams will store data in an internal buffer. The amount of data potentially buffered depends on the highWaterMark option passed into the stream\u0026rsquo;s constructor. For normal streams, the highWaterMark option specifies a total number of bytes. Data is buffered in Readable streams when the implementation calls stream.push(chunk). If the consumer of the Stream does not call stream.read(), the data will sit in the internal queue until it is consumed. Once the total size of the internal read buffer reaches the threshold specified by highWaterMark, the stream will temporarily stop reading data from the underlying resource until the data currently buffered can be consumed (that is, the stream will stop calling the internal readable._read() method that is used to fill the read buffer). Data is buffered in Writable streams when the writable.write(chunk) method is called repeatedly. While the total size of the internal write buffer is below the threshold set by highWaterMark, calls to writable.write() will return true. Once the size of the internal buffer reaches or exceeds the highWaterMark, false will be returned.\nA key goal of the stream API, particularly the stream.pipe() method, is to limit the buffering of data to acceptable levels such that sources and destinations of differing speeds will not overwhelm the available memory.\nTo understand the idea discussed above in the docs let\u0026rsquo;s take an example: If your server does something like:\n1const ws = fs.createWriteStream(\u0026#39;file.bin\u0026#39;) 2req.pipe(ws); Then data flows like this:\n[TCP socket] → [IncomingMessage (Readable)] → [Writable File Stream] → [disk] But these components run at different speeds:\nThe network producer (socket) might push data quickly. The file system consumer might be slower (e.g., disk I/O, file system buffering). So Node.js stream buffering ensures that: The Readable (req) will stop calling its internal _read() once its internal buffer exceeds its highWaterMark. The Writable (fs.WriteStream) will signal backpressure by returning false from .write(), which will pause the Readable until the buffer drains. ┌────────────────────┐ │ Network Socket │ (Producer) │ [TCP Stream] │ └────────┬───────────┘ │ data chunks → ▼ req.push(chunk) ┌────────────────────┐ │ IncomingMessage │ (Readable Stream) │ Internal Buffer │ │ highWaterMark = 16KB │ [Buffered: █████░░░░░░] 12KB / 16KB └────────┬───────────┘ │ │ ws.write(chunk) → called internally after pipe ▼ ┌────────────────────┐ │ File WriteStream │ (Writable Stream) │ Internal Buffer │ │ highWaterMark = 64KB │ [Buffered: ████░░░░░░░░] 20KB / 64KB └────────┬───────────┘ │ fs.write(chunk) ▼ ┌────────────────────┐ │ Disk I/O │ (Slow Consumer) │ write operation → │ └────────────────────┘ As you can see above, if Disk I/O is slower than the rate at which we are receiving chunks from the TCP socket, the ws.write call will continue buffering incoming chunks into its internal memory buffer until it reaches its highWaterMark (64KB in this case). Once the buffer is full, ws.write returns false, signalling the stream above it that the downstream consumer is slower. The IncomingMessage (req), being a readable stream, will then start buffering the chunks coming from the TCP socket in its own internal buffer (16KB highWaterMark). When this buffer fills up, req.push will also return false, indicating it cannot accept more data for now. This backpressure propagates upstream and eventually affects the TCP socket itself, which the OS handles via TCP flow control - essentially slowing down the sender to prevent memory overload.\nAt the OS level, each TCP socket has its own kernel buffer. Normally, data flows like this:\nTCP socket kernel buffer → user-space buffer → writable stream buffer → filesystem buffer → disk Each step involves memory copies. In some cases, if we don’t need to parse or modify the data (like streaming raw uploads to disk), we can bypass user-space entirely using zero-copy techniques, transferring data directly from the socket kernel buffer to the filesystem kernel buffer. This avoids redundant memory copies and improves performance, at the cost of not being able to inspect or modify the data before it’s written. For a deeper dive into TCP connections, sockets, and file descriptors, you can check out my detailed write-up here.\nBusboy Implementation \u0026amp; New Limit Config Now that we have a basic understanding of how data moves through the IncomingRequest object in an HTTP server, let\u0026rsquo;s address the initial question: why do we need Busboy, and what problem does it solve? Busboy is a parser for formdata. We\u0026rsquo;ve already seen how we pipe the readable stream req output to Busboy\u0026rsquo;s multipart writable stream. Before examining the data flow within Busboy, let\u0026rsquo;s understand multipart/form-data. Below is an example of multipart/form-data. We will focus primarily on file uploads for this form, rather than fields.\nPOST /upload HTTP/1.1 Host: example.com User-Agent: curl/8.0.1 Accept: */* Content-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW Content-Length: 512 ------WebKitFormBoundary7MA4YWxkTrZu0gW Content-Disposition: form-data; name=\u0026#34;file1\u0026#34;; filename=\u0026#34;file1.txt\u0026#34; Content-Type: text/plain Hello, this is the content of file1.txt. It could be multiple lines. ------WebKitFormBoundary7MA4YWxkTrZu0gW Content-Disposition: form-data; name=\u0026#34;file2\u0026#34;; filename=\u0026#34;file2.jpg\u0026#34; Content-Type: image/jpeg (binary data of file2.jpg would go here) ------WebKitFormBoundary7MA4YWxkTrZu0gW-- Breakdown of each part of the above raw HTTP multipart/form-data\nHTTP is a text protocol Everything above is sent as plain text over a TCP connection (except the actual binary file contents). The TCP stream is just a stream of bytes; it has no understanding of fields or files. Node.js’s HTTP server reads this TCP stream and parses the start-line and headers (e.g., POST /upload HTTP/1.1, Host, Content-Type, Content-Length) so it can understand the request. After the headers are parsed, the remaining bytes (the body) are exposed to req (IncomingMessage) as a readable stream. This raw body byte stream is exactly what gets pushed into Busboy for parsing. Boundary markers The boundary string (----WebKitFormBoundary7MA4YWxkTrZu0gW) separates each part of the multipart request. Each part starts with --\u0026lt;boundary\u0026gt; and the whole request ends with --\u0026lt;boundary\u0026gt;--. Busboy parses these boundaries to identify where one field ends and another begins. Headers for each part Content-Disposition gives the field name and filename. Content-Type tells the server what type of data is being sent. These headers are part of the raw text stream; without parsing them, the server cannot know the field names or file types. Content / File data Text files are just plain text; binary files are raw bytes included inline in the TCP stream. Node.js or any HTTP server cannot automatically separate the files from the TCP stream. Busboy (or similar parser) is required to convert this raw text+binary stream into usable file streams or buffers in user space. This is why a kernel -\u0026gt; user-space memory copy is necessary: the server must access the bytes in user-space to parse headers, boundaries, and file contents. Content-Length Represents the total bytes of all boundaries, headers, and file data. TCP itself only cares about sending bytes; the HTTP layer interprets them according to this length. How Busboy Parses the Multipart Stream The above raw stream of data is what gets piped to Busboy’s internal Multipart writeable stream. Inside its constructor, Busboy initializes various limits based on the configuration provided. As part of the constructor logic, it creates a HeaderParser and sets up a StreamSearch instance with the multipart boundary string as the needle:\n1this._bparser = new StreamSearch(`\\r\\n--${boundary}`, ssCb); StreamSearch calls the given callback ssCb whenever it encounters non-matching data or finds a match for the needle.\nIn other words, as the raw multipart/form-data byte stream flows through, StreamSearch helps separate the actual boundaries from the data chunks in between, and passes those chunks to the provided callback for processing. The callback ssCb is the core of the multipart parsing logic. During initialization, Busboy sets up several internal tracking variables, such as:\nfileSize - Tracks the size of the current file in bytes across all incoming chunks for that file. Once this reaches the configured limits.fileSize, Busboy stops processing additional chunks for that specific file and skips them until it encounters the next boundary (i.e., the start of a new field). files - Keeps track of the number of files seen so far. Once this count reaches limits.files, Busboy skips all subsequent file data until it detects the end of the form-data. There are other similar limits and tracking variables initialized internally, but these two are the most relevant to the new feature we want to add: A global total file size limit across all uploaded files.\nFor example, if the limit is 50 MB, Busboy should process data only until the total size of all uploaded files reaches 50 MB. After that, it should skip any further chunks - even those belonging to new files - until it reaches the end of the form-data stream.\nTo implement this, we’ll maintain a running counter similar to fileSize, but instead of resetting it per file, we’ll keep accumulating the total bytes processed across all files. Once it reaches limits.totalFileSize, we stop processing any further chunks for all files\nFor each segment identified by ssCb, the HeaderParser is invoked to inspect the Content-Disposition header. From this header, Busboy determines whether the part represents a regular form field or a file upload. If it’s a file, the filename is extracted, and the files counter is incremented.\nAs long as the number of processed files remains below limits.files, Busboy creates a new Readable stream for that file and emits a file event. This event carries both the file metadata and the associated stream, allowing your application to handle the file data as it arrives; for example, by piping it directly to a file or cloud storage.\n1const hparser = new HeaderParser((header) =\u0026gt; { 2// ... 3// ... 4if (files === filesLimit) { 5\tif (!hitFilesLimit) { 6\thitFilesLimit = true; 7\tthis.emit(\u0026#39;filesLimit\u0026#39;); 8\t} 9\tskipPart = true; 10\treturn; 11} 12 13fileSize = 0; 14this._fileStream = new FileStream(fileOpts, this); 15 16this.emit( 17 \u0026#39;file\u0026#39;, 18 partName, 19 this._fileStream, 20 { filename, 21\tencoding: partEncoding, 22\tmimeType: partType } 23); 24// ... 25// ... 26}); 27 28 29// At the other end we can listen to the `file` event on busboy instance 30 bb.on(\u0026#39;file\u0026#39;, (name, file, info) =\u0026gt; { 31 const saveTo = path.join(os.tmpdir(), `busboy-upload-${random()}`); 32 file.pipe(fs.createWriteStream(saveTo)); 33}); When the file event is triggered, the listener receives the stream of decoded file data - meaning by this point, the raw multipart byte stream from the TCP socket has been parsed and separated into meaningful file chunks by Busboy's internal StreamSearch and header parsing logic. We can also see that when the file limit is reached, Busboy sets a boolean flag hitFilesLimit and emits the filesLimit event exactly once on the Busboy instance. This ensures that the user is notified only the first time the limit is exceeded. In addition, skipPart is set to true. This flag tells the parser to ignore the rest of the current part’s body after finishing header parsing. Essentially, once we know we’ve hit the maximum number of allowed files, Busboy continues scanning the incoming stream but skips over any additional file data until it encounters the next boundary. If the limits.files threshold has not been reached, the parser continues reading the body of the current file. For every incoming chunk of data, it updates the fileSize variable and ensures that only data up to the configured fileSizeLimit is passed downstream. Any excess bytes are truncated and trigger a limit event on the file stream. Here’s the corresponding section of code that handles this logic:\n1if (!skipPart) { 2 if (this._fileStream) { 3\tlet chunk; 4\tconst actualLen = Math.min(end - start, fileSizeLimit - fileSize); 5\tif (!isDataSafe) { 6\tchunk = Buffer.allocUnsafe(actualLen); 7\tdata.copy(chunk, 0, start, start + actualLen); 8\t} else { 9\tchunk = data.slice(start, start + actualLen); 10\t} 11 12\tfileSize += chunk.length; 13\tif (fileSize === fileSizeLimit) { 14\tif (chunk.length \u0026gt; 0) 15\tthis._fileStream.push(chunk); 16\tthis._fileStream.emit(\u0026#39;limit\u0026#39;); 17\tthis._fileStream.truncated = true; 18\tskipPart = true; 19\t} else if (!this._fileStream.push(chunk)) { 20\tif (this._writecb) 21\tthis._fileStream._readcb = this._writecb; 22\tthis._writecb = null; 23\t} 24} The important part here is that the limit event is fired only on the individual FileStream instance, not on the Busboy instance. This means the event listener attached to that particular file stream (usually through the 'file' event handler on Busboy) can detect that the stream has been truncated because its size exceeded the per-file limit.\nWith our new totalFileSize feature, however, we’ll extend this behavior, A new event: totalSizelimit event will be fired on both the FileStream and the Busboy instance. This is because exceeding the totalFileSize means the current file is truncated and no further file data across the entire request should be processed. The event now serves as a global signal to stop consumption of further reads as there won\u0026rsquo;t be any more data parsed and pushed to the stream by busboy because of limits.\nBusboy's Back pressure Handling If you look closely at the snippet above, you’ll also notice something interesting about how Busboy handles backpressure. When this._fileStream.push(chunk) returns false, it means that the internal buffer of the Readable stream (_fileStream) itself is full, and therefore it’s temporarily unable to accept more data from its upstream producer: In this case, Busboy’s Multipart writable stream. Here’s the nuance:\nThe FileStream class in Busboy is a custom Readable stream that emits the parsed file data. Busboy (the Multipart writable) acts as the producer of that readable stream’s data. When Busboy calls fileStream.push(chunk): If it returns true, it means the file stream’s internal buffer still has room it’s safe to push more data. If it returns false, the file stream’s internal highWaterMark is reached meaning it’s now buffering as much data as it can before its slow consumer (e.g., fs.createWriteStream) reads more. That “slow consumer” can be anything on the other end of the pipe: a file write stream (disk I/O delay), a transform stream (CPU-bound operation), or even network latency if you’re piping it to another request. To handle this gracefully, Busboy performs a clever trick. When the file stream’s internal buffer is full (push() returns false), Busboy temporarily saves its _writecb - the callback passed by Node.js’ Writable stream to signal readiness for the next chunk - into the file stream’s _readcb property, and then sets _writecb = null.\nLet’s see where this _writecb comes from in the first place:\n1_write(chunk, enc, cb) { 2\tthis._writecb = cb; 3\tthis._bparser.push(chunk, 0); 4\tif (this._writecb) 5\tcallAndUnsetCb(this); 6} This _write() method belongs to the Multipart class, which extends Node’s Writable stream.\nIt’s invoked automatically when the incoming request’s readable stream (for example, req) is piped into the Busboy instance like this:\n1req.pipe(bb); Here, the cb parameter in _write method represents Node’s backpressure callback.\nCalling this callback tells Node.js,\n“I’m done processing this chunk, You can send me the next one.”\nBy saving cb into this._writecb, Busboy is essentially saying:\n“Hold on, I’ll call this callback later, once I’m sure it’s safe to receive more data after parsing current chunk.”\nNow, when the file stream slows down (because its internal buffer is full), Busboy hands off this _writecb to the file stream by assigning it to _readcb.\nThis is a crucial step: it ties the writable side (Busboy) and the readable side (FileStream) together through deferred signalling. Later, when the consumer, Say, an fs.WriteStream writing to disk finishes processing some data and is ready for more, it triggers the FileStream’s _read() method:\n1_read(n) { 2\tconst cb = this._readcb; 3\tif (cb) { 4\tthis._readcb = null; 5\tcb(); 6\t} 7} This is where the “magic” happens.\nThat previously saved callback (cb), which originally came from the writable side (Busboy), is now invoked by the readable side (FileStream).\nThis signals back upstream that the consumer has caught up and Busboy can safely resume parsing and pushing more data.\nsequenceDiagram participant Net as Network Socket (req) participant Busboy as Busboy Multipart (Writable) participant FS as FileStream (Readable) participant Disk as fs.WriteStream (Writable) Note over Net,Busboy: Normal data flow → Net-\u0026gt;\u0026gt;Busboy: chunk Busboy-\u0026gt;\u0026gt;FS: fileStream.push(chunk) FS-\u0026gt;\u0026gt;Disk: pipe(chunk) alt Buffer has space (push() returns true) Note over Busboy: ✅ FileStream buffer has room Busboy-\u0026gt;\u0026gt;Busboy: call this._writecb()\u0026lt;br/\u0026gt;(request next chunk) else Buffer full (push() returns false) Note over FS: ⚠️ FileStream buffer full Busboy-\u0026gt;\u0026gt;FS: FS._readcb = this._writecb\u0026lt;br/\u0026gt;this._writecb = null Note right of FS: Wait until downstream frees buffer end Note over Disk: Disk slowly writes data... Disk--\u0026gt;\u0026gt;FS: _read() called\u0026lt;br/\u0026gt;(ready for more) FS-\u0026gt;\u0026gt;Busboy: call _readcb()\u0026lt;br/\u0026gt;(resumes parsing) Busboy-\u0026gt;\u0026gt;FS: push(next chunk) FS-\u0026gt;\u0026gt;Disk: pipe(chunk) In essence, this handoff of callbacks between _writecb and _readcb is Busboy’s internal mechanism for propagating backpressure.\nIt ensures that data flows smoothly from The req (the network socket and IncomingMessage readable) → into Busboy’s Multipart writable → Through the FileStream readable → And finally into the consumer writable (like the filesystem) All without overwhelming any part of the chain.\nWrapping Up This exploration of how Busboy parses multipart form data, manages backpressure, and coordinates between readable and writable streams gave me a much deeper understanding of Node.js stream internals.\nAs part of this deep dive, As I mentioned, I also proposed a small improvement to Busboy — adding support for a totalFileSize limit that stops processing once the total upload size crosses a configured threshold. You can check out the implementation for more details about the change here:\n🔗 GitHub PR #374 – Add total file size limit support in Busboy\n","permalink":"https://harshrai654.github.io/blogs/multipart-form-uploads---busboy-and-node-streams/","summary":"\u003cp\u003eI was recently investigating ways to improve the efficiency of file uploads to a Node.js server. This need arose after encountering a production bug where the absence of a maximum file size limit for uploads led to an out-of-memory crash due to file buffers consuming excessive heap memory. In this Node.js server, I was using Express and \u003ccode\u003eexpress-openapi-validator\u003c/code\u003e to document the server\u0026rsquo;s API with an \u003ccode\u003eOpenAPI\u003c/code\u003e specification. \u003ccode\u003eexpress-openapi-validator\u003c/code\u003e utilizes \u003ccode\u003emulter\u003c/code\u003e for file uploads. I had previously encountered this library whenever file uploads from forms needed to be handled in Node.js, but I never questioned why a separate library was necessary for file uploads. This time, I decided to go deeper to understand if a dedicated package for file uploads is truly needed, and if so, what specific benefits \u003ccode\u003eMulter\u003c/code\u003e or similar libraries provide.\nI initially needed to find a configuration option in \u003ccode\u003eexpress-openapi-validator\u003c/code\u003e to set a request-wide limit on the maximum size (in bytes) of data allowed in a request, including all file attachments.  The \u003ccode\u003eexpress-openapi-validator\u003c/code\u003e package offers a \u003ccode\u003efileUploader\u003c/code\u003e configuration (\u003ca href=\"https://cdimascio.github.io/express-openapi-validator-documentation/usage-file-uploader/\"\u003efileUploader documentation\u003c/a\u003e) that passes options directly to \u003ccode\u003emulter\u003c/code\u003e.\u003c/p\u003e","title":"Multipart Form Uploads - Busboy and Node Streams"},{"content":"We previously discussed replicated state machines, leader election in the RAFT consensus algorithm, and log-based state maintenance. Now, we\u0026rsquo;ll focus on log replication across peer servers. We\u0026rsquo;ll also examine how RAFT ensures that the same commands are applied to the state machine at a given log index on every peer, because of the leader\u0026rsquo;s one-way log distribution to followers.\nLeader Initialisation Once a candidate becomes leader we call setupLeader function which initiates go routine for each peer in the RAFT cluster, Each go routine of respective peer is responsible for replicating new log entries or sending heartbeat via AppendEntries RPC.\n1func (rf *Raft) setupLeader() { 2\trf.mu.Lock() 3\tdefer rf.mu.Unlock() 4 5\tctx, cancel := context.WithCancel(context.Background()) 6\trf.leaderCancelFunc = cancel 7 8\tfor peerIndex := range rf.peers { 9\tif peerIndex != rf.me { 10\trf.nextIndex[peerIndex] = len(rf.log) + rf.snapshotLastLogIndex 11\tgo rf.replicate(peerIndex, ctx) 12\t} 13\t} 14 15\tgo func() { 16\tfor !rf.killed() { 17\tselect { 18\tcase \u0026lt;-ctx.Done(): 19\treturn 20\tcase \u0026lt;-time.After(HEARTBEAT_TIMEOUT): 21\trf.replicatorCond.Broadcast() 22\t} 23\t} 24\t}() 25} When starting a replication logic thread for each peer, we also send a Go context to rf.replicate. This context shows the current leader\u0026rsquo;s status. If the leader steps down, we call rf.leaderCancelFunc, which cancels the context. When a context is canceled, the ctx.Done() channel closes, stopping any waiting for results from that channel. More information about Go\u0026rsquo;s context can be found here. Our RAFT struct includes a condition variable, replicatorCond (of type *sync.Cond), that signals all peer goroutines of the leader to run the replication logic every HEARTBEAT_TIMEOUT. This ensures that a heartbeat is sent to each peer at the specified interval. A condition variable provides functions like Wait, Signal, and Broadcast. If multiple threads are waiting on a condition variable after releasing the underlying mutex, Signal will wake one of such waiting threads, and Broadcast will wake all waiting threads. Here we are using Broadcast to wake all waiting threads of each peer to send the next heartbeat. A condition variable is an operating system concept, and you can read more about the same from here. To read about the API of Go\u0026rsquo;s of type *sync.Cond, check Go\u0026rsquo;s official Docs here.\nLog Replication Each individual peer thread runs the replicate method, given below is implementation of rf.replicate function\n1func (rf *Raft) replicate(peerIndex int, ctx context.Context) { 2\tlogMismatch := false 3\tfor !rf.killed() { 4\tselect { 5\tcase \u0026lt;-ctx.Done(): 6\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Leader stepped down from leadership before initiating replicate.\\n\u0026#34;, rf.me, peerIndex) 7\treturn 8\tdefault: 9\trf.mu.Lock() 10 11\tif rf.state != StateLeader { 12\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Not a leader anymore, winding up my leadership setup.\\n\u0026#34;, rf.me, peerIndex) 13\tif rf.leaderCancelFunc != nil { 14\trf.leaderCancelFunc() 15\trf.replicatorCond.Broadcast() 16\t} 17\trf.mu.Unlock() 18\treturn 19\t} 20 21\t// Only waiting when: 22\t// - There is no log to send - In this case the wait will be signalled by the heartbeat 23\t// - We are in a continuous loop to find correct nextIndex for this peer with retrial RPCs 24\tif !logMismatch \u0026amp;\u0026amp; rf.nextIndex[peerIndex] \u0026gt;= len(rf.log)+rf.snapshotLastLogIndex { 25\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Wating for next signal to replicate.\\n\u0026#34;, rf.me, peerIndex) 26\trf.replicatorCond.Wait() 27\t} 28 29\tif rf.killed() { 30\treturn 31\t} 32 33\treply := \u0026amp;AppendEntriesReply{} 34\tlogStartIndex := rf.nextIndex[peerIndex] 35\tvar prevLogTerm int 36\tprevLogIndex := logStartIndex - 1 37\tpeer := rf.peers[peerIndex] 38 39\tif prevLogIndex-rf.snapshotLastLogIndex \u0026gt; 0 { 40\tprevLogTerm = rf.log[prevLogIndex-rf.snapshotLastLogIndex].Term 41\t} else if prevLogIndex == rf.snapshotLastLogIndex { 42\tprevLogTerm = rf.snapshotLastLogTerm 43\t} else { 44\t// prevLogIndex \u0026lt; rf.snapshotLastLogIndex 45\tprevLogTerm = -1 46\t} 47 48\tif prevLogTerm == -1 { 49\tlogMismatch = true 50\t// Leader does not have logs at `prevLogIndex` because of compaction 51\t// Leader needs to send snaphot to the peer as part of log repairing 52\targs := \u0026amp;InstallSnapshotArgs{ 53\tTerm: rf.currentTerm, 54\tLeaderId: rf.me, 55\tLastIncludedIndex: rf.snapshotLastLogIndex, 56\tLastIncludedTerm: rf.snapshotLastLogTerm, 57\tData: rf.persister.ReadSnapshot(), 58\t} 59 60\treply := \u0026amp;InstallSnapshotReply{} 61 62\tdprintf(\u0026#34;[leader-install-snapshot: %d: peer: %d]: InstallSnapshot RPC with index: %d and term: %d sent.\\n\u0026#34;, rf.me, peerIndex, args.LastIncludedIndex, args.LastIncludedTerm) 63\trf.mu.Unlock() 64 65\tok := rf.sendRPCWithTimeout(ctx, peer, peerIndex, \u0026#34;InstallSnapshot\u0026#34;, args, reply) 66 67\trf.mu.Lock() 68 69\tif ok { 70\tif reply.Term \u0026gt; rf.currentTerm { 71\tdprintf(\u0026#34;[leader-install-snapshot: %d: peer: %d]: Stepping down from leadership, Received InstallSnapshot reply from peer %d, with term %d \u0026gt; %d - my term\\n\u0026#34;, rf.me, peerIndex, peerIndex, reply.Term, rf.currentTerm) 72 73\trf.state = StateFollower 74\trf.currentTerm = reply.Term 75\trf.lastContactFromLeader = time.Now() 76 77\tif rf.leaderCancelFunc != nil { 78\trf.leaderCancelFunc() 79\trf.replicatorCond.Broadcast() 80\t} 81\trf.persist(nil) 82\trf.mu.Unlock() 83\treturn 84\t} 85 86\tdprintf(\u0026#34;[leader-install-snapshot: %d: peer: %d]: Snapshot installed successfully\\n\u0026#34;, rf.me, peerIndex) 87\trf.nextIndex[peerIndex] = rf.snapshotLastLogIndex + 1 88\trf.mu.Unlock() 89\tcontinue 90\t} else { 91\tdprintf(\u0026#34;[leader-install-snapshot: %d: peer: %d]: Snapshot installtion failed!\\n\u0026#34;, rf.me, peerIndex) 92\trf.nextIndex[peerIndex] = rf.snapshotLastLogIndex 93\trf.mu.Unlock() 94\tcontinue 95\t} 96\t} else { 97\treplicateTerm := rf.currentTerm 98 99\tlogEndIndex := len(rf.log) + rf.snapshotLastLogIndex 100\tnLogs := logEndIndex - logStartIndex 101 102\targs := \u0026amp;AppendEntriesArgs{ 103\tTerm: rf.currentTerm, 104\tLeaderId: rf.me, 105\tPrevLogIndex: prevLogIndex, 106\tPrevLogTerm: prevLogTerm, 107\tLeaderCommit: rf.commitIndex, 108\t} 109 110\tif nLogs \u0026gt; 0 { 111\tentriesToSend := rf.log[logStartIndex-rf.snapshotLastLogIndex:] 112\targs.Entries = make([]LogEntry, len(entriesToSend)) 113\tcopy(args.Entries, entriesToSend) 114\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Sending AppendEntries RPC in term %d with log index range [%d, %d).\\n\u0026#34;, rf.me, peerIndex, replicateTerm, logStartIndex, logEndIndex) 115\t} else { 116\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Sending AppendEntries Heartbeat RPC for term %d.\\n\u0026#34;, rf.me, peerIndex, replicateTerm) 117\t} 118 119\trf.mu.Unlock() 120\tok := rf.sendRPCWithTimeout(ctx, peer, peerIndex, \u0026#34;AppendEntries\u0026#34;, args, reply) 121 122\trf.mu.Lock() 123 124\tif ok { 125\tselect { 126\tcase \u0026lt;-ctx.Done(): 127\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Leader stepped down from leadership after sending AppendEntries RPC.\\n\u0026#34;, rf.me, peerIndex) 128\trf.mu.Unlock() 129\treturn 130\tdefault: 131\t// Check fot change in state during the RPC call 132\tif rf.currentTerm != replicateTerm || rf.state != StateLeader { 133\t// Leader already stepped down 134\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Checked ladership state after getting AppendEntries Reply, Not a leader anymore, Winding up my leadership setup.\\n\u0026#34;, rf.me, peerIndex) 135\tif rf.leaderCancelFunc != nil { 136\trf.leaderCancelFunc() 137\trf.replicatorCond.Broadcast() 138\t} 139\trf.mu.Unlock() 140\treturn 141\t} 142 143\t// Handle Heartbeat response 144\tif !reply.Success { 145\tif reply.Term \u0026gt; rf.currentTerm { 146\tdprintf(\u0026#34;[leader-replicate: %d: peer: %d]: Stepping down from leadership, Received ApppendEntries reply from peer %d, with term %d \u0026gt; %d - my term\\n\u0026#34;, rf.me, peerIndex, peerIndex, reply.Term, rf.currentTerm) 147 148\trf.state = StateFollower 149\trf.currentTerm = reply.Term 150\trf.lastContactFromLeader = time.Now() 151 152\tif rf.leaderCancelFunc != nil { 153\trf.leaderCancelFunc() 154\trf.replicatorCond.Broadcast() 155\t} 156\trf.persist(nil) 157\trf.mu.Unlock() 158\treturn 159\t} 160 161\t// Follower rejected the AppendEntries RPC beacuse of log conflict 162\t// Update the nextIndex for this follower 163\tlogMismatch = true 164\tfollowersConflictTermPresent := false 165\tif reply.ConflictTerm != -1 { 166\tfor i := prevLogIndex - rf.snapshotLastLogIndex; i \u0026gt; 0; i-- { 167\tif rf.log[i].Term == reply.ConflictTerm { 168\trf.nextIndex[peerIndex] = i + 1 + rf.snapshotLastLogIndex 169\tfollowersConflictTermPresent = true 170\tbreak 171\t} 172 173\t} 174 175\tif !followersConflictTermPresent { 176\trf.nextIndex[peerIndex] = reply.ConflictIndex 177\t} 178\t} else { 179\trf.nextIndex[peerIndex] = reply.ConflictIndex 180\t} 181\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Logmismatch - AppendEntries RPC with previous log index %d of previous log term %d failed. Retrying with log index:%d.\\n\u0026#34;, rf.me, peerIndex, prevLogIndex, prevLogTerm, rf.nextIndex[peerIndex]) 182\trf.mu.Unlock() 183\tcontinue 184\t} else { 185\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: responded success to AppendEntries RPC in term %d with log index range [%d, %d).\\n\u0026#34;, rf.me, peerIndex, replicateTerm, logStartIndex, logEndIndex) 186\tlogMismatch = false 187 188\tif nLogs \u0026gt; 0 { 189\t// Log replication successful 190\trf.nextIndex[peerIndex] = prevLogIndex + nLogs + 1 191\trf.matchIndex[peerIndex] = prevLogIndex + nLogs 192 193\t// Need to track majority replication upto latest log index 194\t// - So that we can update commitIndex 195\t// - Apply logs upto commitIndex 196\t// Just an idea - maybe this needs to be done separately in a goroutine 197\t// Where we continuosly check lastApplied and commitIndex 198\t// Apply and lastApplied to commit index and if leader send the response to apply channel 199\tmajority := len(rf.peers)/2 + 1 200 201\tfor i := len(rf.log) - 1; i \u0026gt; rf.commitIndex-rf.snapshotLastLogIndex; i-- { 202\tmatchedPeerCount := 1 203\tif rf.log[i].Term == rf.currentTerm { 204\tfor pi := range rf.peers { 205\tif pi != rf.me \u0026amp;\u0026amp; rf.matchIndex[pi] \u0026gt;= i+rf.snapshotLastLogIndex { 206\tmatchedPeerCount++ 207\t} 208\t} 209\t} 210 211\t// Largest possible log index greater the commitIndex replicated at majority of peers 212\t// update commitIndex 213\tif matchedPeerCount \u0026gt;= majority { 214\trf.commitIndex = i + rf.snapshotLastLogIndex 215 216\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Log index %d replicated to majority of peers.(%d/%d peers), updating commitIndex to : %d, current lastApplied value: %d.\\n\u0026#34;, rf.me, peerIndex, i+rf.snapshotLastLogIndex, matchedPeerCount, len(rf.peers), rf.commitIndex, rf.lastApplied) 217\trf.applyCond.Signal() 218\tbreak 219\t} 220\t} 221\t} 222 223\trf.mu.Unlock() 224\tcontinue 225\t} 226\t} 227\t} 228\tdprintf(\u0026#34;[leader-replicate: %d | peer %d]: Sending AppendEntries RPC at leader\u0026#39;s term: %d, failed. Payload prevLogIndex: %d | prevLogTerm: %d.\\n\u0026#34;, rf.me, peerIndex, replicateTerm, prevLogIndex, prevLogTerm) 229\trf.mu.Unlock() 230\tcontinue 231\t} 232\t} 233\t} 234} The replicate function operates in a continuous loop. Inside this loop, a select statement monitors the leader\u0026rsquo;s context (ctx) status. If ctx.Done() is not yet closed, it confirms the current server is still the leader. To ensure correctness, the server\u0026rsquo;s current state is also checked to be StateLeader. If it\u0026rsquo;s not, rf.leaderCanelFunc is explicitly called to relinquish leadership, which closes the context\u0026rsquo;s done channel, signalling the leader\u0026rsquo;s relinquishment to other components. Additionally, the loop waits on rf.replicatorCond when there are no more logs to transmit and no log mismatch between the leader\u0026rsquo;s and the peer\u0026rsquo;s logs. As mentioned in Part 1 of this blog series, servers periodically compact their logs to manage their size. This involves taking a snapshot of the current state and then truncating the log up to that point. If a follower\u0026rsquo;s log is significantly behind the leader\u0026rsquo;s and the leader has already truncated its log, the leader may need to send a snapshot using the InstallSnapshot RPC. We will delve deeper into log compaction in a later part of this series. Leader maintains volatile state for each peer nextIndex and matchIndex, these two properties for each peer are maintained by the leader, and it tells the leader which next log index to send to the peer and index up to which logs are replicated for that peer respectively.\nHeartbeat rules The AppendEntries RPC also functions as a heartbeat, allowing the leader to communicate its identity and log status to all followers. A follower only accepts this RPC if:\nThe leader\u0026rsquo;s Term value is greater than or equal to the follower\u0026rsquo;s current term. If not, the follower rejects the RPC, signalling that the sender (mistakenly believing itself to be the leader) is outdated. The follower sends its own reply.Term to indicate this. Upon receiving a Term in the reply that is higher than its own, the leader relinquishes leadership. The leader includes PrevLogIndex and PrevLogTerm with each AppendEntries RPC. These values are vital, as they indicate the leader\u0026rsquo;s log state by specifying the index and term of the log up to which the leader believes the follower\u0026rsquo;s log matches. If there is a discrepancy, the follower responds with reply.Success set to false, indicating a log mismatch. The leader then decreases nextIndex[peerIndex] for that peer and sends another heartbeat with an earlier log index. This process continues until a matching log index is found. At that point, the follower accepts the leader\u0026rsquo;s AppendEntries RPC and uses the reconcileLogs function to remove conflicting log entries and replace them with the log range sent by the leader from [PrevLogIndex + 1, LatestLogIndexAtLeader]. extracted from args,Entries field. This RAFT property is followed by each index of the follower, so if the current index and term match with the leader, so does the log index previous to this one, or else a conflict would have occurred previously itself. So this single property ensures that the log state matches between leader and follower up to the latest index; otherwise, the follower will not accept the heartbeat or normal AppendEntries RPC with log entries, and the to-and-fro of heartbeats after that to find the non-conflict index at the follower is called the log correction process. Later on, we will see how we can optimise this log correction because currently the leader needs to try for each log index in decreasing order, which can take a lot of time if the follower has been stale for a long time and during that time the leader\u0026rsquo;s log has grown a lot. The image above, shows a simple successful log replication situation. Here, Follower 1 and Follower 2 have the same initial part of the log as the leader. Also, the leader knows exactly which parts of the log need to be sent to each follower.\nIn the replicate method, we use a logMismatch flag within the replication loop. This flag shows if there was a log problem when sending the AppendEntries RPC in the last loop. If there was a problem, we don\u0026rsquo;t wait (we don\u0026rsquo;t call rf.replicatorCond.Wait(), which releases the lock and puts the thread to sleep). This is because, if there\u0026rsquo;s a log problem, we want to fix the log quickly, so we send the next AppendEntries RPC right away with the updated previous log index and term. If the previous log index is less than the index up to which we\u0026rsquo;ve taken a snapshot of the state and shortened the log, we send an InstallSnapshot RPC to send the snapshot to the follower instead, since we don\u0026rsquo;t have the needed logs. We\u0026rsquo;ll discuss snapshots and log compaction more later. We still follow one rule closely: if we get an RPC reply with a term value higher than the leader\u0026rsquo;s current term, the leader gives up leadership by cancelling the context. See the previous part for more information. In the image, follower 1\u0026rsquo;s nextIndex is at index 7. When an AppendEntries RPC is sent with prevLogIndex=6 and prevLogTerm=2, the follower detects a log mismatch and rejects the RPC. Because the follower\u0026rsquo;s term is also 2, the leader keeps its leadership role and immediately reduces its nextIndex for follower 1 to 6. It then sends another AppendEntries RPC (without waiting due to logMismatch=true) with prevLogIndex=5 and prevLogTerm=2. This second RPC is accepted. The leader sends Entries=[1, 2], causing the entries after index 5 to be replaced by these new entries (handled by ). Thus, the leader can erase uncommitted entries from the follower during log correction. Follower 2 accepts the initial AppendEntries RPC because there is no conflict. Since The leader manages each peer in its own separate goroutine, After each successful RPC response, the leader checks if a majority (replication factor) has replicated the log up to a specific index. In this scenario, once follower 2 responds (likely before follower 1, which is still correcting its log), the leader will have replicated log index 7 on 2/3 of the peers (including itself).\nThe leader then updates its commitIndex and signals a condition variable called rf.applyCond. First, we will examine how the follower handles the AppendEntries RPC. Then, building on this understanding, we will discuss commitIndex, lastApplied, and rf.applyCond, explaining how they ensure log entries are committed, what \u0026ldquo;committed\u0026rdquo; means, what \u0026ldquo;applying\u0026rdquo; a log means, and how it is handled.\u0026quot;\nHandling AppendEntries RPC Lets now look at how a Follower handles an AppendEntries RPC from a leader, we will revisit leader\u0026rsquo;s replicate method when we get to how logs are committed and applied to the state machine by the leader and how this change is propagated to followers. Given below is the implementation of AppendEntries RPC:\n1func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { 2\t// Handling heart beats from leader 3\t// When follower recieves heartbeat it should check for heartbeat validity 4 5\t// [Case: 1] if args.Term \u0026lt; rf.currentTerm then it means this follower is either a new leader 6\t// OR it voted for someone else after least heartbeat. 7\t// In such case follower should return reply.Success = false indicating it does not acknowledge the sender 8\t// as leader anymore and should set reply.Term = rf.currentTerm. (This indicated sender to step down from leadership) 9 10\t// [Case: 2] if args.Term \u0026gt;= rf.currentTerm the the current follower/candidate becomes follower again accepting current leader 11\t// In such case rf.currentTerm = args.Term and reply.Success = true with reply.Term = rf.currentTerm (same term as the sender) 12\t// In Case 2 we should reset the election timer since we have received the heartbeat from a genuine leader 13 14\t// In Case 1 since the previous leader is now left behind, there are 3 possibilities: 15\t// A. The current peer is a candidate now and an election was already started by it or even finished with it being the current leader 16\t// B. Some other peer is now a candidate with an election going on OR is a leader now 17\t// C. No election has taken place till now 18\t// In all the above cases we should not interrupt the election timeout or anything else 19 20\trf.mu.Lock() 21\tdefer rf.mu.Unlock() 22 23\tif len(args.Entries) == 0 { 24\t// Heartbeat 25\tdprintf(\u0026#34;[Peer: %d]: Recieved AppendEntries RPC as heartbeat from leader %d for term %d with commitIndex %d.\\n\u0026#34;, rf.me, args.LeaderId, args.Term, args.LeaderCommit) 26\t} 27 28\tif args.Term \u0026lt; rf.currentTerm { 29\tdprintf(\u0026#34;[Peer: %d]: AppendEntries RPC from leader %d for term %d not acknowledged, Leader\u0026#39;s term: %d is older than my current term: %d.\\n\u0026#34;, rf.me, args.LeaderId, args.Term, args.Term, rf.currentTerm) 30\treply.Success = false 31\treply.Term = rf.currentTerm 32 33\treturn 34\t} else { 35\t// Sent by current leader 36\t// Reset election timeout 37\tif rf.state == StateLeader { 38\tdprintf(\u0026#34;[Peer: %d]: AppendEntries RPC recieved from current leader: %d, winding up my leadership setup.\\n\u0026#34;, rf.me, args.LeaderId) 39\tif rf.leaderCancelFunc != nil { 40\trf.leaderCancelFunc() 41\trf.replicatorCond.Broadcast() 42\t} 43\t} 44 45\trf.lastContactFromLeader = time.Now() 46 47\trf.currentTerm = args.Term 48\trf.state = StateFollower 49 50\trf.persist(nil) 51 52\tlatestLogIndex := len(rf.log) - 1 53\tlogTerm := 0 54 55\tif args.PrevLogIndex \u0026gt; rf.snapshotLastLogIndex \u0026amp;\u0026amp; args.PrevLogIndex \u0026lt;= latestLogIndex+rf.snapshotLastLogIndex { 56\tlogTerm = rf.log[args.PrevLogIndex-rf.snapshotLastLogIndex].Term 57\t} else if args.PrevLogIndex == rf.snapshotLastLogIndex { 58\tlogTerm = rf.snapshotLastLogTerm 59\t} else if args.PrevLogIndex \u0026lt; rf.snapshotLastLogIndex { 60\t// This should trigger InstallSnapshot from leader 61\treply.ConflictIndex = rf.snapshotLastLogIndex + 1 62\treply.ConflictTerm = -1 63\t} 64 65\tif logTerm != args.PrevLogTerm { 66\tdprintf(\u0026#34;[Peer: %d]: AppendEntries RPC from leader %d for term %d not acknowledged. Log terms do not match, (Leader term, Leader index): (%d, %d), peer\u0026#39;s term for same log index: %d.\\n\u0026#34;, rf.me, args.LeaderId, args.Term, args.PrevLogTerm, args.PrevLogIndex, logTerm) 67\treply.Success = false 68\treply.Term = rf.currentTerm 69 70\tif args.PrevLogIndex \u0026lt;= latestLogIndex+rf.snapshotLastLogIndex { 71\treply.ConflictTerm = logTerm 72\ti := args.PrevLogIndex - rf.snapshotLastLogIndex 73\t// Find fist index of `logTerm` in follower\u0026#39;s log 74\tfor ; i \u0026gt; 0; i-- { 75\tif rf.log[i].Term != logTerm { 76\tbreak 77\t} 78\t} 79 80\treply.ConflictIndex = i + 1 + rf.snapshotLastLogIndex 81\t} 82 83\treturn 84\t} 85 86\treply.Success = true 87\treply.Term = rf.currentTerm 88 89\tdprintf(\u0026#34;[Peer: %d]: AppendEntries RPC from leader %d for term %d acknowledged.\\n\u0026#34;, rf.me, args.LeaderId, args.Term) 90 91\tif len(args.Entries) \u0026gt; 0 { 92\trf.reconcileLogs(args.Entries, args.PrevLogIndex) 93\t} 94 95\tif args.LeaderCommit \u0026gt; rf.commitIndex { 96\t// If leaderCommit \u0026gt; commitIndex, set commitIndex = 97\t// min(leaderCommit, index of last new entry) 98 99\trf.commitIndex = args.LeaderCommit 100 101\tif args.LeaderCommit \u0026gt;= len(rf.log)+rf.snapshotLastLogIndex { 102\trf.commitIndex = len(rf.log) - 1 + rf.snapshotLastLogIndex 103\t} 104 105\tdprintf(\u0026#34;[Peer: %d]: Updated commmit index to %d.\\n\u0026#34;, rf.me, rf.commitIndex) 106 107\trf.applyCond.Signal() 108\t} 109\t} 110 111} Like other Remote Procedure Calls, we initially verify if the sender\u0026rsquo;s args.Term is lower than the recipient\u0026rsquo;s term. If it is, we reject the call, notifying the sending leader that its leadership is outdated and the cluster\u0026rsquo;s term has advanced since it was last the leader. We include the term value in the response, prompting the sender to relinquish leadership and begin synchronizing. As mentioned before, this synchronization occurs when the peer receives a heartbeat from the current leader, initiating any necessary log corrections. This demonstrates that log flow consistently originates from the current leader to all other followers. The same principle applies if the receiving peer believes itself to be the leader of a term less than or equal to the term in the AppendEntries RPC. In this scenario, we acknowledge the sender as the rightful current leader and invoke rf.leaderCancelFunc() to force the current peer to step down from leadership. Additionally, we broadcast on rf.replicatorCond to ensure that all peer-specific replication threads spawned by this receiver recognize the effect of the cancelled context as quickly as possible. After confirming the leader\u0026rsquo;s identity, we begin by updating the lastContactFromLeader timestamp. Each follower has a ticker that verifies if the last communication from the leader exceeded a set threshold (the election timeout). If it does, the follower can become a candidate and initiate an election. Updating the timestamp prevents unnecessary elections. More information about leader election can be found in the preceding section. After this We move on to verifying log consistency. This is done using the PrevLogIndex and PrevLogTerm fields of the AppendEntries RPC. The follower checks if it has a log entry at PrevLogIndex and whether that entry’s term matches PrevLogTerm. If either of these checks fail, it indicates that the follower’s log has diverged from the leader’s. In such cases, the follower rejects the RPC by returning Success = false. This rejection signals to the leader that it needs to adjust its nextIndex for this follower and retry from an earlier point in its log, Effectively “backing up” until it finds the last matching entry. Now, without any optimization, this correction process could be quite inefficient: the leader would decrement nextIndex[peer] one step at a time and re-send the RPC repeatedly until it finds a match. If the follower has fallen far behind (say, by N entries), this would take N separate RPCs: a linear-time process that can become costly in large logs. To avoid that, Raft introduces a log inconsistency optimization. Instead of merely rejecting the RPC, the follower includes two additional fields in its response: ConflictTerm and ConflictIndex. These tell the leader the term of the conflicting entry and the first index where that term begins. With this information, the leader can skip behind intelligently. It looks for the last entry in its own log with the same ConflictTerm.\nIf it finds one, it sets the follower’s nextIndex to just after that entry: meaning it assumes both logs agree up to that point, This assumption may fail when the AppendEntries is again rejected with backed up nextIndex value which means the followers log are lagging behind leader\u0026rsquo;s term at that index, so leader will repeat this process again with new ConflictIndex and ConflictTerm If it doesn’t find any entry with that term, it sets nextIndex to ConflictIndex, effectively jumping back to where the follower’s conflicting term started. This optimization allows the leader to perform correction per term, rather than per index, drastically reducing the number of RPCs needed for log repair, especially when followers lag by many entries. You can check the code for replicate method above (especially when the res.Success is set to false with res.Term \u0026lt;= leader.currentTerm) to check the above behaviour of leader during log conflict. Once a consistent prefix is identified, the follower can safely append new entries included in the RPC. In the implementation, this is handled by the reconcileLogs method, which takes the leader’s entries and merges them into the follower’s log while ensuring term consistency. 1func (rf *Raft) reconcileLogs(leaderEntries []LogEntry, leaderPrevLogIndex int) { 2\tnextIndex := leaderPrevLogIndex + 1 3\tcurrentLogLength := len(rf.log) + rf.snapshotLastLogIndex 4\tleaderEntriesIndex := 0 5 6\tfor nextIndex \u0026lt; currentLogLength \u0026amp;\u0026amp; leaderEntriesIndex \u0026lt; len(leaderEntries) { 7\tif rf.log[nextIndex-rf.snapshotLastLogIndex].Term != leaderEntries[leaderEntriesIndex].Term { 8\tbreak 9\t} 10 11\tnextIndex++ 12\tleaderEntriesIndex++ 13\t} 14 15\tif leaderEntriesIndex \u0026lt; len(leaderEntries) { 16\t// If an existing entry conflicts with a new one (same index but different terms), delete the existing entry and all that follow it 17\trf.log = rf.log[:nextIndex-rf.snapshotLastLogIndex] 18 19\t// Append any new entries not already in the log 20\trf.log = append(rf.log, leaderEntries[leaderEntriesIndex:]...) 21 22\trf.persist(nil) 23\t} 24 25\tdprintf(\u0026#34;[Peer %d]: Reconciled logs with leader from index %d, current logs length %d.\\n\u0026#34;, rf.me, leaderPrevLogIndex, len(rf.log)+rf.snapshotLastLogIndex) 26} This function essentially performs the final step of log reconciliation after the agreement point between leader and follower has been established.\nIt starts from the index immediately after leaderPrevLogIndex: the point up to which both leader and follower agree. Then it walks through both logs in parallel, comparing terms for each entry. As soon as it finds a mismatch (a conflict in term at the same index), it stops: that’s where the logs diverge. From that point onward, it truncates the follower’s log (rf.log = rf.log[:nextIndex - rf.snapshotLastLogIndex]) and appends all remaining entries from the leader’s log. This effectively overwrites any inconsistent or “stale” entries on the follower with the authoritative entries from the leader, restoring complete log alignment. So, reconcileLogs is the place where the follower’s log is actually modified. It’s where the “diff” from the leader is applied after the last point of agreement. Once again, this highlights the fundamental directionality of Raft’s replication flow: log entries always move from the leader to the followers, never the other way around.\nCommitting and Applying Log Entries If a leader receives successful AppendEntries RPC responses from a majority of followers, it indicates that the logs of a majority of peers match the leader\u0026rsquo;s up to a specific index. Because agreement on a current log index implies agreement on all preceding indices, the leader can confidently assert that the logs match up to that given index. Consequently, the leader will then designate that index as the commit index.\n1for i := len(rf.log) - 1; i \u0026gt; rf.commitIndex-rf.snapshotLastLogIndex; i-- { 2\tmatchedPeerCount := 1 3\tif rf.log[i].Term == rf.currentTerm { 4\tfor pi := range rf.peers { 5\tif pi != rf.me \u0026amp;\u0026amp; rf.matchIndex[pi] \u0026gt;= i+rf.snapshotLastLogIndex { 6\tmatchedPeerCount++ 7\t} 8\t} 9\t} 10 11\t// Largest possible log index greater the commitIndex replicated at majority of peers 12\t// update commitIndex 13\tif matchedPeerCount \u0026gt;= majority { 14\trf.commitIndex = i + rf.snapshotLastLogIndex 15 16\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Log index %d replicated to majority of peers.(%d/%d peers), updating commitIndex to : %d, current lastApplied value: %d.\\n\u0026#34;, rf.me, peerIndex, i+rf.snapshotLastLogIndex, matchedPeerCount, len(rf.peers), rf.commitIndex, rf.lastApplied) 17\trf.applyCond.Signal() 18\tbreak 19\t} 20} This is evident at the end of the replicate method. After obtaining a success response for each peer-specific thread, the method verifies the matchIndex for all peers. Similar to nextIndex, each leader maintains a matchIndex for each peer. This matchIndex essentially signifies the index up to which the peer\u0026rsquo;s log matches the leader\u0026rsquo;s, or, in other words, the extent to which the leader\u0026rsquo;s log has been replicated to that follower. As you can see if a new log entry has been replicated to majority, We also signal on applyCond condition variable. Upon starting, each peer launches both a ticker go routine and an applier go routine. The applier\u0026rsquo;s function is to retrieve logs from the lastAppliedIndex (the point of the last application) up to the most recent commit index and execute each command on the state machine. In this setup, every Raft server instance receives an apply channel at startup, provided by the service utilizing RAFT. In our key-value server example, commands from log entries sent to the apply channel are processed by the KV server, which then executes them.\n1func (rf *Raft) applier() { 2\tfor !rf.killed() { 3\trf.mu.Lock() 4\tfor rf.lastApplied \u0026gt;= rf.commitIndex { 5\trf.applyCond.Wait() 6\t} 7 8\tif rf.killed() { 9\treturn 10\t} 11 12\ti := rf.lastApplied + 1 13\tif i \u0026lt; rf.snapshotLastLogIndex+1 { 14\ti = rf.snapshotLastLogIndex + 1 15\t} 16 17\tmessages := make([]raftapi.ApplyMsg, 0) 18\tfor ; i \u0026lt;= rf.commitIndex; i++ { 19\t// Skip entries that are already in the snapshot 20\tif i \u0026lt;= rf.snapshotLastLogIndex { 21\tcontinue 22\t} 23\tmsg := raftapi.ApplyMsg{ 24\tCommandValid: true, 25\tCommand: rf.log[i-rf.snapshotLastLogIndex].Command, 26\tCommandIndex: i, 27\t} 28\tmessages = append(messages, msg) 29\t} 30\tcommitIndex := rf.commitIndex 31\trf.mu.Unlock() 32 33\tfor _, msg := range messages { 34\tif rf.killed() { 35\treturn 36\t} 37\trf.applyCh \u0026lt;- msg 38\t} 39 40\trf.mu.Lock() 41\trf.lastApplied = commitIndex 42\tdprintf(\u0026#34;[Peer: %d]: Applied logs till latest commit index: %d, lastApplied : %d.\\n\u0026#34;, rf.me, rf.lastApplied, rf.commitIndex) 43\trf.mu.Unlock() 44\t} 45} While sending something to a Go channel, the channel itself can work concurrently with multiple threads pushing data from it. So, we do not need the rf.mu lock while sending commands, which can be a blocking operation. In a non-buffered channel, an inserted element is only accepted when there is an active consumer to consume that element. Therefore, to avoid holding the Raft server state\u0026rsquo;s lock mu, we release the lock and only reacquire it after we have applied all the logs to log out the Raft server\u0026rsquo;s state.\nIf the commitIndex is not greater than lastAppliedIndex, the applier thread pauses by unlocking and waiting for a signal. When the commitIndex of a leader or follower changes, the applier thread is signaled to resume and apply new logs (between lastAppliedIndex and commitIndex) to the state machine. The leader\u0026rsquo;s commitIndex is updated upon majority replication. To ensure the replicated state of follower matches the leader\u0026rsquo;s RAFT state (same commitIndex) and the underlying state machine\u0026rsquo;s state (e.g., the KV server\u0026rsquo;s state), the updated commitIndex of leader is included in the next upcoming AppendEntries RPC heartbeat. Each follower, after agreeing with the RPC, updates its own commitIndex and signals its applier thread to update the state machine with the new logs received from the leader.\n1func (rf *Raft) (args *AppendEntriesArgs, reply *AppendEntriesReply) { 2// ...existing code 3if args.LeaderCommit \u0026gt; rf.commitIndex { 4\t// If leaderCommit \u0026gt; commitIndex, set commitIndex = 5\t// min(leaderCommit, index of last new entry) 6 7\trf.commitIndex = args.LeaderCommit 8 9\tif args.LeaderCommit \u0026gt;= len(rf.log)+rf.snapshotLastLogIndex { 10\trf.commitIndex = len(rf.log) - 1 + rf.snapshotLastLogIndex 11\t} 12 13\tdprintf(\u0026#34;[Peer: %d]: Updated commmit index to %d.\\n\u0026#34;, rf.me, rf.commitIndex) 14 15\trf.applyCond.Signal() 16\t} 17} Why Majority-Based Commitment Guarantees Safety A critical property of Raft is that once a log entry is committed (replicated on a majority of servers), it is guaranteed to eventually be applied to the state machine even if the current leader crashes immediately after marking it committed. This guarantee comes from how Raft defines commitment and how election safety works together with log matching. When a leader replicates a new log entry to a majority of peers and advances its commitIndex, it knows that at least a majority of servers (including itself) now store that entry in their logs. Even if the leader crashes at this point (before or after applying the entry), that entry cannot be lost, because any future leader must have that entry as well. Here’s why: during the next election, any candidate must receive votes from a majority of peers to become the new leader. And followers will only vote for a candidate whose log is at least as up-to-date as their own, Meaning the candidate’s last log term and index are greater than or equal to the follower’s. Since the previously committed entry was stored on a majority, at least one of those peers (and in fact, at least one from that same majority) will participate in the next election. Because votes also require a majority, any new leader must intersect with that previous majority i.e., it must share at least one server that already contained the committed entry. As a result, the new leader’s log will contain that committed entry (or a longer version of it), ensuring that committed entries never roll back and will always be part of future leaders’ logs. This intersection property between consecutive majorities ensures Raft’s log commitment safety:\nOnce a log entry is committed, it will never be overwritten or lost, It will eventually be applied to the state machine on all servers.\nThis reasoning also explains why it’s safe for the applier thread to apply all entries up to commitIndex. The guarantee that every future leader’s log contains those entries means that applying them to the state machine can never be undone, preserving Raft’s state machine safety property, no server ever applies a log entry that another server later overwrites. However, this safety holds under one important restriction, we will discuss this restriction shortly, but first lets see few scenarios with the end client in picture using a service which is replicated with RAFT.\nLeader replication and response to client At the service layer, this guarantee extends all the way to the client. When a client issues a command to the leader (for example, a PUT request to the key-value server), the service calls the Raft instance’s Start() method. This appends the command as a new log entry and begins replication across peers (We signal the replicatorCond condition variable which resumes the replicate method this time with a new log entry). The Start() call returns immediately, but the KV service does not reply to the client yet, it waits until that log entry is replicated on a majority of servers and marked as committed. Only after the applier thread applies this entry to the state machine does the service respond to the client with success. This ensures that every command acknowledged to the client is backed by a majority-replicated, committed log entry, one that will survive even if the current leader crashes right after responding. Responding before commitment would be unsafe, since uncommitted entries can be discarded if the leader fails before majority replication completes. To visualize this flow, here’s how the end-to-end interaction between the client, leader, followers, and the KV state machine looks: sequenceDiagram participant C as Client participant L as Leader (KV + Raft) participant F1 as Follower 1 participant F2 as Follower 2 participant SM as State Machine C-\u0026gt;\u0026gt;L: PUT x=5 (Client Request) L-\u0026gt;\u0026gt;L: Start(cmd) → Append log entry L-\u0026gt;\u0026gt;F1: AppendEntries RPC (new log entry) L-\u0026gt;\u0026gt;F2: AppendEntries RPC (new log entry) F1--\u0026gt;\u0026gt;L: AppendEntries Reply (Success) F2--\u0026gt;\u0026gt;L: AppendEntries Reply (Success) Note over L: Entry replicated on majority L-\u0026gt;\u0026gt;L: Update commitIndex, signal applier L-\u0026gt;\u0026gt;SM: Apply command x=5 SM--\u0026gt;\u0026gt;L: State updated L--\u0026gt;\u0026gt;C: Success (after apply) Note over L,C: Client response only after\u0026lt;br/\u0026gt;majority replication \u0026amp; application Leader Crashes After Committing but Before Applying: sequenceDiagram participant C as Client participant L as Leader (Term T) participant F1 as Follower 1 participant F2 as Follower 2 participant L\u0026#39; as New Leader (Term T+1) C-\u0026gt;\u0026gt;L: Start(command) L-\u0026gt;\u0026gt;L: Append log entry [term T, index i] L-\u0026gt;\u0026gt;F1: AppendEntries [entry i] L-\u0026gt;\u0026gt;F2: AppendEntries [entry i] F1--\u0026gt;\u0026gt;L: Ack F2--\u0026gt;\u0026gt;L: Ack L-\u0026gt;\u0026gt;L: commitIndex = i (majority replicated) note right of L: ✅ Command is committed but not yet applied L--X C: (Crashes before replying) note over C: ⏱️ Client times out (no response) C-\u0026gt;\u0026gt;L\u0026#39;: Retry Start(command) note right of L\u0026#39;: Election happens, new leader\u0026lt;br\u0026gt;contains entry i (since committed) L\u0026#39;-\u0026gt;\u0026gt;L\u0026#39;: Apply entry i to state machine L\u0026#39;--\u0026gt;\u0026gt;C: Respond with result (deduplicated via client ID) The leader had committed the log (i.e., replicated to majority), but before applying or responding, it crashed. The client times out, since no response was received. A new leader is elected (say, L'), which must contain all committed entries from the previous term. During recovery, L'’s applier thread applies the committed command to its state machine. The client retries the command. If the system uses a unique client ID + command ID, the server can detect it’s a duplicate request and not reapply the command, ensuring exactly-once semantics. Why leader commits log entry when its of currentTerm only? You may have noticed in the replicate method that when counting majority replication we start form the lastest log index at the leader and if that log\u0026rsquo;s term is equal to leader\u0026rsquo;s current term then only we check the replication factor for that log and update commit index and applier thread in case of majority, But why is this the case?\nAbove image is form the original RAFT paper and it explain the scenario pretty well\nA time sequence showing why a leader cannot determine commitment using log entries from older terms. In (a) S1 is leader and partially replicates the log entry at index 2. In (b) S1 crashes; S5 is elected leader for term 3 with votes from S3, S4, and itself, and accepts a different entry at log index 2. In (c) S5 crashes; S1 restarts, is elected leader, and continues replication. At this point, the log entry from term 2 has been replicated on a majority of the servers, but it is not committed. If S1 crashes as in (d), S5 could be elected leader (with votes from S2, S3, and S4) and overwrite the entry with its own entry from term 3. However, if S1 replicates an entry from its current term on a majority of the servers before crashing, as in (e), then this entry is committed (S5 cannot win an election). At this point all preceding entries in the log are committed as well.\nat point (c) S1 is the leader again and it had log entry from term 2 which it was able to replicate to majority (S1, S2 \u0026amp; S3) but the current term at this point should be atleast 4 since there was already an election for term 3 with leader S5 and at point (c) S1 also appended one log for term 4. Now if at point (d) S1 crashes again S5 can again be elected for term 5 because S2, S3 and S4 will grant vote to S5 because form their point of view S5 has atleast uptodate log with lates log index \u0026lt;= theri own log and term value of 3 whihc is greater than 2. Because of this we cannot allow a log of older terms to be committed because a peer with log of term greater then the term of log replicated to majority can become a leader and as part of log correction erase this older term logs so we cannot commit such logs.\nConclusion In this second part, we successfully implemented Raft\u0026rsquo;s Log Replication to build a fault-tolerant in-memory system. We explored the core mechanics: the leader’s use of the AppendEntries RPC for both sending log entries and serving as a heartbeat, the vital log consistency checks using PrevLogIndex/PrevLogTerm, and the log reconciliation process used by followers to fix conflicts. Finally, we detailed how entries achieve a committed state—only after successful replication to a majority of peers, and are then applied to the state machine by a dedicated applier goroutine.\n6.5840 Labs provide us with following test cases for log replication:\n1Test (3B): basic agreement (reliable network)... 2 ... Passed -- time 2.1s #peers 3 #RPCs 14 #Ops 0 3Test (3B): RPC byte count (reliable network)... 4 ... Passed -- time 3.3s #peers 3 #RPCs 46 #Ops 0 5Test (3B): test progressive failure of followers (reliable network)... 6 ... Passed -- time 6.4s #peers 3 #RPCs 41 #Ops 0 7Test (3B): test failure of leaders (reliable network)... 8 ... Passed -- time 8.6s #peers 3 #RPCs 68 #Ops 0 9Test (3B): agreement after follower reconnects (reliable network)... 10 ... Passed -- time 5.4s #peers 3 #RPCs 50 #Ops 0 11Test (3B): no agreement if too many followers disconnect (reliable network)... 12 ... Passed -- time 5.2s #peers 5 #RPCs 75 #Ops 0 13Test (3B): concurrent Start()s (reliable network)... 14 ... Passed -- time 2.7s #peers 3 #RPCs 14 #Ops 0 15Test (3B): rejoin of partitioned leader (reliable network)... 16 ... Passed -- time 9.1s #peers 3 #RPCs 81 #Ops 0 17Test (3B): leader backs up quickly over incorrect follower logs (reliable network)... 18 ... Passed -- time 23.8s #peers 5 #RPCs 775 #Ops 0 19Test (3B): RPC counts aren\u0026#39;t too high (reliable network)... 20 ... Passed -- time 4.1s #peers 3 #RPCs 30 #Ops 0 In the next part we will see some nuances of log compaction with snapshots and how persistence work currently in lab environment.\nReferences https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf Log Replication Visualisation https://thesquareplanet.com/blog/students-guide-to-raft/ https://pages.cs.wisc.edu/~remzi/OSTEP/threads-cv.pdf ","permalink":"https://harshrai654.github.io/blogs/building-fault-tolerant-kv-storage-system---part-2/","summary":"\u003cp\u003eWe \u003ca href=\"/blogs/building-fault-tolerant-kv-storage-system---part-1/\"\u003epreviously discussed\u003c/a\u003e replicated state machines, leader election in the RAFT consensus algorithm, and log-based state maintenance. Now, we\u0026rsquo;ll focus on log replication across peer servers. We\u0026rsquo;ll also examine how RAFT ensures that the same commands are applied to the state machine at a given log index on every peer, because of the leader\u0026rsquo;s one-way log distribution to followers.\u003c/p\u003e\n\u003ch2 id=\"leader-initialisation\"\u003eLeader Initialisation\u003c/h2\u003e\n\u003cp\u003eOnce a candidate becomes leader we call \u003ccode\u003esetupLeader\u003c/code\u003e function which initiates go routine for each peer in the RAFT cluster, Each go routine of respective peer is responsible for replicating new log entries or sending heartbeat via \u003ccode\u003eAppendEntries\u003c/code\u003e  RPC.\u003c/p\u003e","title":"Building Fault Tolerant KV Storage System - Part 2"},{"content":"This blog post is part of a series detailing my implementation of a fault-tolerant key-value server using the RAFT consensus protocol. Before diving into RAFT and its mechanics, it\u0026rsquo;s crucial to grasp the concept of a replicated state machine and its significance in building systems that are both fault-tolerant and highly available.\nReplicated State Machine A replicated state machine is essentially a collection of identical machines working together. One machine acts as the \u0026ldquo;master,\u0026rdquo; handling client requests and dictating the system\u0026rsquo;s operations. The other machines, the \u0026ldquo;replicas,\u0026rdquo; diligently copy the master\u0026rsquo;s state. This \u0026ldquo;state\u0026rdquo; encompasses all the data necessary for the system to function correctly, remembering the effects of previous operations. Think of a key-value store: the state would be the key-value pairs themselves, replicated across all machines to maintain consistency and resilience. Having the master\u0026rsquo;s state copied across multiple machines enables us to use these replicas in several ways. They can take over as the new master if the original fails, or handle read operations to reduce the load on the master. Because the state is copied across machines, network issues like latency, packet loss, and packet reordering significantly affect how closely a replica\u0026rsquo;s state matches the master\u0026rsquo;s. The difference between the master\u0026rsquo;s (most up-to-date) state and a replica\u0026rsquo;s state is known as replication lag. Generally, we aim to minimize this lag, and different systems offer varying levels of consistency (which we will discuss later when covering replication in RAFT).\nAside from application-level replication, which primarily requires context of the state needed for replication or enforces a set of rules for the state that can be replicated, another approach involves replicating the entire machine state at the instruction level. This includes replicating instruction outputs and interrupt behaviour. This method ensures that machines execute the same set and order of instructions and maintain identical memory pages, resulting in an exact replica of the entire machine. However, this approach is more challenging to control, as managing interrupts, I/O, and replicating them to other machines is complex. An example of such an approach is discussed in The Design of a Practical System for Fault-Tolerant Virtual Machines_. This paper details the approach they followed when designing a hypervisor to capture and replicate the state of guest machines.\nRAFT - A Consensus Algorithm RAFT is a consensus algorithm for managing a replicated log. Consensus algorithms allow a collection of machines to work as a group that can survive failures of some of its members. A replicated state is generally maintained as a replicated log. Each server maintains its own copy of logs, and keeping the replicated log consistent is the job of the consensus algorithm.\nLet\u0026rsquo;s take an example of an operation done on a key-value store. A client sends a command like PUT x=2, which is received by the master server of the group. The consensus module of the server receives this command and appends it to its log. The master\u0026rsquo;s consensus module communicates with other servers about this new log and ensures that each server\u0026rsquo;s log contains the same command in the same order. Once commands are replicated, each server processes the command on its own state machine, and since the log is the same on each server, the final state on each server results in the same output. As a result, the servers appear to form a single, highly reliable state machine.\nRAFT implements this consensus between all servers by electing a leader and giving it the responsibility to decide the sequence of log operation that will be appended and propagated to each member. Flow of logs only happen in one direction from leader to other servers so if a particular server\u0026rsquo;s sequence does not match to leader\u0026rsquo;s sequence of logs, leader can instruct the follower (replica) server to erase its log and strictly follow leader itself.\nFrom the RAFT paper: Given the leader approach, Raft decomposes the consensus problem into three relatively independent subproblems, which are discussed in the subsections that follow:\nLeader election: a new leader must be chosen when an existing leader fails. Log replication: the leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with its own. Safety: if any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index. Logs, State Lifecycle and RPCs Each LogEntry consists of the command it contains and the term value. 1type LogEntry struct { 2\tCommand interface{} 3\tTerm int 4} Raft divides time into terms of arbitrary length. Terms are numbered with consecutive integers. Each term begins with an election, in which one or more candidates attempt to become leader.\nEach server has a role, with only one designated as the Leader for a specific period. The remaining servers are either Followers, who heed the Leader\u0026rsquo;s communications, or Candidates. Candidates solicit votes from other servers if they haven\u0026rsquo;t received communication from a Leader within a set timeframe.\n1const ( 2\tStateFollower ServerState = iota 3\tStateLeader 4\tStateCandidate 5) Due to variations in timing, different servers might see the changes between terms at different moments. A server might also miss an election or even entire terms. In Raft, terms serve as a logical clock, enabling servers to identify outdated information like old leaders. Every server keeps track of a current term number, which consistently increases. Servers exchange current terms during communication; if one server has a smaller term than another, it updates to the larger term. Should a candidate or leader find its term is outdated, it immediately becomes a follower. A server will reject any request that includes an old term number.\nThe diagram below illustrates how a server\u0026rsquo;s role changes under different circumstances. To grasp the role of elections and how Term serves as a time marker in RAFT, we must first understand\nHow the threshold time is determined, after which a follower can become a candidate and start an election. How each server of the cluster communicates with one another Imagine a cluster of three servers experiencing a network partition, where server 0, the current leader, cannot communicate with servers 1 and 2. If the servers have a fixed timeout value for election, multiple servers might initiate elections simultaneously, resulting in no majority each time, with each server only receiving its own vote. Consequently, the cluster cannot decide on a leader for the next term. With each re-election, a server\u0026rsquo;s term value (also known as currentTerm) increases by 1. So, when the previous server 0, from, say, term 1, rejoins the cluster after recovery, it will vote for either server 1 or 2, as both are requesting votes for their own currentTerm value, which will now be N if N elections have occurred after server 0 went down, with each server starting its own election and failing to become leader due to the lack of a majority. Even if server 0 votes for one server and a majority is achieved, the new leader needs to communicate with all peers, and if that doesn\u0026rsquo;t happen quickly, we\u0026rsquo;ll see another election. Overall, the cluster will be unstable and unable to progress due to election instability.\nTo resolve this, we introduce a small element of randomness to each server\u0026rsquo;s election timeout. This minimizes the chance of multiple servers starting elections simultaneously and maximizes the likelihood of a single server triggering a re-election and continuing as leader for subsequent terms.\n1func Make(peers []*labrpc.ClientEnd, me int, 2\tpersister *tester.Persister, applyCh chan raftapi.ApplyMsg) raftapi.Raft { 3\t... 4\t// other state initialisation, we will see them later 5\t... 6\t7\trf.electionTimeout = time.Duration(1500+(rand.Int63()%1500)) * time.Millisecond 8\tgo rf.ticker() 9\t... 10\t// other state initialisation, we will see them later 11\t... 12\t13\t} 14\t15func (rf *Raft) ticker() { 16\tfor !rf.killed() { 17\trf.mu.Lock() 18\tif rf.state != StateLeader \u0026amp;\u0026amp; time.Since(rf.lastContactFromLeader) \u0026gt;= rf.electionTimeout { 19\tgo rf.startElection() 20\t} 21\trf.mu.Unlock() 22 23\t// pause for a random amount of time between 50 and 350 24\t// milliseconds. 25\theartbeatMS := 50 + (rand.Int63() % 300) // [50, 350)ms time range 26\ttime.Sleep(time.Duration(heartbeatMS) * time.Millisecond) 27\t} 28} Each RAFT server\u0026rsquo;s state (We will discuss each state in detail later on):\n1type Raft struct { 2\tmu sync.Mutex // Lock to protect shared access to this peer\u0026#39;s state 3\tpeers []*labrpc.ClientEnd // RPC end points of all peers 4\tpersister *tester.Persister // Object to hold this peer\u0026#39;s persisted state 5\tme int // this peer\u0026#39;s index into peers[] 6\tdead int32 // set by Kill() 7 8\t// Persisted State 9\tcurrentTerm int // latest term server has seen 10\tvotedFor int // candidateId that received vote in current term 11\tlog []LogEntry // log entries; each entry contains command for state machine, and term when entry was received by leader 12\tsnapshotLastLogIndex int 13\tsnapshotLastLogTerm int 14\t// snapshot []byte 15 16\t// Volatile state 17\tcommitIndex int // index of highest log entry known to be committed 18\tlastApplied int // index of highest log entry applied to state machine 19\tstate ServerState // role of this server 20\tlastContactFromLeader time.Time // Last timestamp at which leader sent heartbeat to current server 21\telectionTimeout time.Duration // time duration since last recieved heartbeat after which election will be trigerred by this server 22\tapplyCh chan raftapi.ApplyMsg // Channel where a raft server sends it commands to be applied to state machine 23 24\t// Volatile leader state 25\tnextIndex []int //\tfor each server, index of the next log entry to send to that server 26\tmatchIndex []int //\tfor each server, index of highest log entry known to be replicated on server 27\tapplyCond *sync.Cond // Condition validable to signal applier channel to send commands to apply channel 28 29\tleaderCancelFunc context.CancelFunc // Go context for a leader, called when we need to cancel leader\u0026#39;s context and leader is stepping down 30\treplicatorCond *sync.Cond // Leader\u0026#39;s conditon variable to signal replicator threads for each peer to either send heartbeat or new logs to each peer 31} For the communication between servers, RAFT consensus algorithm uses RPC for\nRequesting vote from other peers - RequestVote RPC Propagate changes to log entries from leader to followers - AppendEntries RPC Sending heartbeats from leader to follower - AppendEntries RPC with empty log data RPC, or Remote Procedure Call, is a programming paradigm that allows a program to execute a procedure (function, method) on a different machine as if it were a local procedure call. Essentially, it\u0026rsquo;s a way to build distributed systems where one program (the client) can request a service from another program (the server) over a network. Go provides net/rpc package which abstract most of the work related to serialization of RPC arguments and deserialization at receiving end and the underlying network call with provided data, to know more check Go\u0026rsquo;s net/rpc package RPC arguments and reply structs as also shown in the original paper (page 4, fig 2)\n1type RequestVoteArgs struct { 2\tTerm int // candidate’s term 3\tCandidateId int // candidate requesting vote 4\tLastLogIndex int // index of candidate’s last log entry 5\tLastLogTerm int // term of candidate’s last log entry 6} 7 8type RequestVoteReply struct { 9\tTerm int //currentTerm, for candidate to update itself in case someone else is leader now 10\tVoteGranted bool // true means candidate received vote 11} 12 13// Invoked by leader to replicate log entries; also used as heartbeat. 14type AppendEntriesArgs struct { 15\tTerm int // leader’s term 16\tLeaderId int // so follower can redirect clients 17\tPrevLogIndex int // index of log entry immediately preceding new ones 18\tPrevLogTerm int // term of prevLogIndex entry 19\tEntries []LogEntry // log entries to store (empty for heartbeat; may send more than one for efficiency) 20\tLeaderCommit int // leader’s commitIndex 21} 22 23type AppendEntriesReply struct { 24\tTerm int // currentTerm, for leader to update itself 25\tSuccess bool // true if follower contained entry matching prevLogIndex and prevLogTerm 26\tConflictIndex int // Followers index which is conflicting with leader\u0026#39;s prevLogIndex 27\tConflictTerm int // Followers term of conflicting log 28} 29 30type InstallSnapshotArgs struct { 31\tTerm int 32\tLeaderId int 33\tLastIncludedIndex int 34\tLastIncludedTerm int 35\tData []byte 36} Rules Of Election As we have already seen, an election timeout triggers an election by a given server, which is a Follower. This timeout occurs when the Follower does not receive any AppendEntries RPCs (even empty ones, containing no log) from the leader. When an election is initiated with rf.startElection(), the follower increments its currentTerm by 1 and issues RequestVote RPCs to each of its peers in parallel, awaiting their responses. At this point, three outcomes are possible:\nThe Follower gains a majority and becomes the leader. In this case, the Follower converts to the Leader state, and setupLeader method sets up rf.replicate go routine for each of the other peers in a separate go routine. These go routines are responsible for sending heartbeats and logs using AppendEntries RPC calls, We use condition variable replicatorCond to signal these waiting go threads either when new log entry comes up or when heartbeat timeout occurs. While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be the leader. If the leader’s term (included in its RPC) is at least as large as the candidate’s current term, then the candidate recognizes the leader as legitimate and of newer term hence reverts to the follower state. If the term in the RPC is smaller than the candidate’s current term, the candidate rejects the RPC and remains in the candidate state. A candidate neither wins nor loses the election. If many followers become candidates simultaneously, votes could be split, preventing any single candidate from obtaining a majority. When this happens, each candidate will time out and start a new election by incrementing its term and initiating another round of RequestVote RPCs. The randomness in the election timeout helps prevent split votes from happening indefinitely. According to the current rules, a server receiving a RequestVote RPC with a Term greater than its own should grant its vote. However, this could lead to an outdated server with an incomplete log becoming leader. Since the leader is responsible for log propagation and can overwrite follower logs, it\u0026rsquo;s possible for such an outdated leader to erase already committed logs for which clients have received responses - an undesirable outcome. Re-examining the RequestVoteArgs reveals that, in addition to Term, the struct includes LastLogIndex and LastLogTerm, representing the candidate\u0026rsquo;s last log entry\u0026rsquo;s index and term, respectively. These values help determine if the candidate\u0026rsquo;s log contains at least the latest committed entries. The rules for verifying this when voting are straightforward: Raft determines the more up-to-date of two logs by comparing the index and term of their last entries. If the last entries have different terms, the log with the latter term is considered more up-to-date. If the logs share the same last term, the longer log is deemed more up-to-date.\nLet\u0026rsquo;s understand how RAFT prevents a stale server from winning an election and becoming a leader with the help of an example: consider a follower A that gets partitioned away from the rest of the cluster. Its election timeout fires, so it increments its term and starts an election by sending RequestVote RPCs. Since it cannot reach the majority, it doesn’t become leader. Meanwhile, the rest of the cluster still has a leader B. Because B can talk to a majority of servers, it continues to accept new log entries, replicate them, and safely commit them. Remember: in Raft, an entry is considered committed only after it is stored on a majority of servers. Later, when connectivity is restored, server A now has a higher term than B. This causes A to reject AppendEntries from B, forcing B to step down. At this moment, no leader exists until a new election is held. Here’s where Raft’s rules keep the system safe:\nA cannot win leadership election because its log is missing committed entries that the majority already agreed on. Other servers will refuse to vote for it by comparing their latest log index and term with RequestVote RPC Arg\u0026rsquo;s LastLogIndex and LastLogTerm. A new leader must have logs that are at least as up-to-date as the majority. This ensures that committed entries are never lost. Once a new leader is elected, it brings A back in sync by replicating the missing committed entries. This scenario highlights how Raft’s election rules preserve correctness: even if a partitioned follower returns with a higher term, it cannot override the majority’s progress. Leadership always ends up with a server that reflects all committed entries, and the cluster converges to the same state. In upcoming parts, we’ll dive deeper into Raft’s log replication process and how heartbeats help keep leaders and followers synchronized. Given below is the implementation for RequestVote which highlights all the restrictions and responses for various election restrictions. 1func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { 2\trf.mu.Lock() 3\tdefer rf.mu.Unlock() 4 5\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Candidate %d seeking vote for term: %d.\\n\u0026#34;, rf.me, args.CandidateId, args.Term) 6 7\t// Election voting restrictions for follower 8\t// - Candidate\u0026#39;s term is older than follower from whom it is seeking vote 9\t// - Follower already voted 10\t// - Candidate\u0026#39;s log is older then the follower 11\t// In all the above cases follower will not vote for the candidate and respond back with its current term 12\t// for the candidate to roll back to follower 13\tisCandidateOfOlderTerm := args.Term \u0026lt; rf.currentTerm 14 15\tif isCandidateOfOlderTerm { 16\treply.Term = rf.currentTerm 17\treply.VoteGranted = false 18 19\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Candidate %d is of older term. Candidate\u0026#39;s term: %d | My current term %d\\n\u0026#34;, rf.me, args.CandidateId, args.Term, rf.currentTerm) 20 21\treturn 22\t} else { 23\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Candidate %d is of newer or equal term. Candidate\u0026#39;s term: %d | My current term %d\\n\u0026#34;, rf.me, args.CandidateId, args.Term, rf.currentTerm) 24 25\tif args.Term \u0026gt; rf.currentTerm { 26\tif rf.state == StateLeader { 27\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Recieved vote request from candiate of higher term, winding up my own leadership setup.\\n\u0026#34;, rf.me) 28\tif rf.leaderCancelFunc != nil { 29\trf.leaderCancelFunc() 30\trf.replicatorCond.Broadcast() 31\t} 32\t} 33 34\trf.currentTerm = args.Term 35\trf.state = StateFollower 36\trf.votedFor = -1 37 38\trf.persist(nil) 39\t} 40 41\tcanVote := rf.votedFor == -1 || rf.votedFor == args.CandidateId 42\tvar currentLatestLogTerm int 43\tcurrentLatestLogIndex := len(rf.log) - 1 44 45\tif currentLatestLogIndex \u0026gt; 0 { 46\tcurrentLatestLogTerm = rf.log[currentLatestLogIndex].Term 47\t} else if rf.snapshotLastLogIndex \u0026gt; 0 { 48\tcurrentLatestLogTerm = rf.snapshotLastLogTerm 49\t} 50 51\tcurrentLatestLogIndex += rf.snapshotLastLogIndex 52 53\tisCandidateLogOlder := args.LastLogTerm \u0026lt; currentLatestLogTerm || (args.LastLogTerm == currentLatestLogTerm \u0026amp;\u0026amp; args.LastLogIndex \u0026lt; currentLatestLogIndex) 54 55\tif canVote \u0026amp;\u0026amp; !isCandidateLogOlder { 56\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Granted vote for term: %d, To candidate %d.\\n\u0026#34;, rf.me, args.Term, args.CandidateId) 57\trf.votedFor = args.CandidateId 58\trf.lastContactFromLeader = time.Now() 59 60\treply.VoteGranted = true 61\trf.persist(nil) 62\t} else { 63\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Candidate %d log is older than mine. Log(index/term): Candidate\u0026#39;s: (%d, %d) | Mine: (%d, %d).\\n\u0026#34;, rf.me, args.CandidateId, args.LastLogIndex, args.LastLogTerm, currentLatestLogIndex, currentLatestLogTerm) 64\treply.VoteGranted = false 65\t} 66 67\treply.Term = rf.currentTerm 68\t} 69} Here are some things to keep in mind when implementing the RequestVote RPC:\nNote the use of snapshotLastLogIndex and snapshotLastLogTerm, which relate to log compaction. Think of a snapshot as capturing the current state machine\u0026rsquo;s image, allowing us to shorten logs up to that point, reducing overall log size. We\u0026rsquo;ll explore how this works and its benefits later. For now, understand that a server, when verifying if a candidate has current logs, needs to read its own. If the log is truncated shortly after a snapshot, we store the snapshot\u0026rsquo;s last details, like the index and term of the log at that index. Snapshotting generally shortens the log, but because indexes always increase, we use snapshotLastLogIndex as an offset to get the right index. When a candidate gets a majority, it calls setupLeader, creating a context that can be cancelled, using Go\u0026rsquo;s context package. This context returns a function, leaderCancelFunc, which, when called, cancels the context. We do this when a leader steps down, such as when it receives a RequestVote RPC from a candidate with a higher term. In this case, we cancel the leader\u0026rsquo;s context. This is useful when the leader is performing async operations (like sending heartbeats or logs) and waiting for them. We then wait for the operation to complete or the context to be cancelled, signalling that we no longer need to wait because the current server is no longer the leader. We\u0026rsquo;ll see what happens when a leader\u0026rsquo;s context is cancelled later. A potentially confusing aspect is that when we receive a RequestVote RPC response denying the vote and containing a Term greater than our current one, we examine the candidate\u0026rsquo;s current state. This state might no longer be \u0026ldquo;candidate\u0026rdquo; because the RequestVote RPC could be delayed, and the node might have already gained a majority and become the leader. Despite any RPC delays, we strictly adhere to a core Raft principle: upon receiving an RPC response with a Term greater than our current Term, we immediately update our Term to match the response. If we are the leader, we step down. This rule is crucial because the Term serves as a time indicator for the entire Raft cluster. Discovering that time has progressed requires us to adapt accordingly. So to summarize: When a follower receives a RequestVote, it rejects the request if:\nThe candidate’s term is smaller (args.Term \u0026lt; rf.currentTerm). It has already voted for another candidate in this term (rf.votedFor != -1 \u0026amp;\u0026amp; rf.votedFor != args.CandidateId). The candidate’s log is less up-to-date than its own, according to Raft’s freshness rule: Candidate’s last log term must be greater, or equal with a log index at least as large. We have already seen how ticker calls startElection method based on election timeout, Let\u0026rsquo;s now understand with given below implementation of startElection, How a candidate asks for votes and what are the edge cases to consider while waiting for RequestVote RPC responses\n1func (rf *Raft) startElection() { 2\trf.mu.Lock() 3 4\t// Tigger election, send RequestVote RPC 5\t// Once you have voted for someone in a term the elction timeout should be reset 6\t// Reset election timer for self 7\trf.lastContactFromLeader = time.Now() 8 9\t// Reset the election timeout with new value 10\trf.electionTimeout = time.Duration(1500+(rand.Int63()%1500)) * time.Millisecond 11\trf.currentTerm += 1 // increase term 12\trf.state = StateCandidate 13\tpeerCount := len(rf.peers) 14 15\tvoteCount := 1 // self vote 16\tlastLogIndex := len(rf.log) - 1 17\tvar lastLogTerm int 18 19\tdone := make(chan struct{}) 20 21\tif lastLogIndex \u0026gt; 0 { 22\tlastLogTerm = rf.log[lastLogIndex].Term 23\t} else if rf.snapshotLastLogIndex \u0026gt; 0 { 24\tlastLogTerm = rf.snapshotLastLogTerm 25\t} 26 27\tlastLogIndex += rf.snapshotLastLogIndex 28 29\trf.persist(nil) 30 31\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Election timout! Initiating election for term %d, with lastLogIndex: %d \u0026amp; lastLogTerm: %d.\\n\u0026#34;, rf.me, rf.currentTerm, lastLogIndex, lastLogTerm) 32\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Election timeout reset to: %v.\\n\u0026#34;, rf.me, rf.electionTimeout) 33 34\targs := \u0026amp;RequestVoteArgs{ 35\tTerm: rf.currentTerm, 36\tCandidateId: rf.me, 37\tLastLogIndex: lastLogIndex, 38\tLastLogTerm: lastLogTerm, 39\t} 40\trequestVoteResponses := make(chan *RequestVoteReply) 41 42\tfor peerIndex, peer := range rf.peers { 43\tif peerIndex != rf.me { 44\tgo func(peer *labrpc.ClientEnd) { 45\tselect { 46\tcase \u0026lt;-done: 47\t// Either majority is achieved or candidate is stepping down as candidate 48\t// Dont wait for this peer\u0026#39;s RequestVote RPC response and exit this goroutine 49\t// to prevent goroutine leak 50\treturn 51\tdefault: 52\treply := \u0026amp;RequestVoteReply{} 53\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Requesting vote from peer: %d.\\n\u0026#34;, rf.me, peerIndex) 54\tok := peer.Call(\u0026#34;Raft.RequestVote\u0026#34;, args, reply) 55\tif ok { 56\tselect { 57\tcase requestVoteResponses \u0026lt;- reply: 58\tcase \u0026lt;-done: 59\treturn 60\t} 61\t} 62\t} 63\t}(peer) 64\t} 65\t} 66 67\t// Releasing the lock after making RPC calls 68\t// Each RPC call for RequestVote is in its own thread so its not blocking 69\t// We can release the lock after spawning RequestVote RPC thread for each peer 70\t// Before releasing the lock lets make copy of some state to verify sanity 71\t// After reacquiring the lock 72\telectionTerm := rf.currentTerm 73\trf.mu.Unlock() 74 75\tmajority := peerCount/2 + 1 76 77\tfor i := 0; i \u0026lt; peerCount-1; i++ { 78\tselect { 79\tcase res := \u0026lt;-requestVoteResponses: 80\tif rf.killed() { 81\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Candidate killed while waiting for peer RequestVote response. Aborting election process.\\n\u0026#34;, rf.me) 82\tclose(done) // Signal all other RequestVote goroutines to stop 83\treturn 84\t} 85 86\trf.mu.Lock() 87 88\t// State stale after RequestVote RPC 89\tif rf.currentTerm != electionTerm || rf.state != StateCandidate { 90\trf.mu.Unlock() 91\tclose(done) 92\treturn 93\t} 94 95\tif res.Term \u0026gt; rf.currentTerm { 96\t// A follower voted for someone else 97\t// If they voted for same term then we can ignore 98\t// But if term number is higher than our current term then 99\t// we should step from candidate to follower and update our term as well 100\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Stepping down as Candidate, Recieved RequestVoteReply with term value %d \u0026gt; %d - my currentTerm.\\n\u0026#34;, rf.me, res.Term, rf.currentTerm) 101 102\trf.currentTerm = res.Term 103\trf.state = StateFollower 104\trf.mu.Unlock() 105 106\trf.persist(nil) 107\tclose(done) 108\treturn 109\t} 110 111\tif res.VoteGranted { 112\tvoteCount++ 113\tif voteCount \u0026gt;= majority { 114\t// Won election 115\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Election won with %d/%d majority! New Leader:%d.\\n\u0026#34;, rf.me, voteCount, peerCount, rf.me) 116\trf.state = StateLeader 117 118\trf.mu.Unlock() 119\tclose(done) 120 121\trf.setupLeader() 122\treturn 123\t} 124\t} 125 126\trf.mu.Unlock() 127 128\tcase \u0026lt;-time.After(rf.electionTimeout): 129\trf.mu.Lock() 130\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Election timeout! Wrapping up election for term: %d. Got %d votes. Current state = %d. Current set term: %d.\\n\u0026#34;, rf.me, electionTerm, voteCount, rf.state, rf.currentTerm) 131\trf.mu.Unlock() 132 133\tclose(done) 134\treturn 135\t} 136 137\t} 138} Within startElection, we construct RequestVoteArgs and concurrently dispatch the RPC to every peer using separate goroutines for each call. A done channel is also provided to these goroutines to signal events such as:\nElection cancellation, occurring if the Candidate reverts to Follower status, potentially after sending initial RequestVote RPCs upon receiving a heartbeat OR a RequestVote RPC from another peer whose Term is at least the Candidate\u0026rsquo;s current Term. Cancellation of waiting for RequestVote RPC responses if a majority has been secured or the election timeout is reached. It\u0026rsquo;s worth noting that after sending RPC calls to each peer, we release the lock. This prevents us from blocking other incoming messages to that peer while awaiting RPC responses. This is why we use the done Go channel. It ensures that any concurrent request from elsewhere that modifies the candidate\u0026rsquo;s status is notified. This happens when the done channel is closed, causing the case \u0026lt;-done: statement to return first in the select block. 6.5840 Labs provide us with following test cases for leader election:\n1❯ go test -run 3A 2Test (3A): initial election (reliable network)... 3 ... Passed -- time 5.5s #peers 3 #RPCs 36 #Ops 0 4Test (3A): election after network failure (reliable network)... 5 ... Passed -- time 8.4s #peers 3 #RPCs 50 #Ops 0 6Test (3A): multiple elections (reliable network)... 7 ... Passed -- time 16.9s #peers 7 #RPCs 382 #Ops 0 8PASS 9ok 6.5840/raft1 31.352s Conclusion In this part we understood how RAFT manages a replicated log across a cluster of machines, ensuring consistency and availability. The post details the roles of leader, follower, and candidate, along with the key concepts of terms, log entries, leader election, and log replication. Crucial mechanisms, such as checks on RequestVote and AppendEntries RPCs and random timeouts, guarantee leader accuracy and prevent split votes. The post lays the groundwork for understanding how RAFT ensures that committed log entries are never lost and how a valid leader is reliably elected.\nIn subsequent parts, we will see how we set up a leader when a candidate wins election and how we handle leader stepping down from leadership. Then we will see how log replication actually happens along with cases of log conflicts and log corrections by leader and how heartbeats helps to achieve that, In the end we will trace a client request to see the behaviour of this distributed cluster seen as a single machine from client\u0026rsquo;s point of view.\nReferences https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf Leader Election Visualization https://thesquareplanet.com/blog/students-guide-to-raft/ Implementation: https://github.com/harshrai654/6.5840/blob/master/src/raft1/raft.go ","permalink":"https://harshrai654.github.io/blogs/building-fault-tolerant-kv-storage-system---part-1/","summary":"\u003cp\u003eThis blog post is part of a series detailing my implementation of a fault-tolerant key-value server using the RAFT consensus protocol.\nBefore diving into RAFT and its mechanics, it\u0026rsquo;s crucial to grasp the concept of a replicated state machine and its significance in building systems that are both fault-tolerant and highly available.\u003c/p\u003e\n\u003ch2 id=\"replicated-state-machine\"\u003eReplicated State Machine\u003c/h2\u003e\n\u003cp\u003eA replicated state machine is essentially a collection of identical machines working together. One machine acts as the \u0026ldquo;master,\u0026rdquo; handling client requests and dictating the system\u0026rsquo;s operations. The other machines, the \u0026ldquo;replicas,\u0026rdquo; diligently copy the master\u0026rsquo;s \u003cem\u003estate\u003c/em\u003e. This \u0026ldquo;state\u0026rdquo; encompasses all the data necessary for the system to function correctly, remembering the effects of previous operations. Think of a key-value store: the \u003cem\u003estate\u003c/em\u003e would be the key-value pairs themselves, replicated across all machines to maintain consistency and resilience.\nHaving the master\u0026rsquo;s state copied across multiple machines enables us to use these replicas in several ways. They can take over as the new master if the original fails, or handle read operations to reduce the load on the master. Because the state is copied across machines, network issues like latency, packet loss, and packet reordering significantly affect how closely a replica\u0026rsquo;s state matches the master\u0026rsquo;s. The difference between the master\u0026rsquo;s (most up-to-date) state and a replica\u0026rsquo;s state is known as replication lag. Generally, we aim to minimize this lag, and different systems offer varying levels of consistency (which we will discuss later when covering replication in RAFT).\u003c/p\u003e","title":"Building Fault Tolerant KV Storage System - Part 1"},{"content":"This article shares learnings from Google\u0026rsquo;s influential MapReduce paper and explores the challenges encountered while implementing a simplified version. Our system uses multiple worker processes, running on a single machine and communicating via RPC, to mimic key aspects of a distributed environment.\nWhat is Map-Reduce At its core, MapReduce is a programming model and an associated framework for processing and generating massive datasets using a parallel, distributed algorithm, typically on a cluster of computers. You might already be familiar with map and reduce operations from functional programming languages. For instance, in JavaScript, array.map() transforms each element of an array independently based on a mapper function, while array.reduce() iterates through an array, applying a reducer function to accumulate its elements into a single output value (e.g., a sum, or a new, aggregated object).\nThe MapReduce paradigm, brilliantly scales these fundamental concepts to tackle data processing challenges that are orders of magnitude larger than what a single machine can handle. The general flow typically involves several key stages:\nSplitting: The vast input dataset is initially divided into smaller, independent chunks. Each chunk will be processed by a Map task.\nMap Phase: A user-defined Map function is applied to each input chunk in parallel across many worker machines. The Map function takes an input pair (e.g., a document ID and its content) and produces a set of intermediate key/value pairs. For example, in a word count application, a Map function might take a line of text and output a key/value pair for each word, like (word, 1).\nShuffle and Sort Phase: This is a critical intermediate step. The framework gathers all intermediate key/value pairs produced by the Map tasks, sorts them by key, and groups together all values associated with the same intermediate key. This ensures that all occurrences of (word, 1) for a specific \u0026lsquo;word\u0026rsquo; are brought to the same place for the next phase.\nReduce Phase: A user-defined Reduce function then processes the grouped data for each unique key, also in parallel. The Reduce function takes an intermediate key and a list of all values associated with that key. It iterates through these values to produce a final output, often zero or one output value. Continuing the word count example, the Reduce function for a given word would receive (word, [1, 1, 1, \u0026hellip;]) and sum these ones to produce the total count, e.g., (word, total_count).\nThis distributed approach is highly effective for several reasons:\nScalability: It allows for horizontal scaling, you can process more data faster by simply adding more machines to your cluster.\nParallelism: It inherently parallelizes computation, significantly speeding up processing times for large tasks.\nFault Tolerance: The MapReduce framework is designed to handle machine failures automatically by re-executing failed tasks, which is crucial when working with large clusters where failures are common.\nThis model simplifies large-scale data processing by abstracting away the complexities of distributed programming, such as data distribution, parallelization, and fault tolerance, allowing developers to focus on the logic of their Map and Reduce functions.\nThe MapReduce Execution Flow To understand how MapReduce processes vast amounts of data, let\u0026rsquo;s walk through the typical execution flow, as illustrated in the Google paper and its accompanying diagram (Figure 1 from the paper, shown below). This flow is orchestrated by a central Master (or Coordinator, as in our lab implementation) and executed by multiple Worker processes.\nHere\u0026rsquo;s a breakdown of the key stages:\nInitialization \u0026amp; Input Splitting (Diagram: User Program forks Master, Input files split):\nThe MapReduce library first divides the input files into M smaller, manageable pieces called splits (e.g., split 0 to split 4 in the diagram). Each split is typically 16-64MB. The User Program then starts multiple copies of the program on a cluster. One copy becomes the Master, and the others become Workers. Here the binary contains logic for master and worker as part of map-reduce library. Task Assignment by Master (Diagram: Master assigns map/reduce to workers):\nThe Master is the central coordinator. It\u0026rsquo;s responsible for assigning tasks to idle workers. There are M map tasks (one for each input split) and R reduce tasks (a number chosen by the user for the desired level of output parallelism). Map Phase - Processing Input Splits (Diagram: worker (3) reads split, (4) local write):\nA worker assigned a map task reads the content of its designated input split (e.g., split 2). It parses key/value pairs from this input data. For each pair, it executes the user-defined Map function. The Map function emits intermediate key/value pairs. These intermediate pairs are initially buffered in the worker\u0026rsquo;s memory. Periodically, they are written to the worker\u0026rsquo;s local disk. Crucially, these locally written intermediate files are partitioned into R regions/files (one region/file for each eventual reduce task). This is typically done using a partitioning function (e.g., hash(intermediate_key) % R). The locations of these R partitioned files on the local disk (shown as \u0026ldquo;Intermediate files (on local disks)\u0026rdquo; in the diagram) are then reported back to the Master. The Master now knows where the intermediate data for each reduce task partition resides, spread across possibly many map workers. Reduce Phase - Aggregating Intermediate Data (Diagram: worker (5) remote read, (6) write output):\nOnce the Master sees that map tasks are completing, it begins assigning reduce tasks to other (or the same) workers. When a reduce worker is assigned a partition (say, partition j out of R), the Master provides it with the locations of all the relevant intermediate files (i.e., the j-th region/file from all map workers that produced j-th intermediate file). The reduce worker then performs remote reads from the local disks of the map workers to fetch this buffered intermediate data. After retrieving all necessary intermediate data for its assigned partition, the reduce worker sorts these key/value pairs by the intermediate key. This groups all occurrences of the same key together. (If data is too large for memory, an external sort is used). The worker then iterates through the sorted data. For each unique intermediate key, it calls the user-defined Reduce function, passing the key and the list of all associated intermediate values. The output of the Reduce function is appended to a final output file for that specific reduce partition (e.g., output file 0, output file 1). There will be R such output files. Job Completion:\nWhen all M map tasks and R reduce tasks have successfully completed, the Master signals the original User Program. The MapReduce call in the user code returns, and the results are available in the R output files. Key Design Decisions:\nAbstraction: Developers focus on Map and Reduce logic, while the framework manages distributed complexities like data partitioning, parallel execution, and shuffling. Inherent Fault Tolerance: The system is designed for resilience against common failures: The Master detects worker failures. If a worker assigned a map task fails, the task is re-assigned because its input split is durable. More subtly, if a worker completes a map task (producing intermediate files on its local disk) but then fails before all necessary reduce tasks have read those intermediate files, those files are lost. The Master must then reschedule that original map task on another worker to regenerate its intermediate output. If a worker assigned a reduce task fails, that reduce task can be re-executed by another worker. However, once a reduce task completes successfully and writes its final output (e.g., to mr-out-X), that output is considered final. The system aims to avoid re-executing successfully completed reduce tasks, relying on the durability of their output. One important aspect to note is that intermediate files are stored on the local file system of the worker nodes that produce them. This design choice is deliberate: by keeping intermediate data local, the system significantly reduces network bandwidth consumption and potential network congestion that would arise if all intermediate data had to be written to, and read from, a global file system. However, this means that crashes in map worker nodes can result in the loss of their locally stored intermediate data, requiring the re-execution of those map tasks.\nIn contrast, the final outputs of worker processes executing the reduce operation are typically written to a global, distributed file system (like GFS in Google\u0026rsquo;s case). Once a reduce task successfully writes its output to this global system, it\u0026rsquo;s considered durable and generally does not need to be re-executed, even if the worker that produced it later fails.\nImplementing MapReduce in Go: The Coordinator and Worker The Go implementation translates the conceptual MapReduce master-worker architecture into two main programs: a Coordinator and multiple Worker processes, communicating via RPC. We\u0026rsquo;ll explore the key parts of their implementation, starting with the Coordinator.\nThe Coordinator (mr/coordinator.go) The Coordinator is the central manager of the MapReduce job. Its primary role is to distribute tasks to workers, track their progress, handle failures, and determine when the overall job is complete.\nInitialization (MakeCoordinator) The MakeCoordinator function initializes the Coordinator\u0026rsquo;s state. It\u0026rsquo;s called by main/mrcoordinator.go with the input files and the number of reduce tasks (nReduce). 1// MakeCoordinator is called by main/mrcoordinator.go to create and initialize 2// the coordinator for a MapReduce job. 3// - files: A slice of input file paths for the map tasks. 4// - nReduce: The desired number of reduce tasks. 5func MakeCoordinator(files []string, nReduce int) *Coordinator { 6\t// Step 1: Initialize the list of ready Map tasks. 7\t// NewTaskList() creates a new instance of TaskList (wrapper around container/list). 8\treadyTaskList := NewTaskList() 9 10\t// For each input file, a Map task is created. 11\tfor index, file := range files { 12\treadyTaskList.AddTask(\u0026amp;Task{ // Task struct holds details for a single map or reduce operation. 13\tFilename: file, // Input file for this map task. 14\tStatus: StatusReady, // Initial status: ready to be assigned. 15\tType: MapType, // Task type is Map. 16\tId: TaskId(fmt.Sprintf(\u0026#34;m-%d\u0026#34;, index)), // Unique ID for the map task (e.g., \u0026#34;m-0\u0026#34;). 17\t}) 18\t} 19 20\t// Step 2: Initialize the Coordinator struct with its core state variables. 21\tc := Coordinator{ 22\t// --- Task Tracking --- 23\t// readyTasks: Holds tasks (initially all Map tasks, later Reduce tasks) that are 24\t// waiting to be assigned to a worker. 25\t// Managed by GetTask (removes) and ReportTask/checkWorkerStatus (adds back on failure). 26\treadyTasks: *readyTaskList, 27 28\t// runningTasks: A map from TaskId to *RunningTask. Tracks tasks currently assigned 29\t// to one or more workers. A RunningTask includes the Task details and a 30\t// list of WorkerIds processing it. 31\t// Managed by GetTask (adds) and ReportTask/checkWorkerStatus (modifies/removes). 32\trunningTasks: make(map[TaskId]*RunningTask), 33 34\t// successTasks: A map from TaskId to *Task. Stores tasks that have been successfully 35\t// completed by a worker. 36\t// Managed by ReportTask (adds on success). 37\tsuccessTasks: make(map[TaskId]*Task), 38 39\t// --- Job Parameters \u0026amp; Phase Control --- 40\t// nReduce: The target number of reduce partitions/tasks for the job. 41\t// Used by Map workers to partition intermediate data and by the Coordinator 42\t// to determine when all reduce tasks are done. 43\tnReduce: nReduce, 44 45\t// nMap: The total number of map tasks, simply the count of input files. 46\t// Used to determine when all map tasks are done. 47\tnMap: len(files), 48 49\t// pendingMappers: A counter for map tasks that are not yet successfully completed. 50\t// Crucially used in GetTask to gate the start of Reduce tasks – 51\t// Reduce tasks cannot begin until pendingMappers is 0. 52\t// Decremented in ReportTask upon successful map task completion. 53\tpendingMappers: len(files), 54 55\t// --- Intermediate Data Management --- 56\t// intermediateFiles: An IntermediateFileMap (map[string]map[WorkerId][]string). 57\t// This is vital: maps a partition key (string, for a reduce task) 58\t// to another map. This inner map links a WorkerId (of a map worker) 59\t// to a list of filenames (intermediate files produced by that map worker 60\t// for that specific partition key). 61\t// Populated in ReportTask when a Map task succeeds. 62\t// Read by GetTask to provide Reduce workers with their input locations. 63\tintermediateFiles: make(IntermediateFileMap), 64 65\t// --- Worker Tracking --- 66\t// workers: A map from WorkerId to *WorkerMetdata. Stores metadata about each worker 67\t// that has interacted with the coordinator. WorkerMetdata includes: 68\t// - lastHeartBeat: Time of the worker\u0026#39;s last contact, used by checkWorkerStatus for timeouts. 69\t// - runningTask: TaskId of the task currently assigned to this worker. 70\t// - successfulTasks: A map of tasks this worker has completed (useful for debugging/optimizations, not strictly essential for basic fault tolerance in this lab\u0026#39;s context if tasks are just re-run). 71\t// Populated/updated in GetTask and ReportTask. 72\tworkers: make(map[WorkerId]*WorkerMetdata), 73 74\t// --- Coordinator Shutdown \u0026amp; Job Completion Signaling --- 75\t// finished: A boolean flag set to true when all map and reduce tasks are successfully 76\t// completed (checked in ReportTask). Signals the main job is done. 77\tfinished: false, 78 79\t// done: A channel of empty structs (chan struct{}). Used to signal background goroutines 80\t// (like checkWorkerStatus) to terminate gracefully when the job is `finished`. 81\t// Closed in the Done() method. 82\tdone: make(chan struct{}), 83 84\t// shutdownSignaled: A boolean flag, true after `done` channel is closed. Prevents 85\t// multiple closures or redundant shutdown logic. 86\tshutdownSignaled: false, 87 88\t// allGoroutinesDone: A boolean flag, true after `wg.Wait()` in `Done()` confirms all 89\t// background goroutines have exited. 90\tallGoroutinesDone: false, 91\t// wg (sync.WaitGroup): Used in conjunction with `done` to wait for background goroutines 92\t// to complete their cleanup before the Coordinator fully exits. 93\t// Incremented before launching a goroutine, Done called in goroutine\u0026#39;s defer. 94\t// (wg is part of the Coordinator struct, initialized implicitly here) 95\t} 96 97\tfmt.Printf(\u0026#34;Initialised ready tasklist of %d tasks\\n\u0026#34;, len(files)) 98 99\t// Step 3: Start Services 100\t// Start the RPC server so the coordinator can listen for requests from workers. 101\t// This makes methods like GetTask and ReportTask callable by workers. 102\tc.server() 103 104\t// Step 4: Launch Background Health Checker Goroutine 105\t// This goroutine is responsible for fault tolerance, specifically detecting 106\t// and handling timed-out (presumed crashed) workers. 107\tc.wg.Add(1) // Increment WaitGroup counter before launching the goroutine. 108\tgo func() { 109\tdefer c.wg.Done() // Decrement counter when the goroutine exits. 110\tfor { 111\tselect { 112\tcase \u0026lt;-c.done: // Listen for the shutdown signal from the main coordinator logic. 113\tfmt.Printf(\u0026#34;[Coordinator Shutdown]: Closing worker health check background thread.\\n\u0026#34;) 114\treturn // Exit the goroutine. 115\tdefault: 116\t// Periodically call checkWorkerStatus to handle unresponsive workers. 117\tc.checkWorkerStatus() 118\t// WORKER_TIMEOUT_SECONDS is 10s, so this checks every 5s. 119\ttime.Sleep(WORKER_TIMEOUT_SECONDS / 2) 120\t} 121\t} 122\t}() 123 124\treturn \u0026amp;c // Return the initialized Coordinator instance. 125} Initially, M map tasks are created (one for each input file) and added to readyTasks. Contrary to the paper we can only run reduce tasks only when all mapper tasks are finished as input for a reduce task may require intermediate file output(s) from more than one map task since a map task produces at max R intermediate partition files, each designated to one reduce task and reduce workers needs to fetch these intermediate files from each of the mapper worker\u0026rsquo;s local file system. An RPC server (c.server()) is started for worker communication, and a background goroutine (checkWorkerStatus) is launched for fault tolerance. All shared state within the Coordinator (e.g., task lists, worker metadata) must be protected by mutexes (as seen in its methods like GetTask, ReportTask) since the shared state can be accessed by multiple go routines handling RPC calls from various workers processes which may lead to race conditions. Assigning Tasks to Workers (GetTask RPC Handler) Workers call the GetTask RPC handler to request jobs (either Map or Reduce tasks) from the Coordinator. 1// An RPC handler to find next available task (map or reduce) 2func (c *Coordinator) GetTask(args *GetTaskArgs, reply *GetTaskReply) error { 3\tc.mu.Lock() 4\tdefer c.mu.Unlock() 5 6\tworkerMetadata, ok := c.workers[args.WorkerId] 7 8\t// Requesting worker already processing a task 9\t// Skip task assignment 10\tif ok \u0026amp;\u0026amp; workerMetadata.runningTask != \u0026#34;\u0026#34; { 11\tfmt.Printf(\u0026#34;[GetTask]: Worker %d already processing task %s, rejecting task assignment request.\\n\u0026#34;, args.WorkerId, workerMetadata.runningTask) 12\treturn nil 13\t} 14 15\tif c.readyTasks.GetTaskCount() == 0 { 16\t// No tasks available 17\t// map reduce is complete if we also have len(runningTasks) == 0 18\t// Sending InvalidType task in such cases to worker 19\treply.Task = Task{ 20\tType: InvalidType, 21\t} 22\treturn nil 23\t} 24 25\ttask := c.readyTasks.RemoveTask() 26 27\t// Skipping tasks that are possible retrials with an instance already completed and part of success set 28\t// It is possible that a task here already has a status of `StatusRunning` we are not skipping such tasks in ready queue 29\t// This will result in multiple instances of same task execution, This case is possible if previous worker processing the task 30\t// failed/crashed (timeout of not reporting reached) and we added another instance of the same task. 31\t// Even if two workers report completion of same task only one of them will remove the task from running queue and add it to 32\t// success set, Reporting by slower worker will be skipped. 33 34\t// Only assing a reduce task when we are sure there is no pending map task left 35\t// Since then reduce task will surely fail because of unavailabiltiy of intermeidate fiel data 36\tfor task != nil { 37\tif task.Status == StatusSuccess || (task.Type == ReduceType \u0026amp;\u0026amp; c.pendingMappers \u0026gt; 0) { 38\tif task.Status == StatusSuccess { 39\tfmt.Printf(\u0026#34;[GetTask]: Skipping ready task %s since it is already successfully completed\\n\u0026#34;, task.Id) 40\t} else { 41\tfmt.Printf(\u0026#34;[GetTask]: Skipping reduce task %s since there are %d pending mappers\\n\u0026#34;, task.Id, c.pendingMappers) 42\t} 43\ttask = c.readyTasks.RemoveTask() 44\t} else { 45\tbreak 46\t} 47\t} 48 49\t// Either all tasks are completed (if len(runningTasks) == 0) 50\t// OR all tasks are currently being processed by some workers 51\tif task == nil { 52\treply.Task = Task{ 53\tType: InvalidType, 54\t} 55\tfmt.Printf(\u0026#34;[GetTask]: No task to assign to worker %d, # Tasks Running : %d, # Tasks Completed: %d\\n\u0026#34;, args.WorkerId, len(c.runningTasks), len(c.successTasks)) 56\treturn nil 57\t} 58 59\tfmt.Printf(\u0026#34;[GetTask]: Found a task with id %s for worker %d. Current Task Status: %v\\n\u0026#34;, task.Id, args.WorkerId, task.Status) 60 61\t// Found a task with Status as either `StatusError` or `StatusReady` or `StatusRunning` 62\t// If task\u0026#39;s status is: `StatusError`` -\u0026gt; Retrying failed task again 63\t// If task\u0026#39;s status is `StatusReady` -\u0026gt; First Attempt of processing of task 64\t// If task\u0026#39;s status is `StatusRunning` -\u0026gt; Retrying already running task due to delay from previous assigned worker 65\ttask.Worker = args.WorkerId 66\ttask.StartTime = time.Now() 67\ttask.Status = StatusRunning 68\treply.Task = *task 69 70\tif task.Type == ReduceType { 71\t// Add intermediate file locations collected from various map executions 72\treply.IntermediateFiles = c.intermediateFiles[task.Filename] 73\t} 74 75\treply.NR = c.nReduce 76 77\t// Update list of workers currently processing a taskId 78\trt := c.runningTasks[task.Id] 79 80\tif rt == nil { 81\tc.runningTasks[task.Id] = \u0026amp;RunningTask{} 82\t} 83\tc.runningTasks[task.Id].task = task 84 85\tc.runningTasks[task.Id].workers = append(c.runningTasks[task.Id].workers, args.WorkerId) 86 87\tif workerMetadata != nil { 88\tworkerMetadata.lastHeartBeat = time.Now() 89\tworkerMetadata.runningTask = task.Id 90\t} else { 91\tc.workers[args.WorkerId] = \u0026amp;WorkerMetdata{ 92\tlastHeartBeat: time.Now(), 93\trunningTask: task.Id, 94\tsuccessfulTasks: make(map[TaskId]*TaskOutput), 95\t} 96\t} 97 98\treturn nil 99} As defined in the paper\u0026rsquo;s step-2 of the execution flow this method is called by various workers to request task which are in readyTasks. It deals with scenarios like workers already being busy, no tasks being available, or tasks being unsuitable for immediate assignment (e.g., reduce tasks when mappers are still active). If a valid task is found all necessary details to execute that task are populated in GetTaskReply struct. For Map tasks, it implicitly provides the input file (via task.Filename). For Reduce tasks, it explicitly provides the locations of all relevant intermediate files and the total number of reducers (nR). Handling Task Completion/Failure (ReportTask RPC Handler) Workers call ReportTask to inform the coordinator about the outcome of their assigned task. 1func (c *Coordinator) ReportTask(args *ReportTaskArgs, reply *ReportTaskReply) error { 2\tc.mu.Lock() 3\tdefer c.mu.Unlock() 4 5\treply.Status = true // optimistic reply 6 7\ttaskSuccessInstance := c.successTasks[args.Task.Id] 8\t// ... (validation: check if task already completed, if worker owns task) ... 9 10\t// Reported task is already in success set. 11\t// Possibly retried after timeout by another worker 12\t// One of the worker complted the task. 13\tif taskSuccessInstance != nil { 14\tfmt.Printf(\u0026#34;[ReportTask]: Task %s has already been completed by worker %d\\n\u0026#34;, taskSuccessInstance.Id, taskSuccessInstance.Worker) 15\t// ... (update worker metadata) ... 16\treturn nil 17\t} 18\t19\t// ... (check if worker lost ownership of the task) ... 20 21 22\tif args.Task.Status == StatusError { 23\tfmt.Printf(\u0026#34;[ReportTask]: Task %s reported with status %v by worker %d\\n\u0026#34;, args.Task.Id, args.Task.Status, args.Task.Worker) 24\t// ... (disown worker from task) ... 25\t// If no other worker is processing this task, add it back to readyTasks 26\tif len(c.runningTasks[args.Task.Id].workers) == 0 { 27\ttask := args.Task 28\ttask.Worker = 0 29\ttask.StartTime = time.Time{} 30\ttask.Status = StatusReady 31\tc.readyTasks.AddTask(\u0026amp;task) 32\t} 33\treturn nil 34\t} 35 36\tif args.Task.Status == StatusSuccess { 37\tswitch args.Task.Type { 38\tcase MapType: 39\tintermediateFiles := args.Task.Output 40\tfmt.Printf(\u0026#34;[ReportTask]: Mapper Task %s completed successfully by worker %d, produced following intermediate files: %v\\n\u0026#34;, args.Task.Id, args.Task.Worker, intermediateFiles) 41 42\tfor _, filename := range intermediateFiles { 43\tpartitionKey := strings.Split(filename, \u0026#34;-\u0026#34;)[4] // Assumes filename format like w-\u0026lt;workerId\u0026gt;/mr-m-\u0026lt;taskId\u0026gt;-\u0026lt;hash\u0026gt; 44\tparitionFiles, ok := c.intermediateFiles[partitionKey] 45\tif !ok || paritionFiles == nil { 46\tparitionFiles = make(map[WorkerId][]string) 47\t} 48\tparitionFiles[args.Task.Worker] = append(paritionFiles[args.Task.Worker], filename) 49\tc.intermediateFiles[partitionKey] = paritionFiles 50\t} 51\t// ... (update worker metadata, move task to successTasks, decrement pendingMappers) ... 52\tdelete(c.runningTasks, args.Task.Id) 53\tc.successTasks[args.Task.Id] = \u0026amp;args.Task 54\tc.pendingMappers-- 55 56\tif c.pendingMappers == 0 { 57\tfmt.Printf(\u0026#34;\\nAll map task ran successfully. Tasks Run Details: \\n %v \\n\u0026#34;, c.successTasks) 58\tc.addReduceTasks() // Trigger creation of reduce tasks 59\t} 60\tcase ReduceType: 61\t// ... (update worker metadata, move task to successTasks) ... 62\tdelete(c.runningTasks, args.Task.Id) 63\tc.successTasks[args.Task.Id] = \u0026amp;args.Task 64 65\tif len(c.successTasks) == c.nMap+c.nReduce { 66\tfmt.Printf(\u0026#34;\\nAll reduce tasks ran successfully. Tasks Run Details: \\n %v \\n\u0026#34;, c.successTasks) 67\tc.finished = true // Mark entire job as done 68\t} 69\tdefault: 70\t// ... 71\t} 72\t// ... (logging) ... 73\t} 74\treturn nil 75} 76 77// ... (helper function addReduceTasks) 78func (c *Coordinator) addReduceTasks() { 79\tindex := 0 80\tfor partition, v := range c.intermediateFiles { // Iterate over collected partitions 81\ttask := Task{ 82\tStatus: StatusReady, 83\tType: ReduceType, 84\tId: TaskId(fmt.Sprintf(\u0026#34;r-%d\u0026#34;, index)), 85\tFilename: partition, // Partition key becomes the \u0026#39;filename\u0026#39; for the reduce task 86\t} 87\tif c.successTasks[task.Id] == nil { // Avoid re-adding if already processed (e.g. due to retries) 88\tc.readyTasks.AddTask(\u0026amp;task) 89\tfmt.Printf(\u0026#34;Reduce Task with Id %s Added to ready queue (Intermediate partition %s with %d files)\\n\u0026#34;, task.Id, partition, len(v)) 90\t} 91\tindex++ 92\t} 93\tc.nReduce = index // Update nReduce to actual number of partitions, good for robustness 94} If a task is reported with StatusError, and it\u0026rsquo;s the only instance of that task running, it\u0026rsquo;s re-queued for a later attempt. This is a core part of fault tolerance. Processes Successful Map Tasks: Collects and organizes the locations of intermediate files (output of Map functions) based on their partition key. This information (c.intermediateFiles) is vital for the subsequent Reduce phase, as it tells Reduce workers where to fetch their input data. This aligns with step 4 of the paper\u0026rsquo;s flow. Decrements pendingMappers. When all mappers are done, it triggers addReduceTasks. Once all Map tasks are complete, addReduceTasks is called. It iterates through all the unique partition keys derived from the intermediate files and creates one Reduce task for each, adding them to the readyTasks queue. Processes Successful Reduce Tasks: Marks the reduce task as complete. Checks if all Map and Reduce tasks for the entire job are finished. If so, it sets c.finished = true, signaling that the Coordinator can begin shutting down. By checking c.successTasks at the beginning, it avoids reprocessing reports for tasks already marked as successful, which helps in scenarios with duplicate or late messages. Fault Tolerance (checkWorkerStatus) A background goroutine periodically checks for unresponsive workers. 1func (c *Coordinator) checkWorkerStatus() { 2\tc.mu.Lock() 3\tdefer c.mu.Unlock() 4 5\tfor workerId, metadata := range c.workers { 6\tlastHeartBeatDuration := time.Since(metadata.lastHeartBeat) 7 8\tif metadata.runningTask != \u0026#34;\u0026#34; \u0026amp;\u0026amp; lastHeartBeatDuration \u0026gt;= WORKER_TIMEOUT_SECONDS { 9\tfmt.Printf(\u0026#34;Worker %d have not reported in last %s\\n\u0026#34;, workerId, lastHeartBeatDuration) 10\ttaskToRetry := make([]*Task, 0) 11\t12\trunningTask := c.runningTasks[metadata.runningTask] 13\tif runningTask == nil { 14\t// This case should ideally not happen if state is consistent 15\tfmt.Printf(\u0026#34;[checkWorkerStatus]: Local worker state shows worker %d running rask %s whereas global running tasks state does not show any worker for the same task.\\n\u0026#34;, workerId, metadata.runningTask) 16\t// Potentially clear worker\u0026#39;s running task if inconsistent: metadata.runningTask = \u0026#34;\u0026#34; 17\tcontinue // or return, depending on desired error handling 18\t} 19 20\ttaskToRetry = append(taskToRetry, runningTask.task) 21\tmetadata.runningTask = \u0026#34;\u0026#34; // Worker is no longer considered running this task 22 23\t// Remove this worker from the list of workers for the task 24\trunningTask.workers = slices.DeleteFunc(runningTask.workers, func(w WorkerId) bool { 25\treturn w == workerId 26\t}) 27 28\t// If this was the only worker on this task, or if we want to aggressively reschedule 29\t// (The current code implies rescheduling if *any* assigned worker times out, which is fine for this lab) 30\tif len(taskToRetry) \u0026gt; 0 { // Will always be true if runningTask was not nil 31\tfor _, task := range taskToRetry { 32\tfmt.Printf(\u0026#34;[checkWorkerStatus]: Adding task %s of type %d with status %d back to the ready queue.\\n\u0026#34;, task.Id, task.Type, task.Status) 33\t34\t// Reset task for retry 35\ttask.Status = StatusReady 36\ttask.Worker = 0 // Clear previous worker assignment 37\ttask.Output = make([]string, 0) // Clear previous output 38 39\tc.readyTasks.AddTask(task) 40\t} 41\t} 42\t} 43\t// ... (logging for active workers) ... 44\t} 45}\tKey Decisions Upon Detecting a Failed Worker:\nIdentify the Affected Task: The primary task to consider is metadata.runningTask, which the failed worker was supposed to be executing. The details of this task are retrieved from c.runningTasks. Update Worker\u0026rsquo;s State: The failed worker\u0026rsquo;s metadata.runningTask is cleared, indicating it\u0026rsquo;s no longer considered to be working on that task by the coordinator. Update Task\u0026rsquo;s Worker List: The failed workerId is removed from the runningTaskEntry.workers list, which tracks all workers assigned to that specific task ID. Reset Task for Re-execution: The affected taskInstanceToRetry undergoes several state changes: Status is set back to StatusReady, making it available in the c.readyTasks queue. Worker (assigned worker ID) is cleared. StartTime is reset. Output (list of output files) is cleared, as any partial output is now suspect or irrelevant. Re-queue the Task: The reset task is added back to c.readyTasks.AddTask(task). This ensures another worker can pick it up. Handling Lost Intermediate Data (Implicitly via Task Re-execution): A critical aspect of fault tolerance in MapReduce, as highlighted by the paper, is managing the intermediate files produced by map tasks. These are typically stored on the local disks of the map workers. If a map worker completes its task successfully but then crashes before all relevant reduce tasks have consumed its intermediate output, those intermediate files are lost. Our current checkWorkerStatus implementation primarily focuses on retrying the actively running task of a worker that times out.\n1// In checkWorkerStatus, when a worker (workerId) times out: 2// ... 3runningTask := c.runningTasks[metadata.runningTask] 4// ... 5taskToRetry = append(taskToRetry, runningTask.task) // The currently running task is added for retry 6metadata.runningTask = \u0026#34;\u0026#34; 7// ... task is reset and added back to c.readyTasks ... This handles cases where a worker fails mid-task. But what about its previously completed map tasks whose outputs are now gone?\nThe Challenge of Retrying Previously Successful Map Tasks One might initially think that upon a worker\u0026rsquo;s crash, we should re-queue all map tasks that worker had successfully completed. The following (commented-out) snippet from an earlier version of checkWorkerStatus attempted this:\n1// Original (commented-out) consideration for retrying all successful map tasks of a crashed worker: 2 3// Adding successful map tasks of this worker for retrial 4for taskId, _ := range metadata.successfulTasks { 5\tif c.successTasks[taskId] != nil \u0026amp;\u0026amp; c.successTasks[taskId].Type == MapType { 6\t// If this task was indeed in the global success set and was a MapType: 7\ttaskToRetry = append(taskToRetry, c.successTasks[taskId]) // Add it for retrial 8\tdelete(c.successTasks, taskId) // Remove from global success set 9\t// CRITICAL: We would also need to increment c.pendingMappers here if it had been decremented 10\tc.pendingMappers++ 11\t} 12} 13 14// Tombstoning metadata of intermediate files produced by this worker 15// From global state so that downstream reduce workers get to know about the failure. 16// This would ideally cause reduce tasks that depend on this worker\u0026#39;s output to fail 17// or wait, and get re-added after the map tasks are re-run. 18for _, v := range c.intermediateFiles { 19\t// Mark intermediate files from this worker (workerId) as unavailable/invalid. 20\tdelete(v, workerId) // Or v[workerId] = nil if the structure supports it 21} When a map worker crashes after successfully writing its intermediate files, those files (on its local disk) are lost in a true distributed system. Our lab setup, where all workers share the host\u0026rsquo;s filesystem, can sometimes mask this; a \u0026lsquo;crashed\u0026rsquo; worker\u0026rsquo;s files might still be accessible. This is a crucial difference from a production environment. Simply re-queuing all successfully completed map tasks from a crashed worker can be inefficient:\nPerformance Hit: It can lead to significant re-computation and potential test timeouts, especially if many map tasks were already done by a worker which crashed. Complexity: Managing pendingMappers and preventing reduce tasks from starting prematurely adds complexity if many map tasks are suddenly re-added. A More Targeted Optimization (Future Scope): A more refined approach is to only re-run a successful map task from a crashed worker if its specific output intermediate partitions are actually needed by currently pending (not yet completed) reduce tasks.\nThis involves:\nIdentifying which map tasks the crashed worker completed.\nDetermining if their output partitions are required by any active or future reduce tasks.\nOnly then, re-queueing those specific map tasks and invalidating their previous intermediate file locations.\nNot to prevent all reduce task from processing and maintain list of reduce task which should be skipped if scheduled based on lost intermediate files state from a crashed worker.\nThis smarter retry avoids redundant work but increases coordinator complexity. For our lab, focusing on retrying the currently running task of a failed worker proved sufficient to pass the tests, partly due to the shared filesystem behaviour making the storage of intermediate files also in some sense to global filesystem\nIn essence, checkWorkerStatus implements the \u0026ldquo;timeout and retry\u0026rdquo; strategy. It ensures that work assigned to unresponsive workers is not indefinitely stalled and is eventually re-assigned, which is fundamental for making progress in a distributed system prone to failures.\nJob Completion (Done) main/mrcoordinator.go periodically calls Done() to check if the entire job is finished. 1// main/mrcoordinator.go calls Done() periodically to find out 2// if the entire job has finished. 3func (c *Coordinator) Done() bool { 4\tc.mu.Lock() 5 6\t// If the job is marked as finished and we haven\u0026#39;t started the shutdown sequence for goroutines yet 7\tif c.finished \u0026amp;\u0026amp; !c.shutdownSignaled { 8\tfmt.Printf(\u0026#34;[Coordinator Shutdown]: MR workflow completed. Signaling internal goroutines to stop.\\n\u0026#34;) 9\tclose(c.done) // Signal all listening goroutines 10\tc.shutdownSignaled = true // Mark that we\u0026#39;ve signaled them 11\t} 12 13\t// If we have signaled for shutdown, but haven\u0026#39;t yet confirmed all goroutines are done 14\tif c.shutdownSignaled \u0026amp;\u0026amp; !c.allGoroutinesDone { 15\tc.mu.Unlock() 16\tc.wg.Wait() // Wait for all goroutines (like the health checker) to call c.wg.Done() 17\tc.mu.Lock() 18\tc.allGoroutinesDone = true 19\tfmt.Printf(\u0026#34;[Coordinator Shutdown]: All internal goroutines have completed.\\n\u0026#34;) 20\t} 21 22\tisCompletelyDone := c.finished \u0026amp;\u0026amp; c.allGoroutinesDone 23\tc.mu.Unlock() 24\treturn isCompletelyDone 25} The Worker (mr/worker.go) The Worker process is responsible for executing the actual Map and Reduce functions as directed by the Coordinator. Each worker operates independently, requesting tasks, performing them, and reporting back the results.\nWorker\u0026rsquo;s Main Loop (Worker function) The Worker function, called by main/mrworker.go, contains the main operational loop. 1var workerId WorkerId = WorkerId(os.Getpid()) // Unique ID for this worker process 2var dirName string = fmt.Sprintf(\u0026#34;w-%d\u0026#34;, workerId) // Worker-specific directory for temp files 3 4// ... (Log, ihash functions) ... 5 6// main/mrworker.go calls this function. 7func Worker(mapf func(string, string) []KeyValue, 8\treducef func(string, []string) string) { 9\tLog(\u0026#34;Started\u0026#34;) 10 11\t// Create a worker-specific directory if it doesn\u0026#39;t exist. 12\t// Used for storing temporary files before atomic rename. 13\tif _, err := os.Stat(dirName); os.IsNotExist(err) { 14\terr := os.Mkdir(dirName, 0755) 15\t// ... (error handling) ... 16\t} 17 18\tgetTaskargs := GetTaskArgs{ // Prepare args for GetTask RPC 19\tWorkerId: workerId, 20\t} 21 22\tfor { // Main loop: continuously ask for tasks 23\tgetTaskReply := GetTaskReply{} 24\tLog(\u0026#34;Fetching task from coordinator...\u0026#34;) 25\tok := call(\u0026#34;Coordinator.GetTask\u0026#34;, \u0026amp;getTaskargs, \u0026amp;getTaskReply) 26 27\tif ok { // Successfully contacted Coordinator 28\ttask := \u0026amp;getTaskReply.Task 29\tnReduce := getTaskReply.NR // Number of reduce partitions 30 31\tswitch task.Type { 32\tcase MapType: 33\tLog(fmt.Sprintf(\u0026#34;Assigned map job with task id: %s\u0026#34;, task.Id)) 34\tprocessMapTask(task, nReduce, mapf) // Execute the map task 35\tcase ReduceType: 36\tLog(fmt.Sprintf(\u0026#34;Assigned reduce job with task id: %s\u0026#34;, task.Id)) 37\tintermediateFiles := getTaskReply.IntermediateFiles // Get locations from Coordinator 38\tprocessReduceTask(task, intermediateFiles, reducef) // Execute reduce task 39\tdefault: // InvalidType or unknown 40\tLog(\u0026#34;Invalid task recieved or no tasks available. Sleeping.\u0026#34;) 41\t} 42 43\t// If a valid task was processed (not InvalidType), report its status 44\tif task.Type != InvalidType { 45\treportTaskArgs := ReportTaskArgs{ Task: *task } 46\treportTaskReply := ReportTaskReply{} 47\tok = call(\u0026#34;Coordinator.ReportTask\u0026#34;, \u0026amp;reportTaskArgs, \u0026amp;reportTaskReply) 48\tif !ok || !reportTaskReply.Status { 49\tLog(\u0026#34;Failed to report task or coordinator indicated an issue. Exiting.\u0026#34;) 50\t// The lab hints that if a worker can\u0026#39;t contact the coordinator, 51\t// it can assume the job is done and the coordinator has exited. 52\tremoveLocalWorkerDirectory() // Clean up worker-specific directory 53\treturn 54\t} 55\t} 56\t// Brief pause before asking for the next task. 57\ttime.Sleep(WORKER_SLEEP_DURATION) // WORKER_SLEEP_DURATION is 2s 58\t} else { // Failed to contact Coordinator 59\tLog(\u0026#34;Failed to call \u0026#39;Coordinator.GetTask\u0026#39;! Coordinator not found or exited. Exiting worker.\u0026#34;) 60\t// removeLocalWorkerDirectory() // Cleanup if needed, though not strictly required by lab on exit 61\treturn // Exit the worker process 62\t} 63\t} 64} Core Logic: Continuously polls the Coordinator for tasks (Coordinator.GetTask). Based on the task type (MapType or ReduceType), it calls the respective processing function. After processing, it reports the outcome to the Coordinator (Coordinator.ReportTask). Exit Condition: If communication with the Coordinator fails (e.g., GetTask RPC fails), the worker assumes the job is complete and the Coordinator has shut down, so the worker also exits. This is a simple shutdown mechanism compliant with the lab requirements. Local Directory: Each worker maintains a local directory (dirName like w-workerId) for its temporary files, ensuring isolation before final output naming. Processing Map Tasks (processMapTask) 1// Processes map task by fetching `Filename` from Task 2// Calls provided mapf function and stores intermediate files after 3// paritioninng them based on `ihash` function 4func processMapTask(task *Task, nReduce int, mapf func(string, string) []KeyValue) error { 5\tLog(fmt.Sprintf(\u0026#34;Processing map task with id %s and file: %s\u0026#34;, task.Id, task.Filename)) 6 7\tfile, err := os.Open(task.Filename) // Open the input split (file) 8\t// ... (error handling: set task.Status = StatusError, return) ... 9\tcontent, err := io.ReadAll(file) // Read the entire file content 10\t// ... (error handling: set task.Status = StatusError, return) ... 11\tfile.Close() 12 13\tintermediate := mapf(task.Filename, string(content)) // Execute the user-defined map function 14 15\t// Group intermediate key-value pairs by partition 16\tbuckets := make(map[int][]KeyValue) 17\tfor _, kv := range intermediate { 18\tpartition := ihash(kv.Key) % nReduce // Determine partition using ihash 19\tbuckets[partition] = append(buckets[partition], kv) 20\t} 21 22\ttask.Output = []string{} // Clear previous output, prepare for new output filenames 23 24\t// For each partition, sort and write to a temporary intermediate file 25\tfor partition, kva := range buckets { 26\t// In-memory sort for this partition\u0026#39;s KeyValue pairs. 27\t// The paper mentions external sort if data is too large, but here it\u0026#39;s in-memory. 28\tsort.Sort(ByKey(kva)) 29 30\t// Create a temporary file in the worker\u0026#39;s specific directory. 31\ttempFile, err := os.CreateTemp(dirName, \u0026#34;mwt-*\u0026#34;) // \u0026#34;mwt\u0026#34; for map worker temp 32\t// ... (error handling: set task.Status = StatusError, return) ... 33\t34\tenc := json.NewEncoder(tempFile) 35\tfor _, kv := range kva { // Write sorted KeyValue pairs to the temp file using JSON encoding 36\terr := enc.Encode(\u0026amp;kv) 37\t// ... (error handling: set task.Status = StatusError, tempFile.Close(), return) ... 38\t} 39\ttempFile.Close() // Close after writing 40 41\t// Atomically rename the temporary file to its final intermediate name. 42\t// Filename format: mr-\u0026lt;map_task_id\u0026gt;-\u0026lt;partition_number\u0026gt; (e.g., mr-m-0-1) 43\t// Stored within the worker\u0026#39;s directory: w-\u0026lt;workerId\u0026gt;/mr-m-0-1 44\tintermediateFilename := filepath.Join(dirName, fmt.Sprintf(\u0026#34;mr-%s-%d\u0026#34;, task.Id, partition)) 45\terr = os.Rename(tempFile.Name(), intermediateFilename) 46\t// ... (error handling: set task.Status = StatusError, return) ... 47\t48\ttask.Output = append(task.Output, intermediateFilename) // Add final filename to task\u0026#39;s output list 49\t} 50 51\ttask.Status = StatusSuccess // Mark task as successful 52\treturn nil 53} Core Logic: Reads the assigned input file, applies the user-defined mapf, partitions the output KeyValue pairs using ihash() % nReduce, sorts each partition\u0026rsquo;s data in memory, and writes it to a uniquely named intermediate file within its local directory. Intermediate Files: Output filenames (e.g., w-workerId/mr-mapTaskID-partitionID) are collected in task.Output. Atomic Rename: Uses os.Rename to make intermediate files visible only once fully written, preventing partial reads by reducers. This is crucial for consistency, especially if crashes occur. In-Memory Sort: A simplification for the lab; a production system might use external sorting if intermediate data for a partition is too large for memory. Processing Reduce Tasks (processReduceTask) 1func processReduceTask(task *Task, intermediateFiles map[WorkerId][]string, reducef func(string, []string) string) error { 2\tLog(fmt.Sprintf(\u0026#34;Processing reduce task with id %s for partition key %s\u0026#34;, task.Id, task.Filename)) 3 4\t// Create a temporary output file in the worker\u0026#39;s directory 5\ttempReduceFile, err := os.CreateTemp(dirName, \u0026#34;mwt-*\u0026#34;) // \u0026#34;mwt\u0026#34; for map worker temp (could be \u0026#34;rwt\u0026#34;) 6\t// ... (error handling: set task.Status = StatusError, return) ... 7\tdefer tempReduceFile.Close() // Ensure temp file is closed 8 9\tvar kva []KeyValue // To store all KeyValue pairs for this reduce partition 10 11\t// Gather all intermediate data for this reduce task\u0026#39;s partition from various map workers. 12\t// `intermediateFiles` (map[WorkerId][]string) comes from the Coordinator, 13\t// mapping map worker IDs to the list of intermediate files they produced for *this specific partition*. 14\tfor mapWorkerId, filesFromMapWorker := range intermediateFiles { 15\tfor _, filename := range filesFromMapWorker { 16\tLog(fmt.Sprintf(\u0026#34;Processing intermediate file %s from map worker %d\u0026#34;, filename, mapWorkerId)) 17\tintermediateFile, err := os.Open(filename) 18\t// ... (error handling: set task.Status = StatusError, return) ... 19\t20\tdec := json.NewDecoder(intermediateFile) 21\tfor { // Read all KeyValue pairs from this intermediate file 22\tvar kv KeyValue 23\tif err := dec.Decode(\u0026amp;kv); err != nil { 24\tif err != io.EOF { // Handle actual decoding errors 25\tLog(fmt.Sprintf(\u0026#34;Error decoding KV from intermediate file %s: %v\u0026#34;, filename, err)) 26\ttask.Status = StatusError 27\tintermediateFile.Close() 28\treturn err 29\t} 30\tbreak // EOF reached 31\t} 32\tkva = append(kva, kv) 33\t} 34\tintermediateFile.Close() 35\t} 36\t} 37 38\t// Sort all collected KeyValue pairs by key. This groups identical keys together. 39\t// This is Step 5 of the paper: \u0026#34;When a reduce worker has read all intermediate data, 40\t// it sorts it by the intermediate keys...\u0026#34; 41\t// Again, this is an in-memory sort of all data for this partition. 42\tsort.Sort(ByKey(kva)) 43 44\t// Iterate over sorted data, apply reducef for each unique key 45\ti := 0 46\tfor i \u0026lt; len(kva) { 47\tj := i + 1 48\t// Find all values for the current key kva[i].Key 49\tfor j \u0026lt; len(kva) \u0026amp;\u0026amp; kva[j].Key == kva[i].Key { 50\tj++ 51\t} 52\tvalues := []string{} 53\tfor k := i; k \u0026lt; j; k++ { 54\tvalues = append(values, kva[k].Value) 55\t} 56\t57\toutput := reducef(kva[i].Key, values) // Execute user-defined reduce function 58 59\t// Write output in the format \u0026#34;key value\\n\u0026#34; to the temporary reduce file. 60\t// This matches main/mrsequential.go and the lab\u0026#39;s expected output format. 61\tfmt.Fprintf(tempReduceFile, \u0026#34;%v %v\\n\u0026#34;, kva[i].Key, output) 62\ti = j // Move to the next unique key 63\t} 64 65\t// Atomically rename the temporary output file to its final name (e.g., mr-out-0). 66\t// The final output file is placed in the current directory (mr-tmp/ during tests), 67\t// not the worker-specific one, as it\u0026#39;s global output. 68\tfinalOutputFileName := fmt.Sprintf(\u0026#34;mr-out-%s\u0026#34;, task.Filename) // task.Filename is the partition key (e.g., \u0026#34;0\u0026#34;, \u0026#34;1\u0026#34;) 69\terr = os.Rename(tempReduceFile.Name(), finalOutputFileName) 70\t// ... (error handling: set task.Status = StatusError, return) ... 71 72\ttask.Output = []string{finalOutputFileName} // Record final output filename 73\ttask.Status = StatusSuccess 74\treturn nil 75} Core Logic: Gathers all intermediate KeyValue pairs for its assigned partition (identified by task.Filename) from the locations provided by the Coordinator (intermediateFiles). It then sorts all these KeyValue pairs together, groups them by unique key, applies the user-defined reducef for each key and its list of values, and writes the final output. Data Aggregation: Reads from multiple intermediate files (potentially from different map workers) that correspond to its specific partition. Global Sort (for the partition): All KeyValue pairs for the partition are sorted together in memory before reduction. This is essential for grouping values for the same key. Final Output: Writes output to a temporary file and then atomically renames it to the final output file name (e.g., mr-out-X), which is placed in the main job directory (not the worker\u0026rsquo;s specific temp directory). In-Memory Sort: Similar to map tasks, all data for a reduce partition is sorted in memory. Conclusion Working on this MapReduce project taught me a lot about Go’s concurrency features, how to use RPC for process communication, and how the MapReduce framework organizes big data jobs. Most importantly, I learned to think about what can go wrong in distributed systems and how to handle failures gracefully. It’s been a great hands-on way to understand the real challenges behind large-scale data processing.\nReferences https://pdos.csail.mit.edu/6.824/labs/lab-mr.html https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf https://github.com/harshrai654/6.5840/tree/lab0/src ","permalink":"https://harshrai654.github.io/blogs/map-reduce/","summary":"\u003cp\u003eThis article shares learnings from Google\u0026rsquo;s influential MapReduce paper and explores the challenges encountered while implementing a simplified version. Our system uses multiple worker processes, running on a single machine and communicating via RPC, to mimic key aspects of a distributed environment.\u003c/p\u003e\n\u003ch1 id=\"what-is-map-reduce\"\u003eWhat is Map-Reduce\u003c/h1\u003e\n\u003cp\u003eAt its core, MapReduce is a programming model and an associated framework for processing and generating massive datasets using a parallel, distributed algorithm, typically on a cluster of computers. You might already be familiar with \u003ccode\u003emap\u003c/code\u003e and \u003ccode\u003ereduce\u003c/code\u003e operations from functional programming languages. For instance, in JavaScript, \u003ccode\u003earray.map()\u003c/code\u003e transforms each element of an array independently based on a mapper function, while \u003ccode\u003earray.reduce()\u003c/code\u003e iterates through an array, applying a reducer function to accumulate its elements into a single output value (e.g., a sum, or a new, aggregated object).\u003c/p\u003e","title":"Map Reduce"},{"content":"This article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.\nBackground After introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as ioredis for interacting with our Redis-compatible database. Once we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand why this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions – options ranged from open-source SDKs (OpenTelemetry Node.js SDK) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted Grafana Tempo instance to test the setup and feasibility. (So far, the setup is working great, and I\u0026rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.\nAn example trace showing Redis commands executed while processing an HTTP request.\nOkay, back to the problem. After setting up tracing, we could visually inspect the Redis command spans, like in the example above. Correlating these trace timings with our earlier metrics confirmed the high latency numbers. Indeed, something wasn\u0026rsquo;t right with how our service was connecting to the cache server and executing commands.\nFinding the culprit Dragonfly is a Redis-compatible key-value database but with support for multithreading; Redis, on the other hand, follows a single-threaded, event-based model similar to Node.js.\nOur first step was to check if anything was wrong with the cache server deployment itself. We enabled Dragonfly\u0026rsquo;s slow query logs to check for commands taking longer than 100ms. Interestingly, we only saw SCAN commands in these logs. This didn\u0026rsquo;t immediately make sense because our high latency metrics were observed for commands like GET, SET, DELETE, and UNLINK. These are typically O(1) commands and should not take more than a few milliseconds, so we ruled out the possibility of these specific commands taking significant time to process on the cache server itself.\nTo further monitor command execution time directly on the Dragonfly server, we enabled its Prometheus metrics exporter. We looked at two metrics: \u0026ldquo;Pipeline Latency\u0026rdquo; and \u0026ldquo;Average Pipeline Length\u0026rdquo;. The \u0026ldquo;Average Pipeline Length\u0026rdquo; was always close to 0, and the \u0026ldquo;Pipeline Latency\u0026rdquo; was consistently under 10ms. While there wasn\u0026rsquo;t clear documentation from Dragonfly detailing these metrics precisely, going by the names, we assumed they represented the actual command execution time on the cache server.\nSo, the evidence suggested commands were executing quickly on the cache server (confirmed by both low Prometheus pipeline latency and the absence of GET/SET etc., in the slow query logs). But wait – the slow query logs did show the SCAN command with execution times in the range of 50ms to 200ms. So, what exactly is the SCAN command, and why were we using it?\nThe SCAN Command and Frequent Use First, what is the SCAN command? SCAN is a cursor-based command in Redis used to iterate over the keyspace. It takes a cursor position and a glob pattern, matching the pattern against keys in the database without blocking the server for long periods (unlike its predecessor, KEYS).\nIn our system, we primarily use SCAN to invalidate cache entries for specific users. We publish cache invalidation events from various parts of our application. Depending on the event type, a process triggers that uses SCAN to find and delete cache keys matching a user-specific pattern. Since these invalidation events are very frequent in our workload, the SCAN command was executed much more often than we initially realized.\nThe Restart Clue and Initial Hypotheses During our investigation, we stumbled upon a curious behavior: if we restarted the DragonflyDB instance, the high command latency would drop back to normal levels for a few hours before inevitably climbing back up to the problematic 200ms range. This provided a temporary, albeit disruptive, fix during peak hours (the cost being the loss of cached data, although we later mitigated this by enabling snapshotting for restores).\nThis temporary \u0026ldquo;fix\u0026rdquo; from restarting was a significant clue. It strongly suggested the problem wasn\u0026rsquo;t necessarily the SCAN command\u0026rsquo;s execution on the server (which slow logs and metrics already indicated was fast most of the time, except for SCAN itself sometimes), but perhaps something related to the state of the connection or interaction between our Node.js services and DragonflyDB over time.\nThis led us to two main hypotheses related to connection handling:\nConnection Pooling: ioredis, the driver we were using, maintains a single connection to the Redis server. This is standard for single-threaded Redis, where multiple connections offer little benefit. However, DragonflyDB is multi-threaded. Could our single connection be a bottleneck when dealing with frequent commands, especially potentially long-running SCAN operations, under Dragonfly\u0026rsquo;s multi-threaded architecture? Perhaps connection pooling would allow better parallel execution. Long-Running TCP Connections: Could the TCP connections themselves, after being open for extended periods, degrade in performance or enter a state that caused delays in sending commands or receiving responses? Investigating Connection Pooling To test the connection pooling hypothesis, we considered adding a pooling library like generic-pool on top of ioredis. However, we noticed that node-redis, the official Redis client for Node.js, already included built-in connection pooling capabilities and had an API largely compatible with ioredis. So, as a direct way to test the effect of pooling, we replaced ioredis with node-redis in our service.\nUnfortunately, even with node-redis and its connection pooling configured, the behavior remained the same: high latencies persisted, only dropping temporarily after a DragonflyDB restart. This seemed to rule out simple connection pooling as the solution.\nInvestigating TCP Connection State With the pooling hypothesis proving unfruitful, we turned to the idea of issues with long-running TCP connections. We tried several approaches to detect problems here:\nCode Profiling: We profiled the Node.js service during periods of high latency, generating flame graphs to see if significant time was being spent within the Redis driver\u0026rsquo;s internal methods, specifically looking for delays in writing to or reading from the underlying network socket. Packet Tracing: We used tcpdump on the service instances to capture network traffic between the Node.js service and DragonflyDB, looking for signs of network-level latency, packet loss, or retransmissions that could explain the delays. Both of these efforts came up empty. The profiling data showed no unusual delays within the driver\u0026rsquo;s socket operations, and the tcpdump analysis indicated normal network communication without significant latency.\nWe had confirmed the high frequency of SCAN, observed the strange restart behavior, and ruled out both simple connection pooling and obvious TCP-level network issues as the root cause. We needed a new hypothesis.\nA Perfect Correlation and the Root Cause We refocused on the most reliable clue: why did restarting the cache server temporarily fix the latency? We had ruled out connection management issues. The other major effect of a restart was clearing the in-memory key-value store (remember, at this stage, we weren\u0026rsquo;t restoring snapshots immediately after restarts). Plotting the number of keys in DragonflyDB over time confirmed our suspicion. We saw the key count drop to zero after each restart and then steadily climb until the next restart. Correlating this key count graph with our latency metrics revealed a clear pattern: as the number of keys rose, so did the p99 latency for our Redis commands. Although Redis/DragonflyDB can handle millions of keys, we started seeing significant latency increases once the key count grew into the 100,000–200,000 range in our specific setup. Now, which of our commands would be most affected by the number of keys? Looking at our common operations (GET, SET, DEL, UNLINK, SCAN, EXISTS), SCAN stood out. While most Redis commands have O(1) complexity, SCAN\u0026rsquo;s performance is inherently tied to the number of keys it needs to iterate over. (More details on SCAN\u0026rsquo;s behavior can be found in the official Redis documentation). We were using SCAN extensively for cache invalidation, employing code similar to this:\n1const keysToDelete = []; 2 3for await (const key of this.client.scanIterator({ 4\tMATCH: pattern, 5\tCOUNT: count, 6})) { 7\tkeysToDelete.push(key); 8} Critically, for each cache invalidation event (which were frequent), we potentially ran multiple SCAN operations, and each scanIterator loop continued until the entire relevant portion of the keyspace was traversed to find all keys matching the pattern.\nBut how could SCAN, even if sometimes slow itself (as seen in the slow logs), cause delays for fast O(1) commands like GET or SET? Our server-side metrics (like Dragonfly\u0026rsquo;s Pipeline Latency) showed quick execution times for those O(1) commands. This led to a new hypothesis: the server metrics likely measured only the actual CPU time for command execution, not the total turnaround time experienced by the client, which includes any wait time before the command gets processed.\nEven though SCAN is non-blocking, issuing a large number of SCAN commands concurrently, especially when each needs to iterate over a growing keyspace (100k-200k+ keys), could potentially overwhelm the server\u0026rsquo;s connection-handling threads (even Dragonfly\u0026rsquo;s multiple threads). If threads were busy processing numerous, longer-running SCAN iterations, incoming GET, SET, etc., commands would have to wait longer before being picked up for execution, increasing their total observed latency from the client\u0026rsquo;s perspective. The performance degradation of SCAN with more keys, combined with its high frequency, created a bottleneck that impacted all other operations.\nThe Fix: Replacing SCAN with SETs Armed with this hypothesis, the fix became clear: we needed to drastically reduce or eliminate our reliance on SCAN for finding keys to invalidate.\nWe implemented an alternative approach:\nFor each user (or entity needing cache invalidation), maintain a Redis SET containing the keys associated with that user. When an invalidation event occurs for a user, instead of scanning the keyspace, retrieve the list of keys directly from the user\u0026rsquo;s specific SET using the SMEMBERS command (which is much faster for this purpose). Delete the keys obtained from the SET. This required some additional logic (housekeeping) to keep these SETs up-to-date as new keys were cached, but the performance benefits far outweighed the complexity.\nSo we opted for a more optimal way to invalidate cache where we also stored key names related to a user in a redis SET, so the use of SCAN was not moot because we do not need to scan the namespace every time to first prepare list of keys to delete now we can get that with just a SMEMBERS command which gives list of set elements. A little housekeeping was needed to maintain this set for each user, but it still outweighs the benefits.\nThe Results: Latency Tamed This change dramatically solved the high latency issue.\nFirst, the frequency of the SCAN command dropped to nearly zero, as expected. Consequently, the latency spikes across all commands disappeared. The overall p99 latency stabilized at a much healthier level. Interestingly, even the server-side execution time reported by Dragonfly showed improvement, suggesting the reduced load from SCAN allowed other commands to be processed more efficiently internally as well. The final result was a significant drop in p99 latency, bringing it down from peaks often exceeding 200ms (and sometimes reaching ~500ms as shown below) to consistently around ~40ms. p99 latency comparison showing reduction from peaks around ~500ms down to ~40ms after the fix\nConclusion: Trust the Clues Looking back at how we tackled our high Redis command latency (~200ms+ p99), the journey involved ramping up the setup for observability, and exploring several potential culprits. While investigating connection pooling, profiling code execution, and even analyzing network packets with tcpdump were valuable exercises, they ultimately didn\u0026rsquo;t lead us to the root cause in this case.\nThe most significant clue, in hindsight, was the temporary fix we got from restarting the DragonflyDB instance. If we had focused more intently from the start on why that restart helped – realizing it pointed directly to the state accumulated within the database (specifically, the key count) – and correlated that with our application\u0026rsquo;s command usage patterns, we likely would have identified the high-frequency, full-keyspace SCAN operations as the bottleneck much sooner.\nThe real issue wasn\u0026rsquo;t low-level network glitches or basic connection handling, but an application-level pattern: frequent SCANs over a growing keyspace were overwhelming the server, increasing wait times for all commands. Switching our invalidation logic to use Redis SETs (SMEMBERS) eliminated this problematic pattern, finally bringing our p99 latency down to a stable ~40ms. Although optimizing the SCAN operation itself using [[https://redis.io/docs/latest/operate/oss_and_stack/reference/cluster-spec/#hash-tags]] was another interesting possibility (ensuring keys with the same tag land in the same hash slot to potentially limit scan scope), we didn\u0026rsquo;t opt for this solution since this required a rethink of our cache key nomenclature and would have involved substantial changes. Ultimately, the most direct path to a solution often lies in understanding the application\u0026rsquo;s behavior and trusting the most obvious clues, rather than immediately reaching for the deepest diagnostic tools.\n","permalink":"https://harshrai654.github.io/blogs/debugging-redis-latency/","summary":"\u003cp\u003eThis article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.\u003c/p\u003e\n\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eAfter introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as \u003ccode\u003eioredis\u003c/code\u003e for interacting with our Redis-compatible database.\nOnce we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand \u003cem\u003ewhy\u003c/em\u003e this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions – options ranged from open-source SDKs (\u003ca href=\"https://opentelemetry.io/docs/languages/js/\"\u003eOpenTelemetry Node.js SDK\u003c/a\u003e) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted \u003ca href=\"https://grafana.com/oss/tempo/\"\u003eGrafana Tempo\u003c/a\u003e instance to test the setup and feasibility. (So far, the setup is working great, and I\u0026rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.\u003c/p\u003e","title":"Debugging Redis Latency"},{"content":"Socket File Descriptors and Their Kernel Structures A socket is a special type of file descriptor (FD) in Linux, represented as socket:[inode]. Unlike regular file FDs, socket FDs point to in-memory kernel structures, not disk inodes. The /proc/\u0026lt;pid\u0026gt;/fd directory lists all FDs for a process, including sockets. The inode number of a socket can be used to inspect its details via tools like ss and /proc/net/tcp. Example: Checking Open FDs for Process 216 ls -l /proc/216/fd Output:\nlrwx------. 1 root root 64 Mar 2 09:01 0 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 1 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 2 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 3 -\u0026gt; \u0026#39;socket:[35587]\u0026#39; Here, FD 3 is a socket pointing to inode 35587. Checking FD Details cat /proc/216/fdinfo/3 Output: pos: 0 flags: 02 mnt_id: 10 ino: 35587 How Data Flows Through a Socket (User Space to Kernel Space) When a process writes data to a socket, it is copied from user-space memory to kernel-space buffers (using syscall write()). The kernel then processes and forwards the data to the network interface card (NIC). This copying introduces overhead, which can be mitigated using zero-copy techniques like sendfile() and io_uring. (A tweet which might recall this) TCP 3-Way Handshake (How a Connection is Established) A TCP connection is established through a 3-way handshake between the client and server:\nClient → SYN (Initial sequence number) Server → SYN-ACK (Acknowledges client’s SYN, sends its own) Client → ACK (Acknowledges server’s SYN-ACK) Checking a Listening TCP Port ss -aep | grep 35587 Output:\ntcp LISTEN 0 0 0.0.0.0:41555 0.0.0.0:* users:((\u0026#34;nc\u0026#34;,pid=216,fd=3)) ino:35587 sk:53f53fa7 Port 41555 is in the LISTEN state, bound to nc (netcat). It corresponds to socket inode 35587. TCP Connection Queues in the Kernel Once a TCP connection request arrives, it goes through two queues managed by the kernel:\n1️] SYN Queue (Incomplete Connection Queue) Holds half-open connections (received SYN but not yet fully established). If this queue overflows, new SYN requests may be dropped (SYN flood attack risk). 2]Accept Queue (Backlog Queue, Fully Established Connections) Holds connections that have completed the handshake and are waiting for accept(). Controlled by listen(sockfd, backlog), where backlog defines max queue size If full, new connections are dropped. Checking Connection Queues ss -ltni Output:\nState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 5 0.0.0.0:8080 0.0.0.0:* Recv-Q (Accept Queue, Backlog Queue) → Number of connections waiting in the backlog. Send-Q (Not relevant here) → Usually for outbound data. Checking Kernel TCP Queues via **/proc/net/tcp** cat /proc/net/tcp Output:\nsl local_address rem_address st tx_queue rx_queue tr tm-\u0026gt;when retrnsmt uid timeout inode 0: 00000000:A253 00000000:0000 0A 00000000:00000000 00:00000000 00000000 0 0 35587 1 0000000053f53fa7 100 0 0 10 0 tx_queue → Data waiting to be sent. rx_queue → Data waiting to be read. The Role of the Kernel in TCP Connections The Linux kernel manages the entire TCP stack:\nHandshaking, sequencing, retransmissions, timeouts. Maintaining connection queues \u0026amp; buffering. Interacting with the NIC for packet transmission. Applications don’t deal with raw packets directly—they only read/write to sockets, while the kernel handles the rest.\nFlow Diagram: TCP Connection Journey with Kernel Involvement Client (User Space) Kernel (Server) Application (User Space) | | | | 1. SYN | | |---------------------------\u0026gt;| | | | | | 2. SYN-ACK | | |\u0026lt;---------------------------| | | | | | 3. ACK | | |---------------------------\u0026gt;| | | | Connection Added to SYN Queue | | |-----------------------------\u0026gt;| | | | | | Connection Moved to Accept Queue | | |-----------------------------\u0026gt;| | | | | | Application Calls `accept()` | | |-----------------------------\u0026gt;| | | | | | Data Transfer Begins | Why Each Connection Needs a Separate FD When a server listens on a port, it creates a listening socket FD. When a client initiates a connection: The kernel accepts the connection using the 3-way handshake. The kernel creates a new socket structure for this connection. The server application calls accept(), which returns a new FD. Why is a New FD Required? Each TCP connection requires its own state:\nSequence numbers (to track packets in order) Receive and send buffers Connection state (e.g., established, closed) Does the Communication Happen on the Same Port?\nYes, all connections still use the same local port (the port used for listening for connection on the server side). But, each accepted connection is a unique socket with a different remote IP/port pair. The kernel distinguishes connections by:\n(Local IP, Local Port) \u0026lt;\u0026ndash;\u0026gt; (Remote IP, Remote Port). Think of it like this:\nThe listening socket is like a front desk at a hotel. Every guest (client) gets their own room (new socket), but the front desk (listening socket) stays the same. Multiple Sockets on the Same Port (SO_REUSEPORT) Allows multiple FDs bound to the same port. Kernel load-balances connections across them. Used in: Nginx, HAProxy. Example: Multi-Threaded Server with SO_REUSEPORT int sock1 = socket(AF_INET, SOCK_STREAM, 0); int sock2 = socket(AF_INET, SOCK_STREAM, 0); setsockopt(sock1, SOL_SOCKET, SO_REUSEPORT, \u0026amp;opt, sizeof(opt)); setsockopt(sock2, SOL_SOCKET, SO_REUSEPORT, \u0026amp;opt, sizeof(opt)); bind(sock1, ...); bind(sock2, ...); listen(sock1, BACKLOG); listen(sock2, BACKLOG); References https://chatgpt.com/c/67c414f3-4830-8013-a058-0fd2596e3c07 ","permalink":"https://harshrai654.github.io/blogs/socket-file-descriptor-and-tcp-connections/","summary":"\u003ch2 id=\"socket-file-descriptors-and-their-kernel-structures\"\u003eSocket File Descriptors and Their Kernel Structures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003esocket\u003c/strong\u003e is a special type of file descriptor (FD) in Linux, represented as \u003ccode\u003esocket:[inode]\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eUnlike regular file FDs, socket FDs point to \u003cstrong\u003ein-memory kernel structures\u003c/strong\u003e, not disk inodes.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003e/proc/\u0026lt;pid\u0026gt;/fd\u003c/code\u003e directory lists all FDs for a process, including sockets.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003einode number\u003c/strong\u003e of a socket can be used to inspect its details via tools like \u003ccode\u003ess\u003c/code\u003e and \u003ccode\u003e/proc/net/tcp\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"example-checking-open-fds-for-process-216\"\u003eExample: Checking Open FDs for Process \u003ccode\u003e216\u003c/code\u003e\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003els -l /proc/216/fd\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eOutput:\u003c/strong\u003e\u003c/p\u003e","title":"Socket File Descriptor and TCP connections"},{"content":"Overall Organization Of Data In Disks Assuming we have a 256KB disk.\nDisk Blocks: The basic units of storage on the disk, each 4 KB in size. The disk is divided into these blocks, numbered from 0 to N-1 (where N is the total number of blocks). Inode Bitmap (i): Block 1; a bitmap tracking which inodes are free (0) or in-use (1). Data Bitmap (d): Block 2; a bitmap tracking which data blocks are free (0) or allocated (1). Inode Table (I): Blocks 3-7; an array of inodes, where each inode (256 bytes) holds metadata about a file, like size, permissions, and pointers to data blocks. 5 blocks of 4KB will contain 80 256 byte inode strutures. Data Region (D): Blocks 8-63; the largest section, storing the actual contents of files and directories. Inode Every inode has a unique identifier called an inode number (or i-number). This number acts like a file’s address in the file system, allowing the operating system to quickly locate its inode. For example:\nIn a system with 80 inodes, numbers might range from 0 to 79. Conventionally, the root directory is assigned inode number 2 (numbers 0 and 1 are often reserved or used for special purposes). The inode number is the key to finding a file’s metadata on the disk, and it’s stored in directory entries alongside the file’s name.\nHow Do We Jump to the Disk Block for a Specific Inode Number? In a file system like vsfs, inodes are stored consecutively in an inode table, a reserved area of the disk (e.g., spanning blocks 3 to 7). Each inode has a fixed size—let’s say 256 bytes—and the disk is divided into blocks (e.g., 4096 bytes each). To locate a specific inode given its i-number, we calculate its exact position on the disk. Here’s how:\nIdentify the Inode Table’s Starting Block: Suppose the inode table starts at block 3. Calculate the Block Containing the Inode: Formula: block = (i-number * inode_size) / block_size + start_block Example: For inode 10, inode_size = 256 bytes, block_size = 4096 bytes, start_block = 3 (10 * 256) / 4096 = 2560 / 4096 = 0.625 → integer part is 0. block = 0 + 3 = 3. So, inode 10 is in block 3. Calculate the Offset Within the Block: Formula: offset = (i-number * inode_size) % block_size Example: (10 * 256) % 4096 = 2560 % 4096 = 2560 bytes. Inode 10 starts 2560 bytes into block 3. Result: The operating system reads block 3 from the disk and jumps to offset 2560 bytes to access inode 10’s metadata. This process allows the file system to efficiently retrieve an inode’s information and, from there, its data blocks. Since multiple processes may have file descriptors for the same file opened with their own data of offset to read from, multiple processes will be accessing the same inode structure to read about file and modify its data (Here inodes data would mean modifying things like last accessed time or adding another entry for list of data blocks etc), So i-node needs some sort of concurrency control in place. (More on this)\nHow does inode know the data it owns An inode is a data structure in a file system that stores information about a file, including where its data is located on the disk. Instead of holding the file’s data itself, the inode contains pointers that reference the disk blocks where the data is stored.\nDisk Blocks: These are fixed-size chunks of storage on the disk (e.g., 4 KB each) that hold the actual file content. Pointers: These are entries in the inode that specify the locations of these disk blocks. The inode uses these pointers to keep track of all the blocks that make up a file, allowing the file system to retrieve the data when needed.\nWhat Are Disk Addresses? Disk addresses are the identifiers that tell the file system the exact physical locations of data blocks on the disk. Think of them as a map: each address corresponds to a specific block, such as block number 100, which might map to a particular sector and track on a hard drive.\nFor example, if a file is 8 KB and the block size is 4 KB, the inode might have two pointers with disk addresses like \u0026ldquo;block 50\u0026rdquo; and \u0026ldquo;block 51,\u0026rdquo; pointing to the two blocks that hold the file’s data. How Does an Inode Manage Disk Blocks? The inode organizes its pointers in a way that can handle files of different sizes efficiently. It uses a combination of direct pointers and indirect pointers, forming a multi-level indexing structure.\n1. Direct Pointers The inode starts with a set of direct pointers, which point straight to the disk addresses of data blocks. Example: If the block size is 4 KB and the inode has 12 direct pointers, it can directly address 12 × 4 KB = 48 KB of data. 2. Indirect Pointers (Multi-Level Indexing) For files too big for direct pointers alone, the inode uses indirect pointers, which point to special blocks that themselves contain more pointers. This creates a hierarchical, or multi-level, structure.\nSingle Indirect Pointer\nThis pointer points to a block (called an indirect block) that contains a list of disk addresses to data blocks.\nExample: If a block is 4 KB and each pointer is 4 bytes, the indirect block can hold 4 KB / 4 bytes = 1024 pointers. That’s 1024 × 4 KB = 4 MB of additional data.\nTotal with Direct: With 12 direct pointers and 1 single indirect pointer, the file can reach (12 + 1024) × 4 KB = 4,096 KB (about 4 MB).\nDouble Indirect Pointer\nThis pointer points to a block that contains pointers to other single indirect blocks, each of which points to data blocks.\nExample: The double indirect block might hold 1024 pointers to single indirect blocks. Each of those holds 1024 pointers to data blocks, so that’s 1024 × 1024 = 1,048,576 data blocks, or about 4 GB with 4 KB blocks.\nTotal with Direct and Single: (12 + 1024 + 1,048,576) × 4 KB ≈ 4 GB.\nThis structure acts like an imbalanced tree: small files use only direct pointers, while larger files use additional levels of indirection as needed.\nWhy Use Multi-Level Indexing? The multi-level indexing structure is designed to balance efficiency and scalability:\nSmall Files: Most files are small, so direct pointers handle them quickly without extra overhead. Large Files: Indirect pointers allow the inode to scale up to support massive files by adding more layers of pointers. How It Works in Practice When the file system needs to find a specific block in a file:\nIt checks the inode’s direct pointers first. If the block number is beyond the direct pointers’ range, it follows the single indirect pointer to the indirect block and looks up the address there. For even larger block numbers, it traverses the double indirect or triple indirect pointers, following the hierarchy until it finds the right disk address. This process ensures the file system can efficiently locate any block, no matter how big the file is.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-implementation.pdf https://grok.com/chat/d429de0e-556f-426c-84bd-21ff1b0c4002 (Contains reference to actual inode structure implementation along with locks it uses on Linux) https://grok.com/chat/c86e3040-2f86-4aa9-a317-1c0a464564a3?referrer=website ","permalink":"https://harshrai654.github.io/blogs/file-system-implementation/","summary":"\u003ch2 id=\"overall-organization-of-data-in-disks\"\u003eOverall Organization Of Data In Disks\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eAssuming we have a 256KB disk\u003c/em\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDisk Blocks\u003c/strong\u003e: The basic units of storage on the disk, \u003cem\u003eeach 4 KB in size.\u003c/em\u003e The disk is divided into these blocks, numbered from 0 to N-1 (where N is the total number of blocks).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInode Bitmap (i)\u003c/strong\u003e: Block 1; a bitmap tracking which inodes are free (0) or in-use (1).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Bitmap (d)\u003c/strong\u003e: Block 2; a bitmap tracking which data blocks are free (0) or allocated (1).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInode Table (I)\u003c/strong\u003e: Blocks 3-7; an array of inodes, where each inode (256 bytes) holds metadata about a file, like size, permissions, and pointers to data blocks.\n5 blocks of 4KB will contain 80 256 byte inode strutures.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Region (D)\u003c/strong\u003e: Blocks 8-63; the largest section, storing the actual contents of files and directories.\n\u003cimg alt=\"Pasted image 20250301204506.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20250301204506.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"inode\"\u003eInode\u003c/h2\u003e\n\u003cp\u003eEvery inode has a unique identifier called an \u003cstrong\u003einode number\u003c/strong\u003e (or \u003cstrong\u003ei-number\u003c/strong\u003e). This number acts like a file’s address in the file system, allowing the operating system to quickly locate its inode. For example:\u003c/p\u003e","title":"Understanding Inodes and Disk Layout"},{"content":"Files and directories File systems virtualize persistent storage (e.g., hard drives, SSDs) into user-friendly files and directories, adding a third pillar to OS abstractions (processes for CPU, address spaces for memory).\nFile Paths and System Calls Files are organized in a tree-like directory structure, starting from the root (/). A file’s location is identified by its pathname (e.g., /home/user/file.txt). To interact with files, processes use system calls:\nopen(path, flags): Opens a file and returns a file descriptor (fd). read(fd, buffer, size): Reads data from the file into a buffer using the fd. write(fd, buffer, size): Writes data to the file via the fd. close(fd): Closes the file, freeing the fd. File Descriptors A file descriptor is a small integer, unique to each process, that identifies an open file. When a process calls open(), the operating system assigns it the next available fd (e.g., 3, 4, etc.). Every process starts with three default fds:\n0: Standard input (stdin) 1: Standard output (stdout) 2: Standard error (stderr) Quick Note: File Descriptors \u0026amp; Terminals Every process starts with three standard file descriptors:\n0 (stdin), 1 (stdout), 2 (stderr). Where They Point By default they link to a terminal device (e.g., /dev/pts/0 for Terminal 1, /dev/pts/1 for Terminal 2). These are character devices with a major number (e.g., 136) for the tty driver and a unique minor number (e.g., 0 or 1) for each instance.\nCommand Output (Terminal 1):\n1ls -l /proc/self/fd 2lrwx------ 1 runner runner 64 Feb 23 11:47 0 -\u0026gt; /dev/pts/0 3lrwx------ 1 runner runner 64 Feb 23 11:47 1 -\u0026gt; /dev/pts/0 4lrwx------ 1 runner runner 64 Feb 23 11:47 2 -\u0026gt; /dev/pts/0 Device Details:\n1ls -l /dev/pts/0 2crw--w---- 1 runner tty 136, 0 Feb 23 11:53 /dev/pts/0 3 4~/workspace$ stat /dev/pts/0 5File: /dev/pts/0 6Size: 0 Blocks: 0 IO Block: 1024 character special file 7Device: 0,1350 Inode: 3 Links: 1 Device type: 136,0 8Access: (0620/crw--w----) Uid: ( 1000/ runner) Gid: ( 5/ tty) 9Access: 2025-02-23 12:13:52.419852946 +0000 10Modify: 2025-02-23 12:13:52.419852946 +0000 11Change: 2025-02-23 11:36:28.419852946 +0000 12 Birth: - Versus a File A regular file descriptor (e.g., for test.txt) points to a disk inode with data blocks, tied to a filesystem device (e.g., /dev/sda1), not a driver.\nExample:\n1ls -li test.txt 212345 -rw-r--r-- 1 runner runner 5 Feb 23 12:00 test.txt 3 4~/workspace$ stat test.txt 5 File: test.txt 6 Size: 288 Blocks: 8 IO Block: 4096 regular file 7Device: 0,375 Inode: 1111 Links: 1 8Access: (0644/-rw-r--r--) Uid: ( 1000/ runner) Gid: ( 1000/ runner) 9Access: 2025-02-23 11:40:57.014322027 +0000 10Modify: 2025-02-23 11:41:41.800233516 +0000 11Change: 2025-02-23 11:41:41.800233516 +0000 12 Birth: 2025-02-23 11:40:57.014322027 +0000 Fun Test Redirected Terminal 1’s output to Terminal 2:\nCommand: echo \u0026quot;Hello\u0026quot; \u0026gt; /dev/pts/1 Result: \u0026ldquo;Hello\u0026rdquo; appeared in Terminal 2 (/dev/pts/1, minor 1)! Open File Table The Open File Table (OFT) is a system-wide structure in kernel memory that tracks all open files. Each entry in the OFT includes:\nThe current offset (position for the next read/write). Permissions (e.g., read, write). A reference count (if multiple processes share the file). Each process has its own array of file descriptors, where each fd maps to an entry in the OFT. For example, process A’s fd 3 and process B’s fd 4 might point to the same OFT entry if they’ve opened the same file.\nExample: Showing entries of a PID in open file table\n1~/workspace$ lsof -p 113 2COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME 3bash 113 runner cwd DIR 0,375 394 256 /home/runner/workspace 4bash 113 runner rtd DIR 0,1324 4096 1562007 / 5bash 113 runner 0u CHR 136,0 0t0 3 /dev/pts/0 6bash 113 runner 1u CHR 136,0 0t0 3 /dev/pts/0 7bash 113 runner 2u CHR 136,0 0t0 3 /dev/pts/0 8bash 113 runner 255u CHR 136,0 0t0 3 /dev/pts/0 As you see above that the process had 0,1 and 2 FDs as well as FD for a directory as well.\nReference counting in OFT Reference counting is a technique used in operating systems to manage entries in the Open File Table (OFT), which stores information about open files shared by processes or file descriptors.\nHow It Works Each OFT entry has a reference count that tracks how many file descriptors (from one or more processes) refer to it. When a file is opened, the reference count increases by 1. If the same file is reused (e.g., via fork() or dup()), the count increments without creating a new entry. When a file descriptor is closed (e.g., with close(fd)), the count decreases by 1. The OFT entry is removed only when the reference count reaches 0, meaning no process is using it. How It Helps Remove Entries Reference counting ensures an OFT entry is deleted only when it’s no longer needed. For example, after a fork(), both parent and child processes share the same OFT entry (reference count = 2). Closing the file in one process lowers the count to 1, but the entry persists until the second process closes it, bringing the count to 0.\nWhy It’s Useful This method efficiently manages shared file resources, preventing premature removal of file metadata (like the current offset) while any process still needs it.\nINode What is an Inode? An inode (short for \u0026ldquo;index node\u0026rdquo;) is a fundamental data structure in UNIX-based file systems. It stores essential metadata about a file, enabling the system to manage and locate files efficiently. Each file in the system is uniquely identified by its inode number (also called i-number). The metadata stored in an inode includes:\nFile size: The total size of the file in bytes.\nPermissions: Access rights defining who can read, write, or execute the file.\nOwnership: User ID (UID) and group ID (GID) of the file\u0026rsquo;s owner.\nTimestamps:\nLast access time (when the file was last read). Last modification time (when the file\u0026rsquo;s content was last changed). Last status change time (when the file\u0026rsquo;s metadata, like permissions, was last modified). Pointers to data blocks: Locations on disk where the file\u0026rsquo;s actual content is stored.\nThe inode does not store the file\u0026rsquo;s name or its content; these are managed separately. The inode\u0026rsquo;s role is to provide a compact and efficient way to access a file\u0026rsquo;s metadata. Each inode is stored in inode block in disk.\nHow the stat System Call Works The stat system call is used to retrieve metadata about a file without accessing its actual content. It provides a way for programs to query information like file size, permissions, and timestamps. Here\u0026rsquo;s how it works:\nInput: The stat system call takes a file path as input. Locate the inode: The file system uses the file path to find the corresponding inode number. (Insert link to FS implemetation note here explaining how FS searches inode and data block from given file path) Retrieve metadata: The inode is fetched from disk (or from a cache, if available, for faster access). Populate the struct stat buffer: The metadata stored in the inode is copied into a struct stat buffer, which contains fields for file size, permissions, ownership, timestamps, and more. Return to the user: The struct stat buffer is returned to the calling program, providing all the metadata for the file. Because the stat system call only accesses the inode and not the file\u0026rsquo;s content, it is a fast and efficient operation. This separation of metadata (stored in the inode) and content (stored in data blocks) allows the system to quickly retrieve file information without unnecessary disk I/O.\nDirectories ls -al Output for Directories\n1total 152 2drwxr-xr-x 1 runner runner 394 Feb 23 11:40 . 3drwxrwxrwx 1 runner runner 46 Feb 23 11:36 .. 4-rw-r--r-- 1 runner runner 174 Feb 15 11:31 .breakpoints 5drwxr-xr-x 1 runner runner 34 Feb 15 11:32 .cache 6drwxr-x--- 1 runner runner 260 Feb 15 11:31 .ccls-cache 7-rw-r--r-- 1 runner runner 1564 Dec 21 11:44 common_threads.h 8-rwxr-xr-x 1 runner runner 16128 Dec 21 11:54 deadlock 9-rw-r--r-- 1 runner runner 647 Dec 21 11:51 deadlock.c 10-rwxr-xr-x 1 runner runner 16160 Dec 21 11:57 deadlock-global 11-rw-r--r-- 1 runner runner 748 Dec 21 11:57 deadlock-global.c 12-rw-r--r-- 1 runner runner 6320 Feb 23 11:36 generated-icon.png 13-rw-r--r-- 1 runner runner 429 Mar 8 2024 .gitignore 14-rwxr-xr-x 1 runner runner 15984 Dec 21 11:51 main 15-rw-r--r-- 1 runner runner 415 Dec 21 11:51 main.c 16-rwxr-xr-x 1 runner runner 16816 Aug 16 2024 main-debug 17-rw-r--r-- 1 runner runner 411 Dec 12 2023 Makefile 18-rwxr-xr-x 1 runner runner 16136 Dec 14 11:04 mem 19-rw-r--r-- 1 runner runner 1437 Aug 16 2024 .replit 20-rw-r--r-- 1 runner runner 134 Feb 23 13:54 replit.nix 21-rwxr-xr-x 1 runner runner 15936 Dec 21 11:59 signal 22-rw-r--r-- 1 runner runner 329 Dec 21 11:59 signal.c 23-rw-r--r-- 1 runner runner 288 Feb 23 11:41 test.txt 24drwxr-xr-x 1 runner runner 42 Feb 15 11:32 wcat Size of a directory only means storage needed to store each directory entry which basically comprise entry name and its inode number along with some other metdata\nDirectory Data Block Contents The data blocks of a directory contain:\nDirectory entries: Mapping of file/subdirectory names to their inode numbers. Special entries: . (current directory) and .. (parent directory). Optimization for Small Directories For small directories, some file systems store directory entries directly in the inode itself, avoiding separate data blocks. This optimization saves space and speeds up access.\nReading Directory Entries with System Calls To programmatically read directory contents following sys calls are used:\nopendir(path): Opens the directory. readdir(dir): Reads one entry at a time (returns a struct dirent with name and inode number. closedir(dir): Closes the directory. Permission bits and their octal representation Format: ls -l displays permissions as rwxr-xr-x:\nFirst character: d (directory), - (file), or l (symlink). Next 9 bits: Three groups of rwx for owner, group, and others: r = read, w = write, x = execute (run for files, enter for directories). Converting a Full 9-Bit Permission to Numeric (Octal) Representation Example: rwxr-xr-x\nBreakdown: Split into three groups (owner, group, others): Owner: rwx. Group: r-x. Others: r-x. Step-by-Step Conversion\nOwner: rwx: r = 1, w = 1, x = 1 → Binary: 111. Decimal: 1×4 + 1×2 + 1×1 = 4 + 2 + 1 = 7. Group: r-x: r = 1, w = 0, x = 1 → Binary: 101. Decimal: 1×4 + 0×2 + 1×1 = 4 + 1 = 5. Others: r-x: r = 1, w = 0, x = 1 → Binary: 101. Decimal: 1×4 + 0×2 + 1×1 = 4 + 1 = 5. Final Octal Representation\nCombine the three digits: 7 5 5. Result: rwxr-xr-x = 755. Quick Recall: Split rwx into 3 groups, convert each to binary (1s/0s), sum (4, 2, 1), get octal (e.g., 755). Hard Links, Symbolic Links (Including Size), and File Deletion (Based on OSTEP PDF) Hard Links and unlink Definition: A hard link (PDF p. 18-19) is an extra directory entry pointing to the same inode, created with link(old, new).. When a directory reference its parent directory (with ..) it increases hard link count by 1. For a new empty directory its hard count is always 2 since it is referred by itself (with .) and also referred by its parent\u0026rsquo;s directory entry (with ..) With unlink: Removes a name-to-inode link, decrements the inode’s link count. When count hits 0 (and no processes use it), the inode and data blocks are freed from disk. Symbolic Links and Dangling References Definition: A symbolic (soft) link (PDF p. 20-21) is a distinct file storing a pathname to another file, created with ln -s old new. Size: Its size equals the length of the stored pathname (e.g., 4 bytes for \u0026ldquo;file\u0026rdquo;, 15 bytes for \u0026ldquo;alongerfilename\u0026rdquo;; PDF p. 21). How It Works: References the path, not the inode directly; accessing it resolves the path. Dangling Reference: If the target is deleted (e.g., unlink file), the symbolic link persists but points to nothing, causing errors (e.g., \u0026ldquo;No such file or directory\u0026rdquo;). Disk, Partition, Volume, File System Hierarchy, and Mounting Distinction and Hierarchy Disk: Physical storage device (e.g., hard drive, SSD) holding raw data blocks. Partition: A subdivided section of a disk (e.g., /dev/sda1), treated as a separate unit. Volume: A logical abstraction, often a partition or group of partitions, formatted with a file system (FS). File System: Software structure (e.g., ext3, AFS) organizing data into files and directories on a volume. Hierarchy: Disk → Partitions → Volumes → File Systems.\nMounting Process Creation: Use mkfs to format a partition with a file system (e.g., mkfs -t ext3 /dev/sda1 creates an empty FS). Mounting: The mount command (PDF p. 24) attaches a file system to the directory tree at a mount point (e.g., mount -t ext3 /dev/sda1 /home/users), making its contents accessible under that path. Multiple File Systems on One Machine A single machine can host multiple file systems by mounting them at different points in the tree (PDF p. 26). Example Output (from mount): 1/dev/sda1 on / type ext3 (rw) # Root FS 2/dev/sda5 on /tmp type ext3 (rw) # Separate tmp FS 3AFS on /afs type afs (rw) # Distributed FS How: Each partition or volume gets its own FS type and mount point, unified under one tree (e.g., /). Quick Recall: Disk splits into partitions; volumes get FS; mount glues them into a tree; multiple FS coexist at different paths.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-intro.pdf https://grok.com/share/bGVnYWN5_47ab49d6-aa1d-4de1-9bbe-0d47332e12fe https://grok.com/share/bGVnYWN5_7db77c97-d33d-4543-9993-d5aa362c8b2b ","permalink":"https://harshrai654.github.io/blogs/files-and-directories/","summary":"\u003ch1 id=\"files-and-directories\"\u003eFiles and directories\u003c/h1\u003e\n\u003cp\u003eFile systems virtualize persistent storage (e.g., hard drives, SSDs) into user-friendly files and directories, adding a third pillar to OS abstractions (processes for CPU, address spaces for memory).\u003c/p\u003e\n\u003ch2 id=\"file-paths-and-system-calls\"\u003eFile Paths and System Calls\u003c/h2\u003e\n\u003cp\u003eFiles are organized in a \u003cstrong\u003etree-like directory structure\u003c/strong\u003e, starting from the root (/). A file’s location is identified by its \u003cstrong\u003epathname\u003c/strong\u003e (e.g., /home/user/file.txt). To interact with files, processes use \u003cstrong\u003esystem calls\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eopen(path, flags)\u003c/strong\u003e: Opens a file and returns a \u003cstrong\u003efile descriptor\u003c/strong\u003e (fd).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eread(fd, buffer, size)\u003c/strong\u003e: Reads data from the file into a buffer using the fd.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ewrite(fd, buffer, size)\u003c/strong\u003e: Writes data to the file via the fd.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eclose(fd)\u003c/strong\u003e: Closes the file, freeing the fd.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"file-descriptors\"\u003eFile Descriptors\u003c/h2\u003e\n\u003cp\u003eA \u003cstrong\u003efile descriptor\u003c/strong\u003e is a small integer, unique to each process, that identifies an open file. When a process calls open(), the operating system assigns it the next available fd (e.g., 3, 4, etc.). Every process starts with three default fds:\u003c/p\u003e","title":"Files And Directories"},{"content":"RAID Disks Three axes on which disks are analysed\nCapacity - How much capacity is needed to store X bytes of data Reliability - How much fault-tolerant is the disk Performance - Read and write speeds (Sequential and random) To make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification) When we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database In a way this set of disks also have challenges similar to distributes databases.\nThere are different types of data arrangement in set of physical disks which results in different types/levels of RAID\nRAID Level 0 - Striping Disk 0 Disk 1 Disk 2 Disk 3 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Here each cell is a disk block which will be of fixed size (for example 4KB) A vertical column shows blocks stored by a single disk\nStriping: Writing out chunks (in multiple of disk block size) to each disk, one at a time so that we have the data spread uniformly across the disk. When read or write requests comes up to disk it comes in the form of stripe number (row number in above illustration) and based on RAID level disk controller knows which block it has to access and which disk to read it from.\nTradeoffs:\nThis level has no reliability as a disk failure always means loss of some data. This arrangement is good for capacity as we are utilizing all disks for storage. RAID Level 1 - Mirroring Disk 0 Disk 1 Disk 2 Disk 3 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 Copies of blocks made to two disks, tradeoff of reliability over capacity. Consistent update problem Imagine the write is issued to the RAID, and then the RAID decides that it must be written to two disks, disk 0 and disk 1. The RAID then issues the write to disk 0, but just before the RAID can issue the request to disk 1, a power loss (or system crash) occurs. In this unfortunate case, let us assume that the request to disk 0 completed (but clearly the request to disk 1 did not, as it was never issued). The result of this untimely power loss is that the two copies of the block are now inconsistent; the copy on disk 0 is the new version, and the copy on disk 1 is the old. What we would like to happen is for the state of both disks to change atomically, i.e., either both should end up as the new version or neither. The general way to solve this problem is to use a write-ahead log of some kind to first record what the RAID is about to do (i.e., update two disks with a certain piece of data) before doing it. By taking this approach, we can ensure that in the presence of a crash, the right thing will happen; by running a recovery procedure that replays all pending transactions to the RAID, we can ensure that no two mirrored copies (in the RAID-1 case) are out of sync. One last note: because logging to disk on every write is prohibitively expensive, most RAID hardware includes a small amount of non-volatile RAM (e.g., battery-backed) where it performs this type of logging. Thus, consistent update is provided without the high cost of logging to disk.\nTradeoffs:\nThis level can tolerate up to N/2 disk failures. This arrangement is good for reliability over cost of capacity being half Even though updates for a block needs to happen at two separate disks, The write would be parallel but still slower than updating a single disk (If we consider different seek and rotational time for both disks) RAID Level 4 - Parity Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 4 5 6 7 P1 8 9 10 11 P2 12 13 14 15 P3 N - 1 Disks follow striping with the Nth disk containing parity block for each strip row Concept: RAID 4 adds redundancy to a disk array using parity, which consumes less storage space than mirroring. How it works:\nA single parity block is added to each stripe of data blocks. The parity block stores redundant information calculated from the data blocks in its stripe. The XOR function is used to calculate parity. XOR returns 0 if there are an even number of 1s in the bits, and 1 if there are an odd number of 1s. The parity bit ensures that the number of 1s in any row, including the parity bit, is always even. Example of Parity Calculation: Imagine a RAID 4 system with 4-bit data blocks. Let\u0026rsquo;s say we have the following data in one stripe:\nBlock 0: 0010 Block 1: 1001 Block 2: 0110 Block 3: 1011 To calculate the parity block, we perform a bitwise XOR across the corresponding bits of each data block:\nBit 1 (from left): 0 XOR 1 XOR 0 XOR 1 = 0 Bit 2: 0 XOR 0 XOR 1 XOR 0 = 1 Bit 3: 1 XOR 0 XOR 1 XOR 1 = 1 Bit 4: 0 XOR 1 XOR 0 XOR 1 = 0 Therefore, the parity block would be 0110.\nData recovery:\nIf a data block is lost, the remaining blocks in the stripe, including the parity block, are read. The XOR function is applied to these blocks to reconstruct the missing data. For example, if Block 2 (0110) was lost, we would XOR Block 0, Block 1, Block 3, and the parity block: 0010 XOR 1001 XOR 1011 XOR 0110 = 0110. Performance:\nRAID 4 has a performance cost due to the overhead of parity calculation. Crucially, all write operations must update the parity disk. Write Bottleneck: If multiple random write requests comes for various blocks at the same time then they all will all require to update different parity blocks but all parity blocks are in one disk so multiple updates to different parity block will be done one after the other because all are in the same disk which eventually makes concurrent random write requests sequential in nature this is also known as small-write problem RAID Level 5 - Rotating Parity Instead of having one dedicated disk for parity blocks for each stripe, distribute the parity blocks in rotating manner to all disks for each stripe.\nDisk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 4 5 6 P1 7 8 9 P2 10 11 12 P3 13 14 15 Small-write: Concurrent random write requests to blocks of different blocks of different stripes can now be done parallelly since parity block for each stripe will be in different disk. It is still possible that blocks of different stripes need to update parity blocks which are lying in same disk (due to rotating nature of parity block) write requests to blocks of same stripes will still be sequential since parity block will be on same disk for all the blocks of same stripe. In summary: While RAID 5 significantly improves random write performance compared to RAID 4, it doesn\u0026rsquo;t completely eliminate the possibility of parity-related bottlenecks. The rotated parity distribution reduces the likelihood of contention, but it doesn\u0026rsquo;t guarantee that parity updates will always be fully parallel. The chance of multiple stripes\u0026rsquo; parity residing on the same disk is still there, leading to potential performance degradation.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf How Data recovery happens with parity drive in RAID: https://blogs.oracle.com/solaris/post/understanding-raid-5-recovery-with-elementary-school-math#:~:text=We%20need%20to%20read%20all%20date%20from%20other%20drives%20to%20recovery%20parity.\n","permalink":"https://harshrai654.github.io/blogs/raid-redundant-array-of-inexpensive-disk/","summary":"\u003ch1 id=\"raid-disks\"\u003eRAID Disks\u003c/h1\u003e\n\u003cp\u003eThree axes on which disks are analysed\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCapacity - How much capacity is needed to store X bytes of data\u003c/li\u003e\n\u003cli\u003eReliability - How much fault-tolerant is the disk\u003c/li\u003e\n\u003cli\u003ePerformance - Read and write speeds (Sequential and random)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification)\nWhen we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database\nIn a way this set of disks also have challenges similar to distributes databases.\u003c/p\u003e","title":"RAID (Redundant array of inexpensive disk)"},{"content":"Segmented Page Table Page table can grow large for a 32-bit address space and 4 KB page size we will be using 20 bits for virtual page number resulting in 2^20 bytes (i.e. 4MB of page table) for a single page table and each process will have its own page table so it is possible that we will be storing ~100sMB for page table alone which is not good. For above page table with 4 bits for VPN (Virtual page number) we can see that only VPN 0,4,14 and 15 are valid i.e. pointing to a PFN (Physical Frame Number) other PTEs (Page table entry) are just taking up space which is not used. We can use segmentation here with base and bound registers for each page table to only store valid PTE in the table. This will again split the virtual address to also contain the segment bits to identify which segment the address belongs to (code, heap or stack). Instead of using Base Page Table Register to query page table we will now be using Base Page Table Register [Segment] to get page table physical address for a given segment.\nSN = (VirtualAddress \u0026amp; SEG_MASK) \u0026gt;\u0026gt; SN_SHIFT VPN = (VirtualAddress \u0026amp; VPN_MASK) \u0026gt;\u0026gt; VPN_SHIFT AddressOfPTE = Base[SN] + (VPN * sizeof(PTE)) ^3c2fde\nThis way we can place contents of a process\u0026rsquo;s page table at different locations in memory for different segments and avoiding storing of invalid PTEs.\nMultilevel Page Table Segmented page table still can suffer from space wastage if we have sparse usage of heap and can cause external fragmentation since now size of a page table segment can be different (multiple of PTE size) and finding free space for variable sized page table can be difficult.\nIdea of Multilevel page table is to group page table entries into size of a page and ignore those group of PTE where each entry is invalid Like in above figure PFN 202 and 203 contain all entries as invalid and with multilevel page table we do not require to store PTE inside such pages. Now we would have an indirection where we will first refer page directory and then the actual page of the page table to get physical frame number of a given virtual address. So in a way we now have page table for the actual page table called page directory Lets assume we have 14 bit address space with 4 byte PTE with 64 Byte page size. VPN will be of 8 bits, which means a linear page table will contain 256 PTE each of size 4 byte resulting in 1KB page table (4 * 256). A 1KB page table requires 16 pages of size of 32 bytes so we can group this page table intro 16 different segments which will be now addressed by page table directory. To address 16 segments we need 4 bits so now we will be using first 4 bits of virtual address for page directory index.\nYou can see the similarity with segmentation here with the only difference being we are now creating segments of page size, so everything will be in multiple of page sizes so we won\u0026rsquo;t be facing external fragmentation + We do not need to know the number of segments beforehand like in segmentation with code, heap and stack. Similar to how we were calculating AddressOfPTE [[#^3c2fde]] in linear page table now we will first calculate AddressOfPDE (Page Directory Entry) as\nPDEAddr = PageDirBase + (PDIndex * sizeof(PDE)). Now we are storing page directory base address in register\nOnce we get PDE Address from there we can get the page address of required PTE. Similar to concatenating page offset in linear page table now we will be concatenating rest of the bits after page directory index on virtual address (Page Table Index or Page offset of page-table\u0026rsquo;s page) with PDEAddress to get the physical address of the page table entry.\nPTEAddr = (PDE.PFN \u0026lt;\u0026lt; SHIFT) + (PTIndex * sizeof(PTE)) Once we get the PTE address we can concatenate the physical address inside the entry with the page offset in virtual address to finally get the physical address of the given virtual address.\nSummary As you can see the tradeoff here is indirection to save memory space. Indirection basically means more number of memory access. For a given virtual address now we need following memory accesses:\nAccess memory to fetch page directory address Access memory to fetch page table entry address from page directory index Finally access memory for given overall virtual address As you can see with a TLB miss [[2- Source Materials/Books/OSTEP/TLB#^3bbded]] now we have 2 extra memory access overhead but after this miss we will have cache hit for this virtual address next time, and we will translate virtual address directly into physical address without any memory access. Overall A bigger table such as linear page table means faster service in case of TLB miss (lesser memory access) and reducing page table with indirection means slower TLB miss service. References https://pages.cs.wisc.edu/~remzi/OSTEP/vm-smalltables.pdf\n","permalink":"https://harshrai654.github.io/blogs/multilevel-page-table/","summary":"\u003ch1 id=\"segmented-page-table\"\u003eSegmented Page Table\u003c/h1\u003e\n\u003cp\u003ePage table can grow large for a 32-bit address space and 4 KB page size we will be using 20 bits for virtual page number resulting in 2^20 bytes (i.e. 4MB of page table) for a single page table and each process will have its own page table so it is possible that we will be storing ~100sMB for page table alone which is not good.\n\u003cimg alt=\"Pasted image 20241127093849.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241127093849.png\"\u003e\nFor above page table with 4 bits for VPN (Virtual page number) we can see that only VPN 0,4,14 and 15 are valid i.e. pointing to a PFN (Physical Frame Number) other PTEs (Page table entry) are just taking up space which is not used.\nWe can use segmentation here with base and bound registers for each page table to only store valid PTE in the table.\n\u003cimg alt=\"Pasted image 20241127094506.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241127094506.png\"\u003e\nThis will again split the virtual address to also contain the segment bits to identify which segment the address belongs to (code, heap or stack). Instead of using \u003cem\u003eBase Page Table Register\u003c/em\u003e to query page table we will now be using \u003cem\u003eBase Page Table Register [Segment]\u003c/em\u003e to get page table physical address for a given segment.\u003c/p\u003e","title":"Multilevel Page table"},{"content":"TLB Translation look-aside buffer is a CPU cache which is generally small but since it is closer to CPU a TLB hit results in address translation to happen in 1-5 CPU cycles.\nCPU Cycle Time taken by CPU to fully execute an instruction, while CPU frequency refers to the number of these cycles that occur per second\nA TLB hit means for given virtual address the physical frame number was found in the TLB cache. A TLB hit will benefit all the address that lie on the same page. In the above given image page size is 16 bytes, so 4 INT variables can be saved in a single page, so a TLB hit of VPN 07 will serve address translation for VPN = 07 + page of offset of 0, 4,8 and 12 byte. This type of caching is benefitted from spatial locality of data where a cache hit results in cache hits for surrounding data as well. If we cache data and other data points which are more probable to get accessed in the same time frame (like loop variables etc) then such caching is benefitted from Temporal locality.\nSoftware (OS) handled TLB miss ^3bbded\nWhen a TLB miss happens\nGenerate a trap Trap handler for this trap is OS code. This trap handler will find the translation of virtual address from page table stored in memory A privileged operation to update the TLB Return to trap with PC updated to same instruction which generated the trap. (Usually return to trap updates PC to next instruction address) Context Switch and TLB A naive approach is to flush the TLB (by setting valid bit of the page table entry to 0) so that the next yet to run process has a clean TLB, but this can slow things down since after every context switch new process will have few TLB misses, and we have high frequency of context switches then the performance may degrade. Another approach is to use same TLB for multiple processes with a Address space identifier (ASID, similar to PID but small) in the page table entry signifying the process to which a page table entry belongs Using cache replacement policies like LRU for page table entries References https://pages.cs.wisc.edu/~remzi/OSTEP/vm-tlbs.pdf\n","permalink":"https://harshrai654.github.io/blogs/tlb/","summary":"\u003ch1 id=\"tlb\"\u003eTLB\u003c/h1\u003e\n\u003cp\u003eTranslation look-aside buffer is a CPU cache which is generally small but since it is closer to CPU a TLB hit results in address translation to happen in 1-5 CPU cycles.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCPU Cycle\nTime taken by CPU to fully execute an instruction, while CPU frequency refers to the number of these cycles that occur per second\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eA TLB hit means for given virtual address the physical frame number was found in the TLB cache. A TLB hit will benefit all the address that lie on the same page.\n\u003cimg alt=\"Pasted image 20241120223520.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241120223520.png\"\u003e\nIn the above given image page size is 16 bytes, so 4 INT variables can be saved in a single page, so a TLB hit of VPN 07 will serve address translation for VPN = 07 + page of offset of 0, 4,8 and 12 byte.\nThis type of caching is benefitted from \u003cem\u003e\u003cstrong\u003espatial locality\u003c/strong\u003e\u003c/em\u003e of data where a cache hit results in cache hits for surrounding data as well.\nIf we cache data and other data points which are more probable to get accessed in the same time frame (like loop variables etc) then such caching is benefitted from \u003cem\u003e\u003cstrong\u003eTemporal locality.\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e","title":"TLB"},{"content":"Page Tables Page table contains the translation information of virtual page number to physical frame number. For an address space of 32 bits and page size of 4 KB (i.e. memory of 2^32 is divided into segments of 4 KB where each segment is called a memory page) , The virtual address will be of size 32 bits of which 12 bits (2^12 = 4 KB) will be used as offset inside a single page whereas remaining 20 bits will be used as virtual page number\nExample Memory Access As it can be seen from the above memory access flow, translating and accessing a virtual page address to actual physical frame address requires 2 memory access which can be slow when the process assumed that it is making only one memory access to fetch data or instruction from memory.\nPage table size is directly proportional to address space size Page table size is inversely proportional to a page size References OSTEP\n","permalink":"https://harshrai654.github.io/blogs/page-tables/","summary":"\u003ch1 id=\"page-tables\"\u003ePage Tables\u003c/h1\u003e\n\u003cp\u003ePage table contains the translation information of virtual page number to physical frame number.\nFor an address space of 32 bits and page size of 4 KB \u003cem\u003e(i.e. memory of 2^32 is divided into segments of 4 KB where each segment is called a memory page)\u003c/em\u003e , The virtual address will be of size 32 bits of which 12 bits (2^12 = 4 KB) will be used as offset inside a single page whereas remaining 20 bits will be used as virtual page number\u003c/p\u003e","title":"Page Tables"},{"content":"References 5.6 Problem Generally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:\nIf the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading If the operation is a write operation then there are again two possibilities: Optimistic Locking If the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it\u0026rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify ^ab3c53 Pessimistic Locking But if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking Pessimistic locking involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf\u0026rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff\nNow there is a glaring problem with pessimistic locking, even if the restructuring is limited to the leaf node and its direct parent or neighbours only then also we are taking a write latch on the root restricting potential read operations resulting in slow reads\n8.0 Solution Introduction to SX Lock (Write lock is called X lock - Exclusive lock | Read lock is called S lock - Shared lock)\n- SX LOCK does not conflict with S LOCK but does conflict with X LOCK. SX Locks also conflict with each other. - The purpose of an SX LOCK is to indicate the intention to modify the protected area, but the modification has not yet started. Therefore, the resource is still accessible, but once the modification begins, access will no longer be allowed. Since an intention to modify exists, no other modifications can occur, so it conflicts with X LOCKs.\nSX locks are kind of like X lock among themselves but are like S locks when used with S locks, Now with SX locks are held in following manner.\nREAD OPERATION: We take S lock as before on the root node, but we also take S locks on the internal nodes till the leaf nodes WRITE OPERATION: If the write operation is just going to modify the leaf page we still use Optimistic Locking [[#^ab3c53]] But if the write operation involves restructuring of the tree then instead of taking a write lock on the root as discussed in Pessimistic locking [[#^17a3ff]] We take SX lock on the root and same X locks on leaf node and its direct parent and neighbour, This still allows S locks to be taken in root node to allow other read operation which are not having the same path as the ongoing write operation But still prevents another SX lock on root node by some other write operation. So we can see that SX lock\u0026rsquo;s introduction helps in increasing the read throughput but still has the problem of global contention on write operation even if the writes are not going to happen in the ongoing another write operation. I think a root level latch is under the pessimistic guess of modification of write bubbling up to root which can conflict with another write operation even if not in the same path as bubbling up to the root will impact all the branches, butt the question is do all write operations bubble up to root and if not is it wise to take a root level of SX lock also to prevent other write operations. The answer lies in another type of lock mechanism called Latch Coupling or Latch Crabbing.\nReferences Inno DB B-Tree Latch Optimisation\n","permalink":"https://harshrai654.github.io/blogs/b-tree-latch-optimisation/","summary":"\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\u003ch2 id=\"56-problem\"\u003e5.6 Problem\u003c/h2\u003e\n\u003cp\u003eGenerally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading\u003c/li\u003e\n\u003cli\u003eIf the operation is a write operation then there are again two possibilities:\n\u003cul\u003e\n\u003cli\u003e\n\u003ch3 id=\"optimistic-locking\"\u003eOptimistic Locking\u003c/h3\u003e\nIf the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it\u0026rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify\n\u003cimg alt=\"Pasted image 20241117123300.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241117123300.png\"\u003e ^ab3c53\u003c/li\u003e\n\u003cli\u003e\n\u003ch3 id=\"pessimistic-locking\"\u003ePessimistic Locking\u003c/h3\u003e\nBut if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking\n\u003cimg alt=\"Pasted image 20241117123407.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241117123407.png\"\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003ePessimistic locking\u003c/strong\u003e involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf\u0026rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff\u003c/p\u003e","title":"B-Tree Latch Optimisation"}]