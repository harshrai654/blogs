<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Memory on Learning Loop</title>
    <link>https://harshrai654.github.io/blogs/tags/memory/</link>
    <description>Recent content in Memory on Learning Loop</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Oct 2025 00:48:38 +0530</lastBuildDate>
    <atom:link href="https://harshrai654.github.io/blogs/tags/memory/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Multipart Form Uploads - Busboy and Node Streams</title>
      <link>https://harshrai654.github.io/blogs/multipart-form-uploads---busboy-and-node-streams/</link>
      <pubDate>Sun, 12 Oct 2025 00:48:38 +0530</pubDate>
      <guid>https://harshrai654.github.io/blogs/multipart-form-uploads---busboy-and-node-streams/</guid>
      <description>&lt;p&gt;I was recently investigating ways to improve the efficiency of file uploads to a Node.js server. This need arose after encountering a production bug where the absence of a maximum file size limit for uploads led to an out-of-memory crash due to file buffers consuming excessive heap memory. In this Node.js server, I was using Express and &lt;code&gt;express-openapi-validator&lt;/code&gt; to document the server&amp;rsquo;s API with an &lt;code&gt;OpenAPI&lt;/code&gt; specification. &lt;code&gt;express-openapi-validator&lt;/code&gt; utilizes &lt;code&gt;multer&lt;/code&gt; for file uploads. I had previously encountered this library whenever file uploads from forms needed to be handled in Node.js, but I never questioned why a separate library was necessary for file uploads. This time, I decided to go deeper to understand if a dedicated package for file uploads is truly needed, and if so, what specific benefits &lt;code&gt;Multer&lt;/code&gt; or similar libraries provide.
I initially needed to find a configuration option in &lt;code&gt;express-openapi-validator&lt;/code&gt; to set a request-wide limit on the maximum size (in bytes) of data allowed in a request, including all file attachments.Â  The &lt;code&gt;express-openapi-validator&lt;/code&gt; package offers a &lt;code&gt;fileUploader&lt;/code&gt; configuration (&lt;a href=&#34;https://cdimascio.github.io/express-openapi-validator-documentation/usage-file-uploader/&#34;&gt;fileUploader documentation&lt;/a&gt;) that passes options directly to &lt;code&gt;multer&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multilevel Page table</title>
      <link>https://harshrai654.github.io/blogs/multilevel-page-table/</link>
      <pubDate>Tue, 26 Nov 2024 20:29:34 +0530</pubDate>
      <guid>https://harshrai654.github.io/blogs/multilevel-page-table/</guid>
      <description>&lt;h1 id=&#34;segmented-page-table&#34;&gt;Segmented Page Table&lt;/h1&gt;
&lt;p&gt;Page table can grow large for a 32-bit address space and 4 KB page size we will be using 20 bits for virtual page number resulting in 2^20 bytes (i.e. 4MB of page table) for a single page table and each process will have its own page table so it is possible that we will be storing ~100sMB for page table alone which is not good.
&lt;img alt=&#34;Pasted image 20241127093849.png&#34; loading=&#34;lazy&#34; src=&#34;https://harshrai654.github.io/blogs/media/pasted-image-20241127093849.png&#34;&gt;
For above page table with 4 bits for VPN (Virtual page number) we can see that only VPN 0,4,14 and 15 are valid i.e. pointing to a PFN (Physical Frame Number) other PTEs (Page table entry) are just taking up space which is not used.
We can use segmentation here with base and bound registers for each page table to only store valid PTE in the table.
&lt;img alt=&#34;Pasted image 20241127094506.png&#34; loading=&#34;lazy&#34; src=&#34;https://harshrai654.github.io/blogs/media/pasted-image-20241127094506.png&#34;&gt;
This will again split the virtual address to also contain the segment bits to identify which segment the address belongs to (code, heap or stack). Instead of using &lt;em&gt;Base Page Table Register&lt;/em&gt; to query page table we will now be using &lt;em&gt;Base Page Table Register [Segment]&lt;/em&gt; to get page table physical address for a given segment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>TLB</title>
      <link>https://harshrai654.github.io/blogs/tlb/</link>
      <pubDate>Wed, 20 Nov 2024 22:26:06 +0530</pubDate>
      <guid>https://harshrai654.github.io/blogs/tlb/</guid>
      <description>&lt;h1 id=&#34;tlb&#34;&gt;TLB&lt;/h1&gt;
&lt;p&gt;Translation look-aside buffer is a CPU cache which is generally small but since it is closer to CPU a TLB hit results in address translation to happen in 1-5 CPU cycles.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;CPU Cycle
Time taken by CPU to fully execute an instruction, while CPU frequency refers to the number of these cycles that occur per second&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;A TLB hit means for given virtual address the physical frame number was found in the TLB cache. A TLB hit will benefit all the address that lie on the same page.
&lt;img alt=&#34;Pasted image 20241120223520.png&#34; loading=&#34;lazy&#34; src=&#34;https://harshrai654.github.io/blogs/media/pasted-image-20241120223520.png&#34;&gt;
In the above given image page size is 16 bytes, so 4 INT variables can be saved in a single page, so a TLB hit of VPN 07 will serve address translation for VPN = 07 + page of offset of 0, 4,8 and 12 byte.
This type of caching is benefitted from &lt;em&gt;&lt;strong&gt;spatial locality&lt;/strong&gt;&lt;/em&gt; of data where a cache hit results in cache hits for surrounding data as well.
If we cache data and other data points which are more probable to get accessed in the same time frame (like loop variables etc) then such caching is benefitted from &lt;em&gt;&lt;strong&gt;Temporal locality.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Page Tables</title>
      <link>https://harshrai654.github.io/blogs/page-tables/</link>
      <pubDate>Sun, 17 Nov 2024 20:22:15 +0530</pubDate>
      <guid>https://harshrai654.github.io/blogs/page-tables/</guid>
      <description>&lt;h1 id=&#34;page-tables&#34;&gt;Page Tables&lt;/h1&gt;
&lt;p&gt;Page table contains the translation information of virtual page number to physical frame number.
For an address space of 32 bits and page size of 4 KB &lt;em&gt;(i.e. memory of 2^32 is divided into segments of 4 KB where each segment is called a memory page)&lt;/em&gt; , The virtual address will be of size 32 bits of which 12 bits (2^12 = 4 KB) will be used as offset inside a single page whereas remaining 20 bits will be used as virtual page number&lt;/p&gt;</description>
    </item>
    <item>
      <title>B-Tree Latch Optimisation</title>
      <link>https://harshrai654.github.io/blogs/b-tree-latch-optimisation/</link>
      <pubDate>Sun, 17 Nov 2024 12:08:00 +0530</pubDate>
      <guid>https://harshrai654.github.io/blogs/b-tree-latch-optimisation/</guid>
      <description>&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;h2 id=&#34;56-problem&#34;&gt;5.6 Problem&lt;/h2&gt;
&lt;p&gt;Generally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading&lt;/li&gt;
&lt;li&gt;If the operation is a write operation then there are again two possibilities:
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;optimistic-locking&#34;&gt;Optimistic Locking&lt;/h3&gt;
If the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it&amp;rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify
&lt;img alt=&#34;Pasted image 20241117123300.png&#34; loading=&#34;lazy&#34; src=&#34;https://harshrai654.github.io/blogs/media/pasted-image-20241117123300.png&#34;&gt; ^ab3c53&lt;/li&gt;
&lt;li&gt;
&lt;h3 id=&#34;pessimistic-locking&#34;&gt;Pessimistic Locking&lt;/h3&gt;
But if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking
&lt;img alt=&#34;Pasted image 20241117123407.png&#34; loading=&#34;lazy&#34; src=&#34;https://harshrai654.github.io/blogs/media/pasted-image-20241117123407.png&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Pessimistic locking&lt;/strong&gt; involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf&amp;rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
