<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Learning Loop</title><link>https://harshrai654.github.io/blogs/</link><description>Recent content on Learning Loop</description><generator>Hugo -- 0.145.0</generator><language>en-us</language><lastBuildDate>Sun, 06 Apr 2025 14:21:54 +0530</lastBuildDate><atom:link href="https://harshrai654.github.io/blogs/index.xml" rel="self" type="application/rss+xml"/><item><title>Debugging Redis Latency</title><link>https://harshrai654.github.io/blogs/debugging-redis-latency/</link><pubDate>Sun, 06 Apr 2025 14:21:54 +0530</pubDate><guid>https://harshrai654.github.io/blogs/debugging-redis-latency/</guid><description>&lt;p>This article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>After introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as &lt;code>ioredis&lt;/code> for interacting with our Redis-compatible database.
Once we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand &lt;em>why&lt;/em> this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions – options ranged from open-source SDKs (&lt;a href="https://opentelemetry.io/docs/languages/js/">OpenTelemetry Node.js SDK&lt;/a>) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted &lt;a href="https://grafana.com/oss/tempo/">Grafana Tempo&lt;/a> instance to test the setup and feasibility. (So far, the setup is working great, and I&amp;rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.&lt;/p></description></item><item><title>Socket File Descriptor and TCP connections</title><link>https://harshrai654.github.io/blogs/socket-file-descriptor-and-tcp-connections/</link><pubDate>Sun, 02 Mar 2025 16:15:01 +0530</pubDate><guid>https://harshrai654.github.io/blogs/socket-file-descriptor-and-tcp-connections/</guid><description>&lt;h2 id="socket-file-descriptors-and-their-kernel-structures">Socket File Descriptors and Their Kernel Structures&lt;/h2>
&lt;ul>
&lt;li>A &lt;strong>socket&lt;/strong> is a special type of file descriptor (FD) in Linux, represented as &lt;code>socket:[inode]&lt;/code>.&lt;/li>
&lt;li>Unlike regular file FDs, socket FDs point to &lt;strong>in-memory kernel structures&lt;/strong>, not disk inodes.&lt;/li>
&lt;li>The &lt;code>/proc/&amp;lt;pid&amp;gt;/fd&lt;/code> directory lists all FDs for a process, including sockets.&lt;/li>
&lt;li>The &lt;strong>inode number&lt;/strong> of a socket can be used to inspect its details via tools like &lt;code>ss&lt;/code> and &lt;code>/proc/net/tcp&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h4 id="example-checking-open-fds-for-process-216">Example: Checking Open FDs for Process &lt;code>216&lt;/code>&lt;/h4>
&lt;pre tabindex="0">&lt;code>ls -l /proc/216/fd
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Output:&lt;/strong>&lt;/p></description></item><item><title>Understanding Inodes and Disk Layout</title><link>https://harshrai654.github.io/blogs/file-system-implementation/</link><pubDate>Sat, 01 Mar 2025 20:41:00 +0000</pubDate><guid>https://harshrai654.github.io/blogs/file-system-implementation/</guid><description>&lt;h2 id="overall-organization-of-data-in-disks">Overall Organization Of Data In Disks&lt;/h2>
&lt;p>&lt;em>Assuming we have a 256KB disk&lt;/em>.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Disk Blocks&lt;/strong>: The basic units of storage on the disk, &lt;em>each 4 KB in size.&lt;/em> The disk is divided into these blocks, numbered from 0 to N-1 (where N is the total number of blocks).&lt;/li>
&lt;li>&lt;strong>Inode Bitmap (i)&lt;/strong>: Block 1; a bitmap tracking which inodes are free (0) or in-use (1).&lt;/li>
&lt;li>&lt;strong>Data Bitmap (d)&lt;/strong>: Block 2; a bitmap tracking which data blocks are free (0) or allocated (1).&lt;/li>
&lt;li>&lt;strong>Inode Table (I)&lt;/strong>: Blocks 3-7; an array of inodes, where each inode (256 bytes) holds metadata about a file, like size, permissions, and pointers to data blocks.
5 blocks of 4KB will contain 80 256 byte inode strutures.&lt;/li>
&lt;li>&lt;strong>Data Region (D)&lt;/strong>: Blocks 8-63; the largest section, storing the actual contents of files and directories.
&lt;img alt="Pasted image 20250301204506.png" loading="lazy" src="https://harshrai654.github.io/blogs/media/pasted-image-20250301204506.png">&lt;/li>
&lt;/ul>
&lt;h2 id="inode">Inode&lt;/h2>
&lt;p>Every inode has a unique identifier called an &lt;strong>inode number&lt;/strong> (or &lt;strong>i-number&lt;/strong>). This number acts like a file’s address in the file system, allowing the operating system to quickly locate its inode. For example:&lt;/p></description></item><item><title>Files And Directories</title><link>https://harshrai654.github.io/blogs/files-and-directories/</link><pubDate>Sun, 23 Feb 2025 20:41:00 +0000</pubDate><guid>https://harshrai654.github.io/blogs/files-and-directories/</guid><description>&lt;h1 id="files-and-directories">Files and directories&lt;/h1>
&lt;p>File systems virtualize persistent storage (e.g., hard drives, SSDs) into user-friendly files and directories, adding a third pillar to OS abstractions (processes for CPU, address spaces for memory).&lt;/p>
&lt;h2 id="file-paths-and-system-calls">File Paths and System Calls&lt;/h2>
&lt;p>Files are organized in a &lt;strong>tree-like directory structure&lt;/strong>, starting from the root (/). A file’s location is identified by its &lt;strong>pathname&lt;/strong> (e.g., /home/user/file.txt). To interact with files, processes use &lt;strong>system calls&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>open(path, flags)&lt;/strong>: Opens a file and returns a &lt;strong>file descriptor&lt;/strong> (fd).&lt;/li>
&lt;li>&lt;strong>read(fd, buffer, size)&lt;/strong>: Reads data from the file into a buffer using the fd.&lt;/li>
&lt;li>&lt;strong>write(fd, buffer, size)&lt;/strong>: Writes data to the file via the fd.&lt;/li>
&lt;li>&lt;strong>close(fd)&lt;/strong>: Closes the file, freeing the fd.&lt;/li>
&lt;/ul>
&lt;h2 id="file-descriptors">File Descriptors&lt;/h2>
&lt;p>A &lt;strong>file descriptor&lt;/strong> is a small integer, unique to each process, that identifies an open file. When a process calls open(), the operating system assigns it the next available fd (e.g., 3, 4, etc.). Every process starts with three default fds:&lt;/p></description></item><item><title>RAID (Redundant array of inexpensive disk)</title><link>https://harshrai654.github.io/blogs/raid-redundant-array-of-inexpensive-disk/</link><pubDate>Thu, 13 Feb 2025 20:25:14 +0530</pubDate><guid>https://harshrai654.github.io/blogs/raid-redundant-array-of-inexpensive-disk/</guid><description>&lt;h1 id="raid-disks">RAID Disks&lt;/h1>
&lt;p>Three axes on which disks are analysed&lt;/p>
&lt;ul>
&lt;li>Capacity - How much capacity is needed to store X bytes of data&lt;/li>
&lt;li>Reliability - How much fault-tolerant is the disk&lt;/li>
&lt;li>Performance - Read and write speeds (Sequential and random)&lt;/li>
&lt;/ul>
&lt;p>To make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification)
When we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database
In a way this set of disks also have challenges similar to distributes databases.&lt;/p></description></item><item><title>Multilevel Page table</title><link>https://harshrai654.github.io/blogs/multilevel-page-table/</link><pubDate>Tue, 26 Nov 2024 20:29:34 +0530</pubDate><guid>https://harshrai654.github.io/blogs/multilevel-page-table/</guid><description>&lt;h1 id="segmented-page-table">Segmented Page Table&lt;/h1>
&lt;p>Page table can grow large for a 32-bit address space and 4 KB page size we will be using 20 bits for virtual page number resulting in 2^20 bytes (i.e. 4MB of page table) for a single page table and each process will have its own page table so it is possible that we will be storing ~100sMB for page table alone which is not good.
&lt;img alt="Pasted image 20241127093849.png" loading="lazy" src="https://harshrai654.github.io/blogs/media/pasted-image-20241127093849.png">
For above page table with 4 bits for VPN (Virtual page number) we can see that only VPN 0,4,14 and 15 are valid i.e. pointing to a PFN (Physical Frame Number) other PTEs (Page table entry) are just taking up space which is not used.
We can use segmentation here with base and bound registers for each page table to only store valid PTE in the table.
&lt;img alt="Pasted image 20241127094506.png" loading="lazy" src="https://harshrai654.github.io/blogs/media/pasted-image-20241127094506.png">
This will again split the virtual address to also contain the segment bits to identify which segment the address belongs to (code, heap or stack). Instead of using &lt;em>Base Page Table Register&lt;/em> to query page table we will now be using &lt;em>Base Page Table Register [Segment]&lt;/em> to get page table physical address for a given segment.&lt;/p></description></item><item><title>TLB</title><link>https://harshrai654.github.io/blogs/tlb/</link><pubDate>Wed, 20 Nov 2024 22:26:06 +0530</pubDate><guid>https://harshrai654.github.io/blogs/tlb/</guid><description>&lt;h1 id="tlb">TLB&lt;/h1>
&lt;p>Translation look-aside buffer is a CPU cache which is generally small but since it is closer to CPU a TLB hit results in address translation to happen in 1-5 CPU cycles.&lt;/p>
&lt;blockquote>
&lt;p>CPU Cycle
Time taken by CPU to fully execute an instruction, while CPU frequency refers to the number of these cycles that occur per second&lt;/p>&lt;/blockquote>
&lt;p>A TLB hit means for given virtual address the physical frame number was found in the TLB cache. A TLB hit will benefit all the address that lie on the same page.
&lt;img alt="Pasted image 20241120223520.png" loading="lazy" src="https://harshrai654.github.io/blogs/media/pasted-image-20241120223520.png">
In the above given image page size is 16 bytes, so 4 INT variables can be saved in a single page, so a TLB hit of VPN 07 will serve address translation for VPN = 07 + page of offset of 0, 4,8 and 12 byte.
This type of caching is benefitted from &lt;em>&lt;strong>spatial locality&lt;/strong>&lt;/em> of data where a cache hit results in cache hits for surrounding data as well.
If we cache data and other data points which are more probable to get accessed in the same time frame (like loop variables etc) then such caching is benefitted from &lt;em>&lt;strong>Temporal locality.&lt;/strong>&lt;/em>&lt;/p></description></item><item><title>Page Tables</title><link>https://harshrai654.github.io/blogs/page-tables/</link><pubDate>Sun, 17 Nov 2024 20:22:15 +0530</pubDate><guid>https://harshrai654.github.io/blogs/page-tables/</guid><description>&lt;h1 id="page-tables">Page Tables&lt;/h1>
&lt;p>Page table contains the translation information of virtual page number to physical frame number.
For an address space of 32 bits and page size of 4 KB &lt;em>(i.e. memory of 2^32 is divided into segments of 4 KB where each segment is called a memory page)&lt;/em> , The virtual address will be of size 32 bits of which 12 bits (2^12 = 4 KB) will be used as offset inside a single page whereas remaining 20 bits will be used as virtual page number&lt;/p></description></item><item><title>B-Tree Latch Optimisation</title><link>https://harshrai654.github.io/blogs/b-tree-latch-optimisation/</link><pubDate>Sun, 17 Nov 2024 12:08:00 +0530</pubDate><guid>https://harshrai654.github.io/blogs/b-tree-latch-optimisation/</guid><description>&lt;h1 id="references">References&lt;/h1>
&lt;h2 id="56-problem">5.6 Problem&lt;/h2>
&lt;p>Generally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:&lt;/p>
&lt;ul>
&lt;li>If the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading&lt;/li>
&lt;li>If the operation is a write operation then there are again two possibilities:
&lt;ul>
&lt;li>
&lt;h3 id="optimistic-locking">Optimistic Locking&lt;/h3>
If the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it&amp;rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify
&lt;img alt="Pasted image 20241117123300.png" loading="lazy" src="https://harshrai654.github.io/blogs/media/pasted-image-20241117123300.png"> ^ab3c53&lt;/li>
&lt;li>
&lt;h3 id="pessimistic-locking">Pessimistic Locking&lt;/h3>
But if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking
&lt;img alt="Pasted image 20241117123407.png" loading="lazy" src="https://harshrai654.github.io/blogs/media/pasted-image-20241117123407.png">
&lt;blockquote>
&lt;p>&lt;strong>Pessimistic locking&lt;/strong> involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf&amp;rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff&lt;/p></description></item></channel></rss>