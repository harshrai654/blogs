<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Networking on Learning Loop</title><link>https://harshrai654.github.io/blogs/tags/networking/</link><description>Recent content in Networking on Learning Loop</description><generator>Hugo -- 0.155.1</generator><language>en-us</language><lastBuildDate>Fri, 30 Jan 2026 23:43:37 +0530</lastBuildDate><atom:link href="https://harshrai654.github.io/blogs/tags/networking/index.xml" rel="self" type="application/rss+xml"/><item><title>Debugging HTTP 503 UC Errors in Istio Service Mesh</title><link>https://harshrai654.github.io/blogs/debugging-http-503-uc-errors-in-istio-service-mesh/</link><pubDate>Fri, 30 Jan 2026 23:43:37 +0530</pubDate><guid>https://harshrai654.github.io/blogs/debugging-http-503-uc-errors-in-istio-service-mesh/</guid><description>&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;To obtain more relevant metrics and a comprehensive understanding of traces when external API requests access our systems, We opted to enable tracing for the network mesh components situated above the Kubernetes service. This specifically covers the entire request flow from its initiation: the ingress gateway receives the request and routes it according to defined rules to a particular K8s service; then, the Istio sidecar proxy containers, running alongside our application containers in the same pod, receive and proxy the request to main containers, where our Node.js server process ultimately handle it and generate the appropriate response.
Before enabling tracing for istio mesh, tracing for a HTTP request began when the request reached our application container at the pod level, with HTTP auto-instrumentation serving as the trace root.&lt;/p&gt;</description></item><item><title>Multipart Form Uploads - Busboy and Node Streams</title><link>https://harshrai654.github.io/blogs/multipart-form-uploads---busboy-and-node-streams/</link><pubDate>Sun, 12 Oct 2025 00:48:38 +0530</pubDate><guid>https://harshrai654.github.io/blogs/multipart-form-uploads---busboy-and-node-streams/</guid><description>&lt;p&gt;I was recently investigating ways to improve the efficiency of file uploads to a Node.js server. This need arose after encountering a production bug where the absence of a maximum file size limit for uploads led to an out-of-memory crash due to file buffers consuming excessive heap memory. In this Node.js server, I was using Express and &lt;code&gt;express-openapi-validator&lt;/code&gt; to document the server&amp;rsquo;s API with an &lt;code&gt;OpenAPI&lt;/code&gt; specification. &lt;code&gt;express-openapi-validator&lt;/code&gt; utilizes &lt;code&gt;multer&lt;/code&gt; for file uploads. I had previously encountered this library whenever file uploads from forms needed to be handled in Node.js, but I never questioned why a separate library was necessary for file uploads. This time, I decided to go deeper to understand if a dedicated package for file uploads is truly needed, and if so, what specific benefits &lt;code&gt;Multer&lt;/code&gt; or similar libraries provide.
I initially needed to find a configuration option in &lt;code&gt;express-openapi-validator&lt;/code&gt; to set a request-wide limit on the maximum size (in bytes) of data allowed in a request, including all file attachments.  The &lt;code&gt;express-openapi-validator&lt;/code&gt; package offers a &lt;code&gt;fileUploader&lt;/code&gt; configuration (&lt;a href="https://cdimascio.github.io/express-openapi-validator-documentation/usage-file-uploader/"&gt;fileUploader documentation&lt;/a&gt;) that passes options directly to &lt;code&gt;multer&lt;/code&gt;.&lt;/p&gt;</description></item><item><title>Debugging Redis Latency</title><link>https://harshrai654.github.io/blogs/debugging-redis-latency/</link><pubDate>Sun, 06 Apr 2025 14:21:54 +0530</pubDate><guid>https://harshrai654.github.io/blogs/debugging-redis-latency/</guid><description>&lt;p&gt;This article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;After introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as &lt;code&gt;ioredis&lt;/code&gt; for interacting with our Redis-compatible database.
Once we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand &lt;em&gt;why&lt;/em&gt; this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions – options ranged from open-source SDKs (&lt;a href="https://opentelemetry.io/docs/languages/js/"&gt;OpenTelemetry Node.js SDK&lt;/a&gt;) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted &lt;a href="https://grafana.com/oss/tempo/"&gt;Grafana Tempo&lt;/a&gt; instance to test the setup and feasibility. (So far, the setup is working great, and I&amp;rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.&lt;/p&gt;</description></item><item><title>Socket File Descriptor and TCP connections</title><link>https://harshrai654.github.io/blogs/socket-file-descriptor-and-tcp-connections/</link><pubDate>Sun, 02 Mar 2025 16:15:01 +0530</pubDate><guid>https://harshrai654.github.io/blogs/socket-file-descriptor-and-tcp-connections/</guid><description>&lt;h2 id="socket-file-descriptors-and-their-kernel-structures"&gt;Socket File Descriptors and Their Kernel Structures&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;socket&lt;/strong&gt; is a special type of file descriptor (FD) in Linux, represented as &lt;code&gt;socket:[inode]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Unlike regular file FDs, socket FDs point to &lt;strong&gt;in-memory kernel structures&lt;/strong&gt;, not disk inodes.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;/proc/&amp;lt;pid&amp;gt;/fd&lt;/code&gt; directory lists all FDs for a process, including sockets.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;inode number&lt;/strong&gt; of a socket can be used to inspect its details via tools like &lt;code&gt;ss&lt;/code&gt; and &lt;code&gt;/proc/net/tcp&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="example-checking-open-fds-for-process-216"&gt;Example: Checking Open FDs for Process &lt;code&gt;216&lt;/code&gt;&lt;/h4&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;ls -l /proc/216/fd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt;</description></item></channel></rss>