<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Database on Learning Loop</title>
    <link>https://harshrai654.github.io/blogs/tags/database/</link>
    <description>Recent content in Database on Learning Loop</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 06 Apr 2025 14:21:54 +0530</lastBuildDate>
    <atom:link href="https://harshrai654.github.io/blogs/tags/database/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Debugging Redis Latency</title>
      <link>https://harshrai654.github.io/blogs/debugging-redis-latency/</link>
      <pubDate>Sun, 06 Apr 2025 14:21:54 +0530</pubDate>
      <guid>https://harshrai654.github.io/blogs/debugging-redis-latency/</guid>
      <description>&lt;p&gt;This article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;After introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as &lt;code&gt;ioredis&lt;/code&gt; for interacting with our Redis-compatible database.
Once we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand &lt;em&gt;why&lt;/em&gt; this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions â€“ options ranged from open-source SDKs (&lt;a href=&#34;https://opentelemetry.io/docs/languages/js/&#34;&gt;OpenTelemetry Node.js SDK&lt;/a&gt;) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted &lt;a href=&#34;https://grafana.com/oss/tempo/&#34;&gt;Grafana Tempo&lt;/a&gt; instance to test the setup and feasibility. (So far, the setup is working great, and I&amp;rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.&lt;/p&gt;</description>
    </item>
    <item>
      <title>B-Tree Latch Optimisation</title>
      <link>https://harshrai654.github.io/blogs/b-tree-latch-optimisation/</link>
      <pubDate>Sun, 17 Nov 2024 12:08:00 +0530</pubDate>
      <guid>https://harshrai654.github.io/blogs/b-tree-latch-optimisation/</guid>
      <description>&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;h2 id=&#34;56-problem&#34;&gt;5.6 Problem&lt;/h2&gt;
&lt;p&gt;Generally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading&lt;/li&gt;
&lt;li&gt;If the operation is a write operation then there are again two possibilities:
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;optimistic-locking&#34;&gt;Optimistic Locking&lt;/h3&gt;
If the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it&amp;rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify
&lt;img alt=&#34;Pasted image 20241117123300.png&#34; loading=&#34;lazy&#34; src=&#34;https://harshrai654.github.io/blogs/media/pasted-image-20241117123300.png&#34;&gt; ^ab3c53&lt;/li&gt;
&lt;li&gt;
&lt;h3 id=&#34;pessimistic-locking&#34;&gt;Pessimistic Locking&lt;/h3&gt;
But if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking
&lt;img alt=&#34;Pasted image 20241117123407.png&#34; loading=&#34;lazy&#34; src=&#34;https://harshrai654.github.io/blogs/media/pasted-image-20241117123407.png&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Pessimistic locking&lt;/strong&gt; involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf&amp;rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
