<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>RAID (Redundant array of inexpensive disk) | Learning Loop</title>
<meta name=keywords content="Disk"><meta name=description content="RAID Disks
Three axes on which disks are analysed

Capacity - How much capacity is needed to store X bytes of data
Reliability - How much fault-tolerant is the disk
Performance - Read and write speeds (Sequential and random)

To make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification)
When we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database
In a way this set of disks also have challenges similar to distributes databases."><meta name=author content><link rel=canonical href=https://harshrai654.github.io/blogs/raid-redundant-array-of-inexpensive-disk/><link crossorigin=anonymous href=/blogs/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://harshrai654.github.io/blogs/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://harshrai654.github.io/blogs/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://harshrai654.github.io/blogs/favicon-32x32.png><link rel=apple-touch-icon href=https://harshrai654.github.io/blogs/apple-touch-icon.png><link rel=mask-icon href=https://harshrai654.github.io/blogs/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://harshrai654.github.io/blogs/raid-redundant-array-of-inexpensive-disk/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script data-goatcounter=https://ttharsh.goatcounter.com/count async src=//gc.zgo.at/count.js></script><meta property="og:url" content="https://harshrai654.github.io/blogs/raid-redundant-array-of-inexpensive-disk/"><meta property="og:site_name" content="Learning Loop"><meta property="og:title" content="RAID (Redundant array of inexpensive disk)"><meta property="og:description" content="RAID Disks Three axes on which disks are analysed
Capacity - How much capacity is needed to store X bytes of data Reliability - How much fault-tolerant is the disk Performance - Read and write speeds (Sequential and random) To make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification) When we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database In a way this set of disks also have challenges similar to distributes databases."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-02-13T20:25:14+05:30"><meta property="article:modified_time" content="2025-02-13T20:25:14+05:30"><meta property="article:tag" content="Disk"><meta name=twitter:card content="summary"><meta name=twitter:title content="RAID (Redundant array of inexpensive disk)"><meta name=twitter:description content="RAID Disks
Three axes on which disks are analysed

Capacity - How much capacity is needed to store X bytes of data
Reliability - How much fault-tolerant is the disk
Performance - Read and write speeds (Sequential and random)

To make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification)
When we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database
In a way this set of disks also have challenges similar to distributes databases."></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://harshrai654.github.io/blogs/ accesskey=h title="Learning Loop (Alt + H)">Learning Loop</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://harshrai654.github.io/blogs/>Home</a></div><h1 class="post-title entry-hint-parent">RAID (Redundant array of inexpensive disk)</h1><div class=post-meta><span title='2025-02-13 20:25:14 +0530 IST'>February 13, 2025</span>&nbsp;Â·&nbsp;7 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#raid-disks aria-label="RAID Disks">RAID Disks</a><ul><li><a href=#raid-level-0---striping aria-label="RAID Level 0 - Striping">RAID Level 0 - Striping</a></li><li><a href=#raid-level-1---mirroring aria-label="RAID Level 1 - Mirroring">RAID Level 1 - Mirroring</a><ul><li><a href=#consistent-update-problem aria-label="Consistent update problem">Consistent update problem</a></li></ul></li><li><a href=#raid-level-4---parity aria-label="RAID Level 4 - Parity">RAID Level 4 - Parity</a></li><li><a href=#raid-level-5---rotating-parity aria-label="RAID Level 5 - Rotating Parity">RAID Level 5 - Rotating Parity</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h1 id=raid-disks>RAID Disks<a hidden class=anchor aria-hidden=true href=#raid-disks>#</a></h1><p>Three axes on which disks are analysed</p><ul><li>Capacity - How much capacity is needed to store X bytes of data</li><li>Reliability - How much fault-tolerant is the disk</li><li>Performance - Read and write speeds (Sequential and random)</li></ul><p>To make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification)
When we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database
In a way this set of disks also have challenges similar to distributes databases.</p><p>There are different types of data arrangement in set of physical disks which results in different types/levels of RAID</p><h2 id=raid-level-0---striping>RAID Level 0 - Striping<a hidden class=anchor aria-hidden=true href=#raid-level-0---striping>#</a></h2><table><thead><tr><th>Disk 0</th><th>Disk 1</th><th>Disk 2</th><th>Disk 3</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>2</td><td>3</td></tr><tr><td>4</td><td>5</td><td>6</td><td>7</td></tr><tr><td>8</td><td>9</td><td>10</td><td>11</td></tr><tr><td>12</td><td>13</td><td>14</td><td>15</td></tr></tbody></table><blockquote><p>Here each cell is a disk block which will be of fixed size (for example 4KB)
A vertical column shows blocks stored by a single disk</p></blockquote><p><strong>Striping</strong>: Writing out chunks (in multiple of disk block size) to each disk, one at a time so that we have the data spread uniformly across the disk.
When read or write requests comes up to disk it comes in the form of <strong>stripe number</strong> <em>(row number in above illustration)</em> and based on RAID level disk controller knows which block it has to access and which disk to read it from.</p><p>Tradeoffs:</p><ul><li>This level has no reliability as a disk failure always means loss of some data.</li><li>This arrangement is good for capacity as we are utilizing all disks for storage.</li></ul><h2 id=raid-level-1---mirroring>RAID Level 1 - Mirroring<a hidden class=anchor aria-hidden=true href=#raid-level-1---mirroring>#</a></h2><table><thead><tr><th>Disk 0</th><th>Disk 1</th><th>Disk 2</th><th>Disk 3</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td>2</td><td>2</td><td>3</td><td>3</td></tr><tr><td>4</td><td>4</td><td>5</td><td>5</td></tr><tr><td>6</td><td>6</td><td>7</td><td>7</td></tr><tr><td>Copies of blocks made to two disks, tradeoff of reliability over capacity.</td><td></td><td></td><td></td></tr></tbody></table><h3 id=consistent-update-problem>Consistent update problem<a hidden class=anchor aria-hidden=true href=#consistent-update-problem>#</a></h3><blockquote><p>Imagine the write is issued to the RAID, and then the RAID decides that it must be written to two disks, disk 0 and disk 1. The RAID then issues the write to disk 0, but just before the RAID can issue the request to disk 1, a power loss (or system crash) occurs. In this unfortunate case, let us assume that the request to disk 0 completed (but clearly the request to disk 1 did not, as it was never issued). The result of this untimely power loss is that the two copies of the block are now inconsistent; the copy on disk 0 is the new version, and the copy on disk 1 is the old. What we would like to happen is for the state of both disks to change atomically, i.e., either both should end up as the new version or neither. The general way to solve this problem is to use a write-ahead log of some kind to first record what the RAID is about to do (i.e., update two disks with a certain piece of data) before doing it. By taking this approach, we can ensure that in the presence of a crash, the right thing will happen; by running a recovery procedure that replays all pending transactions to the RAID, we can ensure that no two mirrored copies (in the RAID-1 case) are out of sync. One last note: because logging to disk on every write is prohibitively expensive, most RAID hardware includes a small amount of non-volatile RAM (e.g., battery-backed) where it performs this type of logging. Thus, consistent update is provided without the high cost of logging to disk.</p></blockquote><p>Tradeoffs:</p><ul><li>This level can tolerate up to N/2 disk failures.</li><li>This arrangement is good for reliability over cost of capacity being half</li><li>Even though updates for a block needs to happen at two separate disks, The write would be parallel but still slower than updating a single disk (If we consider different seek and rotational time for both disks)</li></ul><h2 id=raid-level-4---parity>RAID Level 4 - Parity<a hidden class=anchor aria-hidden=true href=#raid-level-4---parity>#</a></h2><table><thead><tr><th>Disk 0</th><th>Disk 1</th><th>Disk 2</th><th>Disk 3</th><th>Disk 4</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>2</td><td>3</td><td><strong>P0</strong></td></tr><tr><td>4</td><td>5</td><td>6</td><td>7</td><td><strong>P1</strong></td></tr><tr><td>8</td><td>9</td><td>10</td><td>11</td><td><strong>P2</strong></td></tr><tr><td>12</td><td>13</td><td>14</td><td>15</td><td><strong>P3</strong></td></tr><tr><td>N - 1 Disks follow striping with the Nth disk containing parity block for each strip row</td><td></td><td></td><td></td><td></td></tr><tr><td><strong>Concept:</strong> RAID 4 adds redundancy to a disk array using parity, which consumes less storage space than mirroring.</td><td></td><td></td><td></td><td></td></tr></tbody></table><p><strong>How it works:</strong></p><ul><li>A single parity block is added to each stripe of data blocks.</li><li>The parity block stores redundant information calculated from the data blocks in its stripe.</li><li>The XOR function is used to calculate parity. XOR returns 0 if there are an even number of 1s in the bits, and 1 if there are an odd number of 1s. Â </li><li>The parity bit ensures that the number of 1s in any row, including the parity bit, is always even.</li></ul><p><strong>Example of Parity Calculation:</strong>
Imagine a RAID 4 system with 4-bit data blocks. Let&rsquo;s say we have the following data in one stripe:</p><ul><li>Block 0: <code>0010</code></li><li>Block 1: <code>1001</code></li><li>Block 2: <code>0110</code></li><li>Block 3: <code>1011</code></li></ul><p>To calculate the parity block, we perform a bitwise XOR across the corresponding bits of each data block:</p><ul><li><strong>Bit 1 (from left):</strong> 0 XOR 1 XOR 0 XOR 1 = 0</li><li><strong>Bit 2:</strong> 0 XOR 0 XOR 1 XOR 0 = 1</li><li><strong>Bit 3:</strong> 1 XOR 0 XOR 1 XOR 1 = 1</li><li><strong>Bit 4:</strong> 0 XOR 1 XOR 0 XOR 1 = 0</li></ul><p>Therefore, the parity block would be <code>0110</code>.</p><p><strong>Data recovery:</strong></p><ul><li>If a data block is lost, the remaining blocks in the stripe, including the parity block, are read.</li><li>The XOR function is applied to these blocks to reconstruct the missing data. For example, if Block 2 (<code>0110</code>) was lost, we would XOR Block 0, Block 1, Block 3, and the parity block: <code>0010</code> XOR <code>1001</code> XOR <code>1011</code> XOR <code>0110</code> = <code>0110</code>.</li></ul><p><strong>Performance:</strong></p><ul><li>RAID 4 has a performance cost due to the overhead of parity calculation. Crucially, all write operations must update the parity disk.</li><li><strong>Write Bottleneck:</strong> If multiple random write requests comes for various blocks at the same time then they all will all require to update different parity blocks but all parity blocks are in one disk so multiple updates to different parity block will be done one after the other because all are in the same disk which eventually makes concurrent random write requests sequential in nature <em><strong>this is also known as small-write problem</strong></em></li></ul><h2 id=raid-level-5---rotating-parity>RAID Level 5 - Rotating Parity<a hidden class=anchor aria-hidden=true href=#raid-level-5---rotating-parity>#</a></h2><p>Instead of having one dedicated disk for parity blocks for each stripe, distribute the parity blocks in rotating manner to all disks for each stripe.</p><table><thead><tr><th>Disk 0</th><th>Disk 1</th><th>Disk 2</th><th>Disk 3</th><th>Disk 4</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>2</td><td>3</td><td><strong>P0</strong></td></tr><tr><td>4</td><td>5</td><td>6</td><td><strong>P1</strong></td><td>7</td></tr><tr><td>8</td><td>9</td><td><strong>P2</strong></td><td>10</td><td>11</td></tr><tr><td>12</td><td><strong>P3</strong></td><td>13</td><td>14</td><td>15</td></tr><tr><td><strong>Small-write:</strong> Concurrent random write requests to blocks of different blocks of different stripes can now be done parallelly since parity block for each stripe will be in different disk.</td><td></td><td></td><td></td><td></td></tr><tr><td>It is still possible that blocks of different stripes need to update parity blocks which are lying in same disk (due to rotating nature of parity block)</td><td></td><td></td><td></td><td></td></tr><tr><td><em>write requests to blocks of same stripes will still be sequential since parity block will be on same disk for all the blocks of same stripe.</em></td><td></td><td></td><td></td><td></td></tr></tbody></table><p><strong>In summary:</strong> While RAID 5 significantly improves random write performance compared to RAID 4, it doesn&rsquo;t completely eliminate the possibility of parity-related bottlenecks. The rotated parity distribution reduces the <em>likelihood</em> of contention, but it doesn&rsquo;t guarantee that parity updates will always be fully parallel. The chance of multiple stripes&rsquo; parity residing on the same disk is still there, leading to potential performance degradation.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p><a href=https://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf>https://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf</a>
How Data recovery happens with parity drive in RAID: <a href="https://blogs.oracle.com/solaris/post/understanding-raid-5-recovery-with-elementary-school-math#:~:text=We%20need%20to%20read%20all%20date%20from%20other%20drives%20to%20recovery%20parity">https://blogs.oracle.com/solaris/post/understanding-raid-5-recovery-with-elementary-school-math#:~:text=We%20need%20to%20read%20all%20date%20from%20other%20drives%20to%20recovery%20parity</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://harshrai654.github.io/blogs/tags/disk/>Disk</a></li></ul><nav class=paginav><a class=prev href=https://harshrai654.github.io/blogs/files-and-directories/><span class=title>Â« Prev</span><br><span>Files And Directories</span>
</a><a class=next href=https://harshrai654.github.io/blogs/multilevel-page-table/><span class=title>Next Â»</span><br><span>Multilevel Page table</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share RAID (Redundant array of inexpensive disk) on x" href="https://x.com/intent/tweet/?text=RAID%20%28Redundant%20array%20of%20inexpensive%20disk%29&amp;url=https%3a%2f%2fharshrai654.github.io%2fblogs%2fraid-redundant-array-of-inexpensive-disk%2f&amp;hashtags=Disk"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAID (Redundant array of inexpensive disk) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fharshrai654.github.io%2fblogs%2fraid-redundant-array-of-inexpensive-disk%2f&amp;title=RAID%20%28Redundant%20array%20of%20inexpensive%20disk%29&amp;summary=RAID%20%28Redundant%20array%20of%20inexpensive%20disk%29&amp;source=https%3a%2f%2fharshrai654.github.io%2fblogs%2fraid-redundant-array-of-inexpensive-disk%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAID (Redundant array of inexpensive disk) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fharshrai654.github.io%2fblogs%2fraid-redundant-array-of-inexpensive-disk%2f&title=RAID%20%28Redundant%20array%20of%20inexpensive%20disk%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAID (Redundant array of inexpensive disk) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fharshrai654.github.io%2fblogs%2fraid-redundant-array-of-inexpensive-disk%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAID (Redundant array of inexpensive disk) on whatsapp" href="https://api.whatsapp.com/send?text=RAID%20%28Redundant%20array%20of%20inexpensive%20disk%29%20-%20https%3a%2f%2fharshrai654.github.io%2fblogs%2fraid-redundant-array-of-inexpensive-disk%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAID (Redundant array of inexpensive disk) on telegram" href="https://telegram.me/share/url?text=RAID%20%28Redundant%20array%20of%20inexpensive%20disk%29&amp;url=https%3a%2f%2fharshrai654.github.io%2fblogs%2fraid-redundant-array-of-inexpensive-disk%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAID (Redundant array of inexpensive disk) on ycombinator" href="https://news.ycombinator.com/submitlink?t=RAID%20%28Redundant%20array%20of%20inexpensive%20disk%29&u=https%3a%2f%2fharshrai654.github.io%2fblogs%2fraid-redundant-array-of-inexpensive-disk%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://harshrai654.github.io/blogs/>Learning Loop</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>