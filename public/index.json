[{"content":"I was recently investigating ways to improve the efficiency of file uploads to a Node.js server. This need arose after encountering a production bug where the absence of a maximum file size limit for uploads led to an out-of-memory crash due to file buffers consuming excessive heap memory. In this Node.js server, I was using Express and express-openapi-validator to document the server\u0026rsquo;s API with an OpenAPI specification. express-openapi-validator utilizes multer for file uploads. I had previously encountered this library whenever file uploads from forms needed to be handled in Node.js, but I never questioned why a separate library was necessary for file uploads. This time, I decided to go deeper to understand if a dedicated package for file uploads is truly needed, and if so, what specific benefits Multer or similar libraries provide. I initially needed to find a configuration option in express-openapi-validator to set a request-wide limit on the maximum size (in bytes) of data allowed in a request, including all file attachments.¬†The express-openapi-validator package offers a fileUploader configuration (fileUploader documentation) that passes options directly to multer.\nTherefore, I checked the multer documentation and discovered the limits configuration option (multer limits documentation), which perfectly suited the needs. multer uses busboy to handle the parsing of multipart form data in a stream fashion. Consequently, the limits options defined in multer are actually passed directly to busboy, and represent busboy's limit configurations.\n1- **limits**¬†-¬†_object_¬†- Various limits on incoming data. Valid properties are: 2 3- **fieldNameSize**¬†-¬†_integer_¬†- Max field name size (in bytes).¬†**Default:**¬†`100`. 4\t5- **fieldSize**¬†-¬†_integer_¬†- Max field value size (in bytes).¬†**Default:**¬†`1048576`¬†(1MB). 6\t7- **fields**¬†-¬†_integer_¬†- Max number of non-file fields.¬†**Default:**¬†`Infinity`. 8\t9- **fileSize**¬†-¬†_integer_¬†- For multipart forms, the max file size (in bytes).¬†**Default:**¬†`Infinity`. 10\t11- **files**¬†-¬†_integer_¬†- For multipart forms, the max number of file fields.¬†**Default:**¬†`Infinity`. 12\t13- **parts**¬†-¬†_integer_¬†- For multipart forms, the max number of parts (fields + files).¬†**Default:**¬†`Infinity`. 14\t15- **headerPairs**¬†-¬†_integer_¬†- For multipart forms, the max number of header key-value pairs to parse.¬†**Default:**¬†`2000`¬†(same as node\u0026#39;s http module). So, based on the available options, I can have a per-file-based size limit along with a limit on the number of files.¬†However, that wasn\u0026rsquo;t quite what I needed, since the requirement wasn\u0026rsquo;t just to cap the number of files, but to limit the overall request upload size.¬†Eventually, I stumbled upon this feature request in the busboy repository, which addressed exactly what I wanted.¬†This discovery kicked off a intresting exploration of how busboy handles multipart form data.¬†It led to uncovering many details regarding back pressure in streams, how TCP control flow windows help in maintaining back pressure, and all this in an effort to contribute a request-wide total size limit configuration option to busboy.¬†Streams in NodeJS Let\u0026rsquo;s first look at an example usage of busboy with plain old HTTP server in Node.js\n1const http = require(\u0026#39;http\u0026#39;); 2 3const busboy = require(\u0026#39;busboy\u0026#39;); 4 5http.createServer((req, res) =\u0026gt; { 6 if (req.method === \u0026#39;POST\u0026#39;) { 7 console.log(\u0026#39;POST request\u0026#39;); 8 const bb = busboy({ headers: req.headers }); 9 bb.on(\u0026#39;file\u0026#39;, (name, file, info) =\u0026gt; { 10 const { filename, encoding, mimeType } = info; 11 console.log( 12 `File [${name}]: filename: %j, encoding: %j, mimeType: %j`, 13 filename, 14 encoding, 15 mimeType 16 ); 17 file.on(\u0026#39;data\u0026#39;, (data) =\u0026gt; { 18 console.log(`File [${name}] got ${data.length} bytes`); 19 }).on(\u0026#39;close\u0026#39;, () =\u0026gt; { 20 console.log(`File [${name}] done`); 21 }); 22 }); 23 bb.on(\u0026#39;field\u0026#39;, (name, val, info) =\u0026gt; { 24 console.log(`Field [${name}]: value: %j`, val); 25 }); 26 bb.on(\u0026#39;close\u0026#39;, () =\u0026gt; { 27 console.log(\u0026#39;Done parsing form!\u0026#39;); 28 res.writeHead(303, { Connection: \u0026#39;close\u0026#39;, Location: \u0026#39;/\u0026#39; }); 29 res.end(); 30 }); 31 req.pipe(bb); 32 } 33}).listen(8000, () =\u0026gt; { 34 console.log(\u0026#39;Listening for requests\u0026#39;); 35}); Looking at the code, the req object (a Readable stream) is piped into the busboy instance (bb). This bb instance returns a MultiPart class object (specifically, an instance of the class found here), which inherits from Writable.\nThe Readable and Writable streams have several methods that are invoked at different times during operations such as read, write, and pause. Because each stream relies on the consumer downstream and the producer upstream, Node.js buffers the generated data by default if there\u0026rsquo;s backpressure in the stream pipeline. You can find more information about this in the Node.js documentation:\nBoth¬†Writable¬†and¬†Readable¬†streams will store data in an internal buffer. The amount of data potentially buffered depends on the¬†highWaterMark¬†option passed into the stream\u0026rsquo;s constructor. For normal streams, the¬†highWaterMark¬†option specifies a¬†total number of bytes. Data is buffered in¬†Readable¬†streams when the implementation calls¬†stream.push(chunk). If the consumer of the Stream does not call¬†stream.read(), the data will sit in the internal queue until it is consumed. Once the total size of the internal read buffer reaches the threshold specified by¬†highWaterMark, the stream will temporarily stop reading data from the underlying resource until the data currently buffered can be consumed (that is, the stream will stop calling the internal¬†readable._read()¬†method that is used to fill the read buffer). Data is buffered in¬†Writable¬†streams when the¬†writable.write(chunk)¬†method is called repeatedly. While the total size of the internal write buffer is below the threshold set by¬†highWaterMark, calls to¬†writable.write()¬†will return¬†true. Once the size of the internal buffer reaches or exceeds the¬†highWaterMark,¬†false¬†will be returned.\nA key goal of the¬†stream¬†API, particularly the¬†stream.pipe()¬†method, is to limit the buffering of data to acceptable levels such that sources and destinations of differing speeds will not overwhelm the available memory.\nTo understand the idea discussed above in the docs let\u0026rsquo;s take an example: If your server does something like:\n1const ws = fs.createWriteStream(\u0026#39;file.bin\u0026#39;) 2req.pipe(ws); Then data flows like this:\n[TCP socket] ‚Üí [IncomingMessage (Readable)] ‚Üí [Writable File Stream] ‚Üí [disk] But these components run at different speeds:\nThe network producer (socket) might push data quickly. The file system consumer might be slower (e.g., disk I/O, file system buffering). So Node.js stream buffering ensures that: The Readable (req) will stop calling its internal _read() once its internal buffer exceeds its highWaterMark. The Writable (fs.WriteStream) will signal backpressure by returning false from .write(), which will pause the Readable until the buffer drains. ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Network Socket ‚îÇ (Producer) ‚îÇ [TCP Stream] ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ data chunks ‚Üí ‚ñº req.push(chunk) ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ IncomingMessage ‚îÇ (Readable Stream) ‚îÇ Internal Buffer ‚îÇ ‚îÇ highWaterMark = 16KB ‚îÇ [Buffered: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 12KB / 16KB ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ws.write(chunk) ‚Üí called internally after pipe ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ File WriteStream ‚îÇ (Writable Stream) ‚îÇ Internal Buffer ‚îÇ ‚îÇ highWaterMark = 64KB ‚îÇ [Buffered: ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 20KB / 64KB ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ fs.write(chunk) ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Disk I/O ‚îÇ (Slow Consumer) ‚îÇ write operation ‚Üí ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò As you can see above, if Disk I/O is slower than the rate at which we are receiving chunks from the TCP socket, the ws.write call will continue buffering incoming chunks into its internal memory buffer until it reaches its highWaterMark (64KB in this case). Once the buffer is full, ws.write returns false, signalling the stream above it that the downstream consumer is slower. The IncomingMessage (req), being a readable stream, will then start buffering the chunks coming from the TCP socket in its own internal buffer (16KB highWaterMark). When this buffer fills up, req.push will also return false, indicating it cannot accept more data for now. This backpressure propagates upstream and eventually affects the TCP socket itself, which the OS handles via TCP flow control - essentially slowing down the sender to prevent memory overload.\nAt the OS level, each TCP socket has its own kernel buffer. Normally, data flows like this:\nTCP socket kernel buffer ‚Üí user-space buffer ‚Üí writable stream buffer ‚Üí filesystem buffer ‚Üí disk Each step involves memory copies. In some cases, if we don‚Äôt need to parse or modify the data (like streaming raw uploads to disk), we can bypass user-space entirely using zero-copy techniques, transferring data directly from the socket kernel buffer to the filesystem kernel buffer. This avoids redundant memory copies and improves performance, at the cost of not being able to inspect or modify the data before it‚Äôs written. For a deeper dive into TCP connections, sockets, and file descriptors, you can check out my detailed write-up here.\nBusboy Implementation \u0026amp; New Limit Config Now that we have a basic understanding of how data moves through the IncomingRequest object in an HTTP server, let\u0026rsquo;s address the initial question: why do we need Busboy, and what problem does it solve? Busboy is a parser for formdata. We\u0026rsquo;ve already seen how we pipe the readable stream req output to Busboy\u0026rsquo;s multipart writable stream. Before examining the data flow within Busboy, let\u0026rsquo;s understand multipart/form-data. Below is an example of multipart/form-data. We will focus primarily on file uploads for this form, rather than fields.\nPOST /upload HTTP/1.1 Host: example.com User-Agent: curl/8.0.1 Accept: */* Content-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW Content-Length: 512 ------WebKitFormBoundary7MA4YWxkTrZu0gW Content-Disposition: form-data; name=\u0026#34;file1\u0026#34;; filename=\u0026#34;file1.txt\u0026#34; Content-Type: text/plain Hello, this is the content of file1.txt. It could be multiple lines. ------WebKitFormBoundary7MA4YWxkTrZu0gW Content-Disposition: form-data; name=\u0026#34;file2\u0026#34;; filename=\u0026#34;file2.jpg\u0026#34; Content-Type: image/jpeg (binary data of file2.jpg would go here) ------WebKitFormBoundary7MA4YWxkTrZu0gW-- Breakdown of each part of the above raw HTTP multipart/form-data\nHTTP is a text protocol Everything above is sent as plain text over a TCP connection (except the actual binary file contents). The TCP stream is just a stream of bytes; it has no understanding of fields or files. Node.js‚Äôs HTTP server reads this TCP stream and parses the start-line and headers (e.g., POST /upload HTTP/1.1, Host, Content-Type, Content-Length) so it can understand the request. After the headers are parsed, the remaining bytes (the body) are exposed to req (IncomingMessage) as a readable stream. This raw body byte stream is exactly what gets pushed into Busboy for parsing. Boundary markers The boundary string (----WebKitFormBoundary7MA4YWxkTrZu0gW) separates each part of the multipart request. Each part starts with --\u0026lt;boundary\u0026gt; and the whole request ends with --\u0026lt;boundary\u0026gt;--. Busboy parses these boundaries to identify where one field ends and another begins. Headers for each part Content-Disposition gives the field name and filename. Content-Type tells the server what type of data is being sent. These headers are part of the raw text stream; without parsing them, the server cannot know the field names or file types. Content / File data Text files are just plain text; binary files are raw bytes included inline in the TCP stream. Node.js or any HTTP server cannot automatically separate the files from the TCP stream. Busboy (or similar parser) is required to convert this raw text+binary stream into usable file streams or buffers in user space. This is why a kernel -\u0026gt; user-space memory copy is necessary: the server must access the bytes in user-space to parse headers, boundaries, and file contents. Content-Length Represents the total bytes of all boundaries, headers, and file data. TCP itself only cares about sending bytes; the HTTP layer interprets them according to this length. How Busboy Parses the Multipart Stream The above raw stream of data is what gets piped to Busboy‚Äôs internal Multipart writeable stream. Inside its constructor, Busboy initializes various limits based on the configuration provided. As part of the constructor logic, it creates a HeaderParser and sets up a StreamSearch instance with the multipart boundary string as the needle:\n1this._bparser = new StreamSearch(`\\r\\n--${boundary}`, ssCb); StreamSearch calls the given callback ssCb whenever it encounters non-matching data or finds a match for the needle.\nIn other words, as the raw multipart/form-data byte stream flows through, StreamSearch helps separate the actual boundaries from the data chunks in between, and passes those chunks to the provided callback for processing. The callback ssCb is the core of the multipart parsing logic. During initialization, Busboy sets up several internal tracking variables, such as:\nfileSize - Tracks the size of the current file in bytes across all incoming chunks for that file. Once this reaches the configured limits.fileSize, Busboy stops processing additional chunks for that specific file and skips them until it encounters the next boundary (i.e., the start of a new field). files - Keeps track of the number of files seen so far. Once this count reaches limits.files, Busboy skips all subsequent file data until it detects the end of the form-data. There are other similar limits and tracking variables initialized internally, but these two are the most relevant to the new feature we want to add: A global total file size limit across all uploaded files.\nFor example, if the limit is 50 MB, Busboy should process data only until the total size of all uploaded files reaches 50 MB. After that, it should skip any further chunks - even those belonging to new files - until it reaches the end of the form-data stream.\nTo implement this, we‚Äôll maintain a running counter similar to fileSize, but instead of resetting it per file, we‚Äôll keep accumulating the total bytes processed across all files. Once it reaches limits.totalFileSize, we stop processing any further chunks for all files\nFor each segment identified by ssCb, the HeaderParser is invoked to inspect the Content-Disposition header. From this header, Busboy determines whether the part represents a regular form field or a file upload. If it‚Äôs a file, the filename is extracted, and the files counter is incremented.\nAs long as the number of processed files remains below limits.files, Busboy creates a new Readable stream for that file and emits a file event. This event carries both the file metadata and the associated stream, allowing your application to handle the file data as it arrives; for example, by piping it directly to a file or cloud storage.\n1const hparser = new HeaderParser((header) =\u0026gt; { 2// ... 3// ... 4if (files === filesLimit) { 5\tif (!hitFilesLimit) { 6\thitFilesLimit = true; 7\tthis.emit(\u0026#39;filesLimit\u0026#39;); 8\t} 9\tskipPart = true; 10\treturn; 11} 12 13fileSize = 0; 14this._fileStream = new FileStream(fileOpts, this); 15 16this.emit( 17 \u0026#39;file\u0026#39;, 18 partName, 19 this._fileStream, 20 { filename, 21\tencoding: partEncoding, 22\tmimeType: partType } 23); 24// ... 25// ... 26}); 27 28 29// At the other end we can listen to the `file` event on busboy instance 30 bb.on(\u0026#39;file\u0026#39;, (name, file, info) =\u0026gt; { 31 const saveTo = path.join(os.tmpdir(), `busboy-upload-${random()}`); 32 file.pipe(fs.createWriteStream(saveTo)); 33}); When the file event is triggered, the listener receives the stream of decoded file data - meaning by this point, the raw multipart byte stream from the TCP socket has been parsed and separated into meaningful file chunks by Busboy's internal StreamSearch and header parsing logic. We can also see that when the file limit is reached, Busboy sets a boolean flag hitFilesLimit and emits the filesLimit event exactly once on the Busboy instance. This ensures that the user is notified only the first time the limit is exceeded. In addition, skipPart is set to true. This flag tells the parser to ignore the rest of the current part‚Äôs body after finishing header parsing. Essentially, once we know we‚Äôve hit the maximum number of allowed files, Busboy continues scanning the incoming stream but skips over any additional file data until it encounters the next boundary. If the limits.files threshold has not been reached, the parser continues reading the body of the current file. For every incoming chunk of data, it updates the fileSize variable and ensures that only data up to the configured fileSizeLimit is passed downstream. Any excess bytes are truncated and trigger a limit event on the file stream. Here‚Äôs the corresponding section of code that handles this logic:\n1if (!skipPart) { 2 if (this._fileStream) { 3\tlet chunk; 4\tconst actualLen = Math.min(end - start, fileSizeLimit - fileSize); 5\tif (!isDataSafe) { 6\tchunk = Buffer.allocUnsafe(actualLen); 7\tdata.copy(chunk, 0, start, start + actualLen); 8\t} else { 9\tchunk = data.slice(start, start + actualLen); 10\t} 11 12\tfileSize += chunk.length; 13\tif (fileSize === fileSizeLimit) { 14\tif (chunk.length \u0026gt; 0) 15\tthis._fileStream.push(chunk); 16\tthis._fileStream.emit(\u0026#39;limit\u0026#39;); 17\tthis._fileStream.truncated = true; 18\tskipPart = true; 19\t} else if (!this._fileStream.push(chunk)) { 20\tif (this._writecb) 21\tthis._fileStream._readcb = this._writecb; 22\tthis._writecb = null; 23\t} 24} The important part here is that the limit event is fired only on the individual FileStream instance, not on the Busboy instance. This means the event listener attached to that particular file stream (usually through the 'file' event handler on Busboy) can detect that the stream has been truncated because its size exceeded the per-file limit.\nWith our new totalFileSize feature, however, we‚Äôll extend this behavior, A new event: totalSizelimit event will be fired on both the FileStream and the Busboy instance. This is because exceeding the totalFileSize means the current file is truncated and no further file data across the entire request should be processed. The event now serves as a global signal to stop consumption of further reads as there won\u0026rsquo;t be any more data parsed and pushed to the stream by busboy because of limits.\nBusboy's Back pressure Handling If you look closely at the snippet above, you‚Äôll also notice something interesting about how Busboy handles backpressure. When this._fileStream.push(chunk) returns false, it means that the internal buffer of the Readable stream (_fileStream) itself is full, and therefore it‚Äôs temporarily unable to accept more data from its upstream producer: In this case, Busboy‚Äôs Multipart writable stream. Here‚Äôs the nuance:\nThe FileStream class in Busboy is a custom Readable stream that emits the parsed file data. Busboy (the Multipart writable) acts as the producer of that readable stream‚Äôs data. When Busboy calls fileStream.push(chunk): If it returns true, it means the file stream‚Äôs internal buffer still has room it‚Äôs safe to push more data. If it returns false, the file stream‚Äôs internal highWaterMark is reached meaning it‚Äôs now buffering as much data as it can before its slow consumer (e.g., fs.createWriteStream) reads more. That ‚Äúslow consumer‚Äù can be anything on the other end of the pipe: a file write stream (disk I/O delay), a transform stream (CPU-bound operation), or even network latency if you‚Äôre piping it to another request. To handle this gracefully, Busboy performs a clever trick. When the file stream‚Äôs internal buffer is full (push() returns false), Busboy temporarily saves its _writecb - the callback passed by Node.js‚Äô Writable stream to signal readiness for the next chunk - into the file stream‚Äôs _readcb property, and then sets _writecb = null.\nLet‚Äôs see where this _writecb comes from in the first place:\n1_write(chunk, enc, cb) { 2\tthis._writecb = cb; 3\tthis._bparser.push(chunk, 0); 4\tif (this._writecb) 5\tcallAndUnsetCb(this); 6} This _write() method belongs to the Multipart class, which extends Node‚Äôs Writable stream.\nIt‚Äôs invoked automatically when the incoming request‚Äôs readable stream (for example, req) is piped into the Busboy instance like this:\n1req.pipe(bb); Here, the cb parameter in _write method represents Node‚Äôs backpressure callback.\nCalling this callback tells Node.js,\n‚ÄúI‚Äôm done processing this chunk, You can send me the next one.‚Äù\nBy saving cb into this._writecb, Busboy is essentially saying:\n‚ÄúHold on, I‚Äôll call this callback later, once I‚Äôm sure it‚Äôs safe to receive more data after parsing current chunk.‚Äù\nNow, when the file stream slows down (because its internal buffer is full), Busboy hands off this _writecb to the file stream by assigning it to _readcb.\nThis is a crucial step: it ties the writable side (Busboy) and the readable side (FileStream) together through deferred signalling. Later, when the consumer, Say, an fs.WriteStream writing to disk finishes processing some data and is ready for more, it triggers the FileStream‚Äôs _read() method:\n1_read(n) { 2\tconst cb = this._readcb; 3\tif (cb) { 4\tthis._readcb = null; 5\tcb(); 6\t} 7} This is where the ‚Äúmagic‚Äù happens.\nThat previously saved callback (cb), which originally came from the writable side (Busboy), is now invoked by the readable side (FileStream).\nThis signals back upstream that the consumer has caught up and Busboy can safely resume parsing and pushing more data.\nsequenceDiagram participant Net as Network Socket (req) participant Busboy as Busboy Multipart (Writable) participant FS as FileStream (Readable) participant Disk as fs.WriteStream (Writable) Note over Net,Busboy: Normal data flow ‚Üí Net-\u0026gt;\u0026gt;Busboy: chunk Busboy-\u0026gt;\u0026gt;FS: fileStream.push(chunk) FS-\u0026gt;\u0026gt;Disk: pipe(chunk) alt Buffer has space (push() returns true) Note over Busboy: ‚úÖ FileStream buffer has room Busboy-\u0026gt;\u0026gt;Busboy: call this._writecb()\u0026lt;br/\u0026gt;(request next chunk) else Buffer full (push() returns false) Note over FS: ‚ö†Ô∏è FileStream buffer full Busboy-\u0026gt;\u0026gt;FS: FS._readcb = this._writecb\u0026lt;br/\u0026gt;this._writecb = null Note right of FS: Wait until downstream frees buffer end Note over Disk: Disk slowly writes data... Disk--\u0026gt;\u0026gt;FS: _read() called\u0026lt;br/\u0026gt;(ready for more) FS-\u0026gt;\u0026gt;Busboy: call _readcb()\u0026lt;br/\u0026gt;(resumes parsing) Busboy-\u0026gt;\u0026gt;FS: push(next chunk) FS-\u0026gt;\u0026gt;Disk: pipe(chunk) In essence, this handoff of callbacks between _writecb and _readcb is Busboy‚Äôs internal mechanism for propagating backpressure.\nIt ensures that data flows smoothly from The req (the network socket and IncomingMessage readable) ‚Üí into Busboy‚Äôs Multipart writable ‚Üí Through the FileStream readable ‚Üí And finally into the consumer writable (like the filesystem) All without overwhelming any part of the chain.\nWrapping Up This exploration of how Busboy parses multipart form data, manages backpressure, and coordinates between readable and writable streams gave me a much deeper understanding of Node.js stream internals.\nAs part of this deep dive, As I mentioned, I also proposed a small improvement to Busboy ‚Äî adding support for a totalFileSize limit that stops processing once the total upload size crosses a configured threshold. You can check out the implementation for more details about the change here:\nüîó GitHub PR #374 ‚Äì Add total file size limit support in Busboy\n","permalink":"https://harshrai654.github.io/blogs/multipart-form-uploads---busboy-and-node-streams/","summary":"\u003cp\u003eI was recently investigating ways to improve the efficiency of file uploads to a Node.js server. This need arose after encountering a production bug where the absence of a maximum file size limit for uploads led to an out-of-memory crash due to file buffers consuming excessive heap memory. In this Node.js server, I was using Express and \u003ccode\u003eexpress-openapi-validator\u003c/code\u003e to document the server\u0026rsquo;s API with an \u003ccode\u003eOpenAPI\u003c/code\u003e specification. \u003ccode\u003eexpress-openapi-validator\u003c/code\u003e utilizes \u003ccode\u003emulter\u003c/code\u003e for file uploads. I had previously encountered this library whenever file uploads from forms needed to be handled in Node.js, but I never questioned why a separate library was necessary for file uploads. This time, I decided to go deeper to understand if a dedicated package for file uploads is truly needed, and if so, what specific benefits \u003ccode\u003eMulter\u003c/code\u003e or similar libraries provide.\nI initially needed to find a configuration option in \u003ccode\u003eexpress-openapi-validator\u003c/code\u003e to set a request-wide limit on the maximum size (in bytes) of data allowed in a request, including all file attachments.¬† The \u003ccode\u003eexpress-openapi-validator\u003c/code\u003e package offers a \u003ccode\u003efileUploader\u003c/code\u003e configuration (\u003ca href=\"https://cdimascio.github.io/express-openapi-validator-documentation/usage-file-uploader/\"\u003efileUploader documentation\u003c/a\u003e) that passes options directly to \u003ccode\u003emulter\u003c/code\u003e.\u003c/p\u003e","title":"Multipart Form Uploads - Busboy and Node Streams"},{"content":"%% Explain sequence:\nConcept of commit Index, lastApplied Index in RAFT state Explain setupLeader, lastContactFromLeader - how it helps in preventing election. A egenral flow of request from client with Start continuing with replicate method introduces in setpLeader and how CV helps in signalling for heartbeat or logs How AppendEntries as a single RPC does the jobs of both heartbeat and log replication How leader maintains next and match index and what does that mean Explain everything about replicate and AppendEntries RPC How Log correction happen reconileLogs and how we use optimisation there to reduce RPC trips Explain how applier runs on different thtread why it runs on different thread and how CV help their %% We previously discussed replicated state machines, leader election in the RAFT consensus algorithm, and log-based state maintenance. Now, we\u0026rsquo;ll focus on log replication across peer servers. We\u0026rsquo;ll also examine how RAFT ensures that the same commands are applied to the state machine at a given log index on every peer, because of the leader\u0026rsquo;s one-way log distribution to followers.\nLeader Initialisation Once a candidate becomes leader we call setupLeader function which initiates go routine for each peer in the RAFT cluster, Each go routine of respective peer is responsible for replicating new log entries or sending heartbeat via AppendEntries RPC.\n1func (rf *Raft) setupLeader() { 2\trf.mu.Lock() 3\tdefer rf.mu.Unlock() 4 5\tctx, cancel := context.WithCancel(context.Background()) 6\trf.leaderCancelFunc = cancel 7 8\tfor peerIndex := range rf.peers { 9\tif peerIndex != rf.me { 10\trf.nextIndex[peerIndex] = len(rf.log) + rf.snapshotLastLogIndex 11\tgo rf.replicate(peerIndex, ctx) 12\t} 13\t} 14 15\tgo func() { 16\tfor !rf.killed() { 17\tselect { 18\tcase \u0026lt;-ctx.Done(): 19\treturn 20\tcase \u0026lt;-time.After(HEARTBEAT_TIMEOUT): 21\trf.replicatorCond.Broadcast() 22\t} 23\t} 24\t}() 25} When starting a replication logic thread for each peer, we also send a Go context to rf.replicate. This context shows the current leader\u0026rsquo;s status. If the leader steps down, we call rf.leaderCancelFunc, which cancels the context. When a context is canceled, the ctx.Done() channel closes, stopping any waiting for results from that channel. More information about Go\u0026rsquo;s context can be found here. Our RAFT struct includes a condition variable, replicatorCond (of type *sync.Cond), that signals all peer goroutines of the leader to run the replication logic every HEARTBEAT_TIMEOUT. This ensures that a heartbeat is sent to each peer at the specified interval. A condition variable provides functions like Wait, Signal, and Broadcast. If multiple threads are waiting on a condition variable after releasing the underlying mutex, Signal will wake one of such waiting threads, and Broadcast will wake all waiting threads. Here we are using Broadcast to wake all waiting threads of each peer to send the next heartbeat. A condition variable is an operating system concept, and you can read more about the same from here. To read about the API of Go\u0026rsquo;s of type *sync.Cond, check Go\u0026rsquo;s official Docs here.\nLog Replication Each individual peer thread runs the replicate method, given below is implementation of rf.replicate function\n1func (rf *Raft) replicate(peerIndex int, ctx context.Context) { 2\tlogMismatch := false 3\tfor !rf.killed() { 4\tselect { 5\tcase \u0026lt;-ctx.Done(): 6\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Leader stepped down from leadership before initiating replicate.\\n\u0026#34;, rf.me, peerIndex) 7\treturn 8\tdefault: 9\trf.mu.Lock() 10 11\tif rf.state != StateLeader { 12\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Not a leader anymore, winding up my leadership setup.\\n\u0026#34;, rf.me, peerIndex) 13\tif rf.leaderCancelFunc != nil { 14\trf.leaderCancelFunc() 15\trf.replicatorCond.Broadcast() 16\t} 17\trf.mu.Unlock() 18\treturn 19\t} 20 21\t// Only waiting when: 22\t// - There is no log to send - In this case the wait will be signalled by the heartbeat 23\t// - We are in a continuous loop to find correct nextIndex for this peer with retrial RPCs 24\tif !logMismatch \u0026amp;\u0026amp; rf.nextIndex[peerIndex] \u0026gt;= len(rf.log)+rf.snapshotLastLogIndex { 25\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Wating for next signal to replicate.\\n\u0026#34;, rf.me, peerIndex) 26\trf.replicatorCond.Wait() 27\t} 28 29\tif rf.killed() { 30\treturn 31\t} 32 33\treply := \u0026amp;AppendEntriesReply{} 34\tlogStartIndex := rf.nextIndex[peerIndex] 35\tvar prevLogTerm int 36\tprevLogIndex := logStartIndex - 1 37\tpeer := rf.peers[peerIndex] 38 39\tif prevLogIndex-rf.snapshotLastLogIndex \u0026gt; 0 { 40\tprevLogTerm = rf.log[prevLogIndex-rf.snapshotLastLogIndex].Term 41\t} else if prevLogIndex == rf.snapshotLastLogIndex { 42\tprevLogTerm = rf.snapshotLastLogTerm 43\t} else { 44\t// prevLogIndex \u0026lt; rf.snapshotLastLogIndex 45\tprevLogTerm = -1 46\t} 47 48\tif prevLogTerm == -1 { 49\tlogMismatch = true 50\t// Leader does not have logs at `prevLogIndex` because of compaction 51\t// Leader needs to send snaphot to the peer as part of log repairing 52\targs := \u0026amp;InstallSnapshotArgs{ 53\tTerm: rf.currentTerm, 54\tLeaderId: rf.me, 55\tLastIncludedIndex: rf.snapshotLastLogIndex, 56\tLastIncludedTerm: rf.snapshotLastLogTerm, 57\tData: rf.persister.ReadSnapshot(), 58\t} 59 60\treply := \u0026amp;InstallSnapshotReply{} 61 62\tdprintf(\u0026#34;[leader-install-snapshot: %d: peer: %d]: InstallSnapshot RPC with index: %d and term: %d sent.\\n\u0026#34;, rf.me, peerIndex, args.LastIncludedIndex, args.LastIncludedTerm) 63\trf.mu.Unlock() 64 65\tok := rf.sendRPCWithTimeout(ctx, peer, peerIndex, \u0026#34;InstallSnapshot\u0026#34;, args, reply) 66 67\trf.mu.Lock() 68 69\tif ok { 70\tif reply.Term \u0026gt; rf.currentTerm { 71\tdprintf(\u0026#34;[leader-install-snapshot: %d: peer: %d]: Stepping down from leadership, Received InstallSnapshot reply from peer %d, with term %d \u0026gt; %d - my term\\n\u0026#34;, rf.me, peerIndex, peerIndex, reply.Term, rf.currentTerm) 72 73\trf.state = StateFollower 74\trf.currentTerm = reply.Term 75\trf.lastContactFromLeader = time.Now() 76 77\tif rf.leaderCancelFunc != nil { 78\trf.leaderCancelFunc() 79\trf.replicatorCond.Broadcast() 80\t} 81\trf.persist(nil) 82\trf.mu.Unlock() 83\treturn 84\t} 85 86\tdprintf(\u0026#34;[leader-install-snapshot: %d: peer: %d]: Snapshot installed successfully\\n\u0026#34;, rf.me, peerIndex) 87\trf.nextIndex[peerIndex] = rf.snapshotLastLogIndex + 1 88\trf.mu.Unlock() 89\tcontinue 90\t} else { 91\tdprintf(\u0026#34;[leader-install-snapshot: %d: peer: %d]: Snapshot installtion failed!\\n\u0026#34;, rf.me, peerIndex) 92\trf.nextIndex[peerIndex] = rf.snapshotLastLogIndex 93\trf.mu.Unlock() 94\tcontinue 95\t} 96\t} else { 97\treplicateTerm := rf.currentTerm 98 99\tlogEndIndex := len(rf.log) + rf.snapshotLastLogIndex 100\tnLogs := logEndIndex - logStartIndex 101 102\targs := \u0026amp;AppendEntriesArgs{ 103\tTerm: rf.currentTerm, 104\tLeaderId: rf.me, 105\tPrevLogIndex: prevLogIndex, 106\tPrevLogTerm: prevLogTerm, 107\tLeaderCommit: rf.commitIndex, 108\t} 109 110\tif nLogs \u0026gt; 0 { 111\tentriesToSend := rf.log[logStartIndex-rf.snapshotLastLogIndex:] 112\targs.Entries = make([]LogEntry, len(entriesToSend)) 113\tcopy(args.Entries, entriesToSend) 114\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Sending AppendEntries RPC in term %d with log index range [%d, %d).\\n\u0026#34;, rf.me, peerIndex, replicateTerm, logStartIndex, logEndIndex) 115\t} else { 116\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Sending AppendEntries Heartbeat RPC for term %d.\\n\u0026#34;, rf.me, peerIndex, replicateTerm) 117\t} 118 119\trf.mu.Unlock() 120\tok := rf.sendRPCWithTimeout(ctx, peer, peerIndex, \u0026#34;AppendEntries\u0026#34;, args, reply) 121 122\trf.mu.Lock() 123 124\tif ok { 125\tselect { 126\tcase \u0026lt;-ctx.Done(): 127\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Leader stepped down from leadership after sending AppendEntries RPC.\\n\u0026#34;, rf.me, peerIndex) 128\trf.mu.Unlock() 129\treturn 130\tdefault: 131\t// Check fot change in state during the RPC call 132\tif rf.currentTerm != replicateTerm || rf.state != StateLeader { 133\t// Leader already stepped down 134\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Checked ladership state after getting AppendEntries Reply, Not a leader anymore, Winding up my leadership setup.\\n\u0026#34;, rf.me, peerIndex) 135\tif rf.leaderCancelFunc != nil { 136\trf.leaderCancelFunc() 137\trf.replicatorCond.Broadcast() 138\t} 139\trf.mu.Unlock() 140\treturn 141\t} 142 143\t// Handle Heartbeat response 144\tif !reply.Success { 145\tif reply.Term \u0026gt; rf.currentTerm { 146\tdprintf(\u0026#34;[leader-replicate: %d: peer: %d]: Stepping down from leadership, Received ApppendEntries reply from peer %d, with term %d \u0026gt; %d - my term\\n\u0026#34;, rf.me, peerIndex, peerIndex, reply.Term, rf.currentTerm) 147 148\trf.state = StateFollower 149\trf.currentTerm = reply.Term 150\trf.lastContactFromLeader = time.Now() 151 152\tif rf.leaderCancelFunc != nil { 153\trf.leaderCancelFunc() 154\trf.replicatorCond.Broadcast() 155\t} 156\trf.persist(nil) 157\trf.mu.Unlock() 158\treturn 159\t} 160 161\t// Follower rejected the AppendEntries RPC beacuse of log conflict 162\t// Update the nextIndex for this follower 163\tlogMismatch = true 164\tfollowersConflictTermPresent := false 165\tif reply.ConflictTerm != -1 { 166\tfor i := prevLogIndex - rf.snapshotLastLogIndex; i \u0026gt; 0; i-- { 167\tif rf.log[i].Term == reply.ConflictTerm { 168\trf.nextIndex[peerIndex] = i + 1 + rf.snapshotLastLogIndex 169\tfollowersConflictTermPresent = true 170\tbreak 171\t} 172 173\t} 174 175\tif !followersConflictTermPresent { 176\trf.nextIndex[peerIndex] = reply.ConflictIndex 177\t} 178\t} else { 179\trf.nextIndex[peerIndex] = reply.ConflictIndex 180\t} 181\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Logmismatch - AppendEntries RPC with previous log index %d of previous log term %d failed. Retrying with log index:%d.\\n\u0026#34;, rf.me, peerIndex, prevLogIndex, prevLogTerm, rf.nextIndex[peerIndex]) 182\trf.mu.Unlock() 183\tcontinue 184\t} else { 185\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: responded success to AppendEntries RPC in term %d with log index range [%d, %d).\\n\u0026#34;, rf.me, peerIndex, replicateTerm, logStartIndex, logEndIndex) 186\tlogMismatch = false 187 188\tif nLogs \u0026gt; 0 { 189\t// Log replication successful 190\trf.nextIndex[peerIndex] = prevLogIndex + nLogs + 1 191\trf.matchIndex[peerIndex] = prevLogIndex + nLogs 192 193\t// Need to track majority replication upto latest log index 194\t// - So that we can update commitIndex 195\t// - Apply logs upto commitIndex 196\t// Just an idea - maybe this needs to be done separately in a goroutine 197\t// Where we continuosly check lastApplied and commitIndex 198\t// Apply and lastApplied to commit index and if leader send the response to apply channel 199\tmajority := len(rf.peers)/2 + 1 200 201\tfor i := len(rf.log) - 1; i \u0026gt; rf.commitIndex-rf.snapshotLastLogIndex; i-- { 202\tmatchedPeerCount := 1 203\tif rf.log[i].Term == rf.currentTerm { 204\tfor pi := range rf.peers { 205\tif pi != rf.me \u0026amp;\u0026amp; rf.matchIndex[pi] \u0026gt;= i+rf.snapshotLastLogIndex { 206\tmatchedPeerCount++ 207\t} 208\t} 209\t} 210 211\t// Largest possible log index greater the commitIndex replicated at majority of peers 212\t// update commitIndex 213\tif matchedPeerCount \u0026gt;= majority { 214\trf.commitIndex = i + rf.snapshotLastLogIndex 215 216\tdprintf(\u0026#34;[leader-replicate: %d | peer: %d]: Log index %d replicated to majority of peers.(%d/%d peers), updating commitIndex to : %d, current lastApplied value: %d.\\n\u0026#34;, rf.me, peerIndex, i+rf.snapshotLastLogIndex, matchedPeerCount, len(rf.peers), rf.commitIndex, rf.lastApplied) 217\trf.applyCond.Signal() 218\tbreak 219\t} 220\t} 221\t} 222 223\trf.mu.Unlock() 224\tcontinue 225\t} 226\t} 227\t} 228\tdprintf(\u0026#34;[leader-replicate: %d | peer %d]: Sending AppendEntries RPC at leader\u0026#39;s term: %d, failed. Payload prevLogIndex: %d | prevLogTerm: %d.\\n\u0026#34;, rf.me, peerIndex, replicateTerm, prevLogIndex, prevLogTerm) 229\trf.mu.Unlock() 230\tcontinue 231\t} 232\t} 233\t} 234} The replicate function operates in a continuous loop. Inside this loop, a select statement monitors the leader\u0026rsquo;s context (ctx) status. If ctx.Done() is not yet closed, it confirms the current server is still the leader. To ensure correctness, the server\u0026rsquo;s current state is also checked to be StateLeader. If it\u0026rsquo;s not, rf.leaderCanelFunc is explicitly called to relinquish leadership, which closes the context\u0026rsquo;s done channel, signalling the leader\u0026rsquo;s relinquishment to other components. Additionally, the loop waits on rf.replicatorCond when there are no more logs to transmit and no log mismatch between the leader\u0026rsquo;s and the peer\u0026rsquo;s logs. As mentioned in Part 1 of this blog series, servers periodically compact their logs to manage their size. This involves taking a snapshot of the current state and then truncating the log up to that point. If a follower\u0026rsquo;s log is significantly behind the leader\u0026rsquo;s and the leader has already truncated its log, the leader may need to send a snapshot using the InstallSnapshot RPC. We will delve deeper into log compaction in a later part of this series. Leader maintains volatile state for each peer nextIndex and matchIndex, these two properties for each peer are maintained by the leader, and it tells the leader which next log index to send to the peer and index up to which logs are replicated for that peer respectively.\nHeartbeat rules The AppendEntries RPC also functions as a heartbeat, allowing the leader to communicate its identity and log status to all followers. A follower only accepts this RPC if:\nThe leader\u0026rsquo;s Term value is greater than or equal to the follower\u0026rsquo;s current term. If not, the follower rejects the RPC, signalling that the sender (mistakenly believing itself to be the leader) is outdated. The follower sends its own reply.Term to indicate this. Upon receiving a Term in the reply that is higher than its own, the leader relinquishes leadership. The leader includes PrevLogIndex and PrevLogTerm with each AppendEntries RPC. These values are vital, as they indicate the leader\u0026rsquo;s log state by specifying the index and term of the log up to which the leader believes the follower\u0026rsquo;s log matches. If there is a discrepancy, the follower responds with reply.Success set to false, indicating a log mismatch. The leader then decreases nextIndex[peerIndex] for that peer and sends another heartbeat with an earlier log index. This process continues until a matching log index is found. At that point, the follower accepts the leader\u0026rsquo;s AppendEntries RPC and uses the reconcileLogs function to remove conflicting log entries and replace them with the log range sent by the leader from [PrevLogIndex + 1, LatestLogIndexAtLeader]. extracted from args,Entries field. This RAFT property is followed by each index of the follower, so if the current index and term match with the leader, so does the log index previous to this one, or else a conflict would have occurred previously itself. So this single property ensures that the log state matches between leader and follower up to the latest index; otherwise, the follower will not accept the heartbeat or normal AppendEntries RPC with log entries, and the to-and-fro of heartbeats after that to find the non-conflict index at the follower is called the log correction process. Later on, we will see how we can optimise this log correction because currently the leader needs to try for each log index in decreasing order, which can take a lot of time if the follower has been stale for a long time and during that time the leader\u0026rsquo;s log has grown a lot. The image above, shows a simple successful log replication situation. Here, Follower 1 and Follower 2 have the same initial part of the log as the leader. Also, the leader knows exactly which parts of the log need to be sent to each follower.\nIn the replicate method, we use a logMismatch flag within the replication loop. This flag shows if there was a log problem when sending the AppendEntries RPC in the last loop. If there was a problem, we don\u0026rsquo;t wait (we don\u0026rsquo;t call rf.replicatorCond.Wait(), which releases the lock and puts the thread to sleep). This is because, if there\u0026rsquo;s a log problem, we want to fix the log quickly, so we send the next AppendEntries RPC right away with the updated previous log index and term. If the previous log index is less than the index up to which we\u0026rsquo;ve taken a snapshot of the state and shortened the log, we send an InstallSnapshot RPC to send the snapshot to the follower instead, since we don\u0026rsquo;t have the needed logs. We\u0026rsquo;ll discuss snapshots and log compaction more later. We still follow one rule closely: if we get an RPC reply with a term value higher than the leader\u0026rsquo;s current term, the leader gives up leadership by cancelling the context. See the previous part for more information.\nIn the image, follower 1\u0026rsquo;s nextIndex is at index 7. When an AppendEntries RPC is sent with prevLogIndex=6 and prevLogTerm=2, the follower detects a log mismatch and rejects the RPC. Because the follower\u0026rsquo;s term is also 2, the leader keeps its leadership role and immediately reduces its nextIndex for follower 1 to 6. It then sends another AppendEntries RPC (without waiting due to logMismatch=true) with prevLogIndex=5 and prevLogTerm=2. This second RPC is accepted. The leader sends Entries=[1, 2], causing the entries after index 5 to be replaced by these new entries (handled by ). Thus, the leader can erase uncommitted entries from the follower during log correction. Follower 2 accepts the initial AppendEntries RPC because there is no conflict. Since The leader manages each peer in its own separate goroutine, After each successful RPC response, the leader checks if a majority (replication factor) has replicated the log up to a specific index. In this scenario, once follower 2 responds (likely before follower 1, which is still correcting its log), the leader will have replicated log index 7 on 2/3 of the peers (including itself).\nThe leader then updates its commitIndex and signals a condition variable called rf.applyCond. First, we will examine how the follower handles the AppendEntries RPC.¬†Then, building on this understanding, we will discuss commitIndex, lastApplied, and rf.applyCond, explaining how they ensure log entries are committed, what \u0026ldquo;committed\u0026rdquo; means, what \u0026ldquo;applying\u0026rdquo; a log means, and how it is handled.\u0026quot;\nHandling AppendEntries RPC AppendEntries code and expalantion reconcileLogs lastContactFromLeader Committing and Applying Log Entries ``\nReferences https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf Log Replication Visualisation https://thesquareplanet.com/blog/students-guide-to-raft/ https://pages.cs.wisc.edu/~remzi/OSTEP/threads-cv.pdf ","permalink":"https://harshrai654.github.io/blogs/building-fault-tolerant-kv-storage-system---part-2/","summary":"\u003cp\u003e%% Explain sequence:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConcept of commit Index, lastApplied Index in RAFT state\u003c/li\u003e\n\u003cli\u003eExplain setupLeader, lastContactFromLeader - how it helps in preventing election.\u003c/li\u003e\n\u003cli\u003eA egenral flow of request from client with Start continuing with\u003c/li\u003e\n\u003cli\u003e\u003cdel\u003ereplicate method introduces in setpLeader and how CV helps in signalling for heartbeat or logs\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e\u003cdel\u003eHow AppendEntries as a single RPC does the jobs of both heartbeat and log replication\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e\u003cdel\u003eHow leader maintains next and match index and what does that mean\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003eExplain everything about replicate and AppendEntries RPC\u003c/li\u003e\n\u003cli\u003eHow Log correction happen \u003ccode\u003ereconileLogs\u003c/code\u003e  and how we use optimisation there to reduce RPC trips\u003c/li\u003e\n\u003cli\u003eExplain how applier runs on different thtread why it runs on different thread and how CV help their %%\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe \u003ca href=\"/blogs/building-fault-tolerant-kv-storage-system---part-1/\"\u003epreviously discussed\u003c/a\u003e replicated state machines, leader election in the RAFT consensus algorithm, and log-based state maintenance. Now, we\u0026rsquo;ll focus on log replication across peer servers. We\u0026rsquo;ll also examine how RAFT ensures that the same commands are applied to the state machine at a given log index on every peer, because of the leader\u0026rsquo;s one-way log distribution to followers.\u003c/p\u003e","title":"Building Fault Tolerant KV Storage System - Part 2"},{"content":"This blog post is part of a series detailing my implementation of a fault-tolerant key-value server using the RAFT consensus protocol. Before diving into RAFT and its mechanics, it\u0026rsquo;s crucial to grasp the concept of a replicated state machine and its significance in building systems that are both fault-tolerant and highly available.\nReplicated State Machine A replicated state machine is essentially a collection of identical machines working together. One machine acts as the \u0026ldquo;master,\u0026rdquo; handling client requests and dictating the system\u0026rsquo;s operations. The other machines, the \u0026ldquo;replicas,\u0026rdquo; diligently copy the master\u0026rsquo;s state. This \u0026ldquo;state\u0026rdquo; encompasses all the data necessary for the system to function correctly, remembering the effects of previous operations. Think of a key-value store: the state would be the key-value pairs themselves, replicated across all machines to maintain consistency and resilience. Having the master\u0026rsquo;s state copied across multiple machines enables us to use these replicas in several ways. They can take over as the new master if the original fails, or handle read operations to reduce the load on the master. Because the state is copied across machines, network issues like latency, packet loss, and packet reordering significantly affect how closely a replica\u0026rsquo;s state matches the master\u0026rsquo;s. The difference between the master\u0026rsquo;s (most up-to-date) state and a replica\u0026rsquo;s state is known as replication lag. Generally, we aim to minimize this lag, and different systems offer varying levels of consistency (which we will discuss later when covering replication in RAFT).\nAside from application-level replication, which primarily requires context of the state needed for replication or enforces a set of rules for the state that can be replicated, another approach involves replicating the entire machine state at the instruction level. This includes replicating instruction outputs and interrupt behaviour.¬†This method ensures that machines execute the same set and order of instructions and maintain identical memory pages, resulting in an exact replica of the entire machine. However, this approach is more challenging to control, as managing interrupts, I/O, and replicating them to other machines is complex. An example of such an approach is discussed in The Design of a Practical System for Fault-Tolerant Virtual Machines_. This paper details the approach they followed when designing a hypervisor to capture and replicate the state of guest machines.\nRAFT - A Consensus Algorithm RAFT is a consensus algorithm for managing a replicated log. Consensus algorithms allow a collection of machines to work as a group that can survive failures of some of its members. A replicated state is generally maintained as a replicated log. Each server maintains its own copy of logs, and keeping the replicated log consistent is the job of the consensus algorithm.\nLet\u0026rsquo;s take an example of an operation done on a key-value store. A client sends a command like PUT x=2, which is received by the master server of the group. The consensus module of the server receives this command and appends it to its log. The master\u0026rsquo;s consensus module communicates with other servers about this new log and ensures that each server\u0026rsquo;s log contains the same command in the same order. Once commands are replicated, each server processes the command on its own state machine, and since the log is the same on each server, the final state on each server results in the same output. As a result, the servers appear to form a single, highly reliable state machine.\nRAFT implements this consensus between all servers by electing a leader and giving it the responsibility to decide the sequence of log operation that will be appended and propagated to each member. Flow of logs only happen in one direction from leader to other servers so if a particular server\u0026rsquo;s sequence does not match to leader\u0026rsquo;s sequence of logs, leader can instruct the follower (replica) server to erase its log and strictly follow leader itself.\nFrom the RAFT paper: Given the leader approach, Raft decomposes the consensus problem into three relatively independent subproblems, which are discussed in the subsections that follow:\nLeader election: a new leader must be chosen when an existing leader fails. Log replication: the leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with its own. Safety: if any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index. Logs, State Lifecycle and RPCs Each LogEntry consists of the command it contains and the term value. 1type LogEntry struct { 2\tCommand interface{} 3\tTerm int 4} Raft divides time into terms of arbitrary length. Terms are numbered with consecutive integers. Each term begins with an election, in which one or more candidates attempt to become leader.\nEach server has a role, with only one designated as the Leader for a specific period. The remaining servers are either Followers, who heed the Leader\u0026rsquo;s communications, or Candidates. Candidates solicit votes from other servers if they haven\u0026rsquo;t received communication from a Leader within a set timeframe.\n1const ( 2\tStateFollower ServerState = iota 3\tStateLeader 4\tStateCandidate 5) Due to variations in timing, different servers might see the changes between terms at different moments. A server might also miss an election or even entire terms. In Raft, terms serve as a logical clock, enabling servers to identify outdated information like old leaders. Every server keeps track of a current term number, which consistently increases. Servers exchange current terms during communication; if one server has a smaller term than another, it updates to the larger term. Should a candidate or leader find its term is outdated, it immediately becomes a follower. A server will reject any request that includes an old term number.\nThe diagram below illustrates how a server\u0026rsquo;s role changes under different circumstances. To grasp the role of elections and how Term serves as a time marker in RAFT, we must first understand\nHow the threshold time is determined, after which a follower can become a candidate and start an election. How each server of the cluster communicates with one another Imagine a cluster of three servers experiencing a network partition, where server 0, the current leader, cannot communicate with servers 1 and 2. If the servers have a fixed timeout value for election, multiple servers might initiate elections simultaneously, resulting in no majority each time, with each server only receiving its own vote. Consequently, the cluster cannot decide on a leader for the next term. With each re-election, a server\u0026rsquo;s term value (also known as currentTerm) increases by 1. So, when the previous server 0, from, say, term 1, rejoins the cluster after recovery, it will vote for either server 1 or 2, as both are requesting votes for their own currentTerm value, which will now be N if N elections have occurred after server 0 went down, with each server starting its own election and failing to become leader due to the lack of a majority. Even if server 0 votes for one server and a majority is achieved, the new leader needs to communicate with all peers, and if that doesn\u0026rsquo;t happen quickly, we\u0026rsquo;ll see another election. Overall, the cluster will be unstable and unable to progress due to election instability.\nTo resolve this, we introduce a small element of randomness to each server\u0026rsquo;s election timeout. This minimizes the chance of multiple servers starting elections simultaneously and maximizes the likelihood of a single server triggering a re-election and continuing as leader for subsequent terms.\n1func Make(peers []*labrpc.ClientEnd, me int, 2\tpersister *tester.Persister, applyCh chan raftapi.ApplyMsg) raftapi.Raft { 3\t... 4\t// other state initialisation, we will see them later 5\t... 6\t7\trf.electionTimeout = time.Duration(1500+(rand.Int63()%1500)) * time.Millisecond 8\tgo rf.ticker() 9\t... 10\t// other state initialisation, we will see them later 11\t... 12\t13\t} 14\t15func (rf *Raft) ticker() { 16\tfor !rf.killed() { 17\trf.mu.Lock() 18\tif rf.state != StateLeader \u0026amp;\u0026amp; time.Since(rf.lastContactFromLeader) \u0026gt;= rf.electionTimeout { 19\tgo rf.startElection() 20\t} 21\trf.mu.Unlock() 22 23\t// pause for a random amount of time between 50 and 350 24\t// milliseconds. 25\theartbeatMS := 50 + (rand.Int63() % 300) // [50, 350)ms time range 26\ttime.Sleep(time.Duration(heartbeatMS) * time.Millisecond) 27\t} 28} Each RAFT server\u0026rsquo;s state (We will discuss each state in detail later on):\n1type Raft struct { 2\tmu sync.Mutex // Lock to protect shared access to this peer\u0026#39;s state 3\tpeers []*labrpc.ClientEnd // RPC end points of all peers 4\tpersister *tester.Persister // Object to hold this peer\u0026#39;s persisted state 5\tme int // this peer\u0026#39;s index into peers[] 6\tdead int32 // set by Kill() 7 8\t// Persisted State 9\tcurrentTerm int // latest term server has seen 10\tvotedFor int // candidateId that received vote in current term 11\tlog []LogEntry // log entries; each entry contains command for state machine, and term when entry was received by leader 12\tsnapshotLastLogIndex int 13\tsnapshotLastLogTerm int 14\t// snapshot []byte 15 16\t// Volatile state 17\tcommitIndex int // index of highest log entry known to be committed 18\tlastApplied int // index of highest log entry applied to state machine 19\tstate ServerState // role of this server 20\tlastContactFromLeader time.Time // Last timestamp at which leader sent heartbeat to current server 21\telectionTimeout time.Duration // time duration since last recieved heartbeat after which election will be trigerred by this server 22\tapplyCh chan raftapi.ApplyMsg // Channel where a raft server sends it commands to be applied to state machine 23 24\t// Volatile leader state 25\tnextIndex []int //\tfor each server, index of the next log entry to send to that server 26\tmatchIndex []int //\tfor each server, index of highest log entry known to be replicated on server 27\tapplyCond *sync.Cond // Condition validable to signal applier channel to send commands to apply channel 28 29\tleaderCancelFunc context.CancelFunc // Go context for a leader, called when we need to cancel leader\u0026#39;s context and leader is stepping down 30\treplicatorCond *sync.Cond // Leader\u0026#39;s conditon variable to signal replicator threads for each peer to either send heartbeat or new logs to each peer 31} For the communication between servers, RAFT consensus algorithm uses RPC for\nRequesting vote from other peers - RequestVote RPC Propagate changes to log entries from leader to followers - AppendEntries RPC Sending heartbeats from leader to follower - AppendEntries RPC with empty log data RPC, or Remote Procedure Call, is a programming paradigm that allows a program to execute a procedure (function, method) on a different machine as if it were a local procedure call.¬†Essentially, it\u0026rsquo;s a way to build distributed systems where one program (the client) can request a service from another program (the server) over a network. Go provides net/rpc package which abstract most of the work related to serialization of RPC arguments and deserialization at receiving end and the underlying network call with provided data, to know more check Go\u0026rsquo;s net/rpc package RPC arguments and reply structs as also shown in the original paper (page 4, fig 2)\n1type RequestVoteArgs struct { 2\tTerm int // candidate‚Äôs term 3\tCandidateId int // candidate requesting vote 4\tLastLogIndex int // index of candidate‚Äôs last log entry 5\tLastLogTerm int // term of candidate‚Äôs last log entry 6} 7 8type RequestVoteReply struct { 9\tTerm int //currentTerm, for candidate to update itself in case someone else is leader now 10\tVoteGranted bool // true means candidate received vote 11} 12 13// Invoked by leader to replicate log entries; also used as heartbeat. 14type AppendEntriesArgs struct { 15\tTerm int // leader‚Äôs term 16\tLeaderId int // so follower can redirect clients 17\tPrevLogIndex int // index of log entry immediately preceding new ones 18\tPrevLogTerm int // term of prevLogIndex entry 19\tEntries []LogEntry // log entries to store (empty for heartbeat; may send more than one for efficiency) 20\tLeaderCommit int // leader‚Äôs commitIndex 21} 22 23type AppendEntriesReply struct { 24\tTerm int // currentTerm, for leader to update itself 25\tSuccess bool // true if follower contained entry matching prevLogIndex and prevLogTerm 26\tConflictIndex int // Followers index which is conflicting with leader\u0026#39;s prevLogIndex 27\tConflictTerm int // Followers term of conflicting log 28} 29 30type InstallSnapshotArgs struct { 31\tTerm int 32\tLeaderId int 33\tLastIncludedIndex int 34\tLastIncludedTerm int 35\tData []byte 36} Rules Of Election As we have already seen, an election timeout triggers an election by a given server, which is a Follower. This timeout occurs when the Follower does not receive any AppendEntries RPCs (even empty ones, containing no log) from the leader. When an election is initiated with rf.startElection(), the follower increments its currentTerm by 1 and issues RequestVote RPCs to each of its peers in parallel, awaiting their responses. At this point, three outcomes are possible:\nThe Follower gains a majority and becomes the leader. In this case, the Follower converts to the Leader state, and setupLeader method sets up rf.replicate go routine for each of the other peers in a separate go routine. These go routines are responsible for sending heartbeats and logs using AppendEntries RPC calls, We use condition variable replicatorCond to signal these waiting go threads either when new log entry comes up or when heartbeat timeout occurs. While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be the leader. If the leader‚Äôs term (included in its RPC) is at least as large as the candidate‚Äôs current term, then the candidate recognizes the leader as legitimate and of newer term hence reverts to the follower state. If the term in the RPC is smaller than the candidate‚Äôs current term, the candidate rejects the RPC and remains in the candidate state. A candidate neither wins nor loses the election. If many followers become candidates simultaneously, votes could be split, preventing any single candidate from obtaining a majority. When this happens, each candidate will time out and start a new election by incrementing its term and initiating another round of RequestVote RPCs. The randomness in the election timeout helps prevent split votes from happening indefinitely. According to the current rules, a server receiving a RequestVote RPC with a Term greater than its own should grant its vote. However, this could lead to an outdated server with an incomplete log becoming leader. Since the leader is responsible for log propagation and can overwrite follower logs, it\u0026rsquo;s possible for such an outdated leader to erase already committed logs for which clients have received responses - an undesirable outcome. Re-examining the RequestVoteArgs reveals that, in addition to Term, the struct includes LastLogIndex and LastLogTerm, representing the candidate\u0026rsquo;s last log entry\u0026rsquo;s index and term, respectively. These values help determine if the candidate\u0026rsquo;s log contains at least the latest committed entries. The rules for verifying this when voting are straightforward: Raft determines the more up-to-date of two logs by comparing the index and term of their last entries. If the last entries have different terms, the log with the latter term is considered more up-to-date. If the logs share the same last term, the longer log is deemed more up-to-date.\nLet\u0026rsquo;s understand how RAFT prevents a stale server from winning an election and becoming a leader with the help of an example: consider a follower A that gets partitioned away from the rest of the cluster. Its election timeout fires, so it increments its term and starts an election by sending RequestVote RPCs. Since it cannot reach the majority, it doesn‚Äôt become leader. Meanwhile, the rest of the cluster still has a leader B. Because B can talk to a majority of servers, it continues to accept new log entries, replicate them, and safely commit them. Remember: in Raft, an entry is considered committed only after it is stored on a majority of servers. Later, when connectivity is restored, server A now has a higher term than B. This causes A to reject AppendEntries from B, forcing B to step down. At this moment, no leader exists until a new election is held. Here‚Äôs where Raft‚Äôs rules keep the system safe:\nA cannot win leadership election because its log is missing committed entries that the majority already agreed on. Other servers will refuse to vote for it by comparing their latest log index and term with RequestVote RPC Arg\u0026rsquo;s LastLogIndex and LastLogTerm. A new leader must have logs that are at least as up-to-date as the majority. This ensures that committed entries are never lost. Once a new leader is elected, it brings A back in sync by replicating the missing committed entries. This scenario highlights how Raft‚Äôs election rules preserve correctness: even if a partitioned follower returns with a higher term, it cannot override the majority‚Äôs progress. Leadership always ends up with a server that reflects all committed entries, and the cluster converges to the same state. In upcoming parts, we‚Äôll dive deeper into Raft‚Äôs log replication process and how heartbeats help keep leaders and followers synchronized. Given below is the implementation for RequestVote which highlights all the restrictions and responses for various election restrictions. 1func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { 2\trf.mu.Lock() 3\tdefer rf.mu.Unlock() 4 5\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Candidate %d seeking vote for term: %d.\\n\u0026#34;, rf.me, args.CandidateId, args.Term) 6 7\t// Election voting restrictions for follower 8\t// - Candidate\u0026#39;s term is older than follower from whom it is seeking vote 9\t// - Follower already voted 10\t// - Candidate\u0026#39;s log is older then the follower 11\t// In all the above cases follower will not vote for the candidate and respond back with its current term 12\t// for the candidate to roll back to follower 13\tisCandidateOfOlderTerm := args.Term \u0026lt; rf.currentTerm 14 15\tif isCandidateOfOlderTerm { 16\treply.Term = rf.currentTerm 17\treply.VoteGranted = false 18 19\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Candidate %d is of older term. Candidate\u0026#39;s term: %d | My current term %d\\n\u0026#34;, rf.me, args.CandidateId, args.Term, rf.currentTerm) 20 21\treturn 22\t} else { 23\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Candidate %d is of newer or equal term. Candidate\u0026#39;s term: %d | My current term %d\\n\u0026#34;, rf.me, args.CandidateId, args.Term, rf.currentTerm) 24 25\tif args.Term \u0026gt; rf.currentTerm { 26\tif rf.state == StateLeader { 27\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Recieved vote request from candiate of higher term, winding up my own leadership setup.\\n\u0026#34;, rf.me) 28\tif rf.leaderCancelFunc != nil { 29\trf.leaderCancelFunc() 30\trf.replicatorCond.Broadcast() 31\t} 32\t} 33 34\trf.currentTerm = args.Term 35\trf.state = StateFollower 36\trf.votedFor = -1 37 38\trf.persist(nil) 39\t} 40 41\tcanVote := rf.votedFor == -1 || rf.votedFor == args.CandidateId 42\tvar currentLatestLogTerm int 43\tcurrentLatestLogIndex := len(rf.log) - 1 44 45\tif currentLatestLogIndex \u0026gt; 0 { 46\tcurrentLatestLogTerm = rf.log[currentLatestLogIndex].Term 47\t} else if rf.snapshotLastLogIndex \u0026gt; 0 { 48\tcurrentLatestLogTerm = rf.snapshotLastLogTerm 49\t} 50 51\tcurrentLatestLogIndex += rf.snapshotLastLogIndex 52 53\tisCandidateLogOlder := args.LastLogTerm \u0026lt; currentLatestLogTerm || (args.LastLogTerm == currentLatestLogTerm \u0026amp;\u0026amp; args.LastLogIndex \u0026lt; currentLatestLogIndex) 54 55\tif canVote \u0026amp;\u0026amp; !isCandidateLogOlder { 56\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Granted vote for term: %d, To candidate %d.\\n\u0026#34;, rf.me, args.Term, args.CandidateId) 57\trf.votedFor = args.CandidateId 58\trf.lastContactFromLeader = time.Now() 59 60\treply.VoteGranted = true 61\trf.persist(nil) 62\t} else { 63\tfmt.Printf(\u0026#34;[Peer: %d | RequestVote]: Candidate %d log is older than mine. Log(index/term): Candidate\u0026#39;s: (%d, %d) | Mine: (%d, %d).\\n\u0026#34;, rf.me, args.CandidateId, args.LastLogIndex, args.LastLogTerm, currentLatestLogIndex, currentLatestLogTerm) 64\treply.VoteGranted = false 65\t} 66 67\treply.Term = rf.currentTerm 68\t} 69} Here are some things to keep in mind when implementing the RequestVote RPC:\nNote the use of snapshotLastLogIndex and snapshotLastLogTerm, which relate to log compaction. Think of a snapshot as capturing the current state machine\u0026rsquo;s image, allowing us to shorten logs up to that point, reducing overall log size. We\u0026rsquo;ll explore how this works and its benefits later. For now, understand that a server, when verifying if a candidate has current logs, needs to read its own. If the log is truncated shortly after a snapshot, we store the snapshot\u0026rsquo;s last details, like the index and term of the log at that index. Snapshotting generally shortens the log, but because indexes always increase, we use snapshotLastLogIndex as an offset to get the right index. When a candidate gets a majority, it calls setupLeader, creating a context that can be cancelled, using Go\u0026rsquo;s context package. This context returns a function, leaderCancelFunc, which, when called, cancels the context. We do this when a leader steps down, such as when it receives a RequestVote RPC from a candidate with a higher term. In this case, we cancel the leader\u0026rsquo;s context. This is useful when the leader is performing async operations (like sending heartbeats or logs) and waiting for them. We then wait for the operation to complete or the context to be cancelled, signalling that we no longer need to wait because the current server is no longer the leader. We\u0026rsquo;ll see what happens when a leader\u0026rsquo;s context is cancelled later. A potentially confusing aspect is that when we receive a RequestVote RPC response denying the vote and containing a Term greater than our current one, we examine the candidate\u0026rsquo;s current state. This state might no longer be \u0026ldquo;candidate\u0026rdquo; because the RequestVote RPC could be delayed, and the node might have already gained a majority and become the leader. Despite any RPC delays, we strictly adhere to a core Raft principle: upon receiving an RPC response with a Term greater than our current Term, we immediately update our Term to match the response. If we are the leader, we step down. This rule is crucial because the Term serves as a time indicator for the entire Raft cluster. Discovering that time has progressed requires us to adapt accordingly. So to summarize: When a follower receives a RequestVote, it rejects the request if:\nThe candidate‚Äôs term is smaller (args.Term \u0026lt; rf.currentTerm). It has already voted for another candidate in this term (rf.votedFor != -1 \u0026amp;\u0026amp; rf.votedFor != args.CandidateId). The candidate‚Äôs log is less up-to-date than its own, according to Raft‚Äôs freshness rule: Candidate‚Äôs last log term must be greater, or equal with a log index at least as large. We have already seen how ticker calls startElection method based on election timeout, Let\u0026rsquo;s now understand with given below implementation of startElection, How a candidate asks for votes and what are the edge cases to consider while waiting for RequestVote RPC responses\n1func (rf *Raft) startElection() { 2\trf.mu.Lock() 3 4\t// Tigger election, send RequestVote RPC 5\t// Once you have voted for someone in a term the elction timeout should be reset 6\t// Reset election timer for self 7\trf.lastContactFromLeader = time.Now() 8 9\t// Reset the election timeout with new value 10\trf.electionTimeout = time.Duration(1500+(rand.Int63()%1500)) * time.Millisecond 11\trf.currentTerm += 1 // increase term 12\trf.state = StateCandidate 13\tpeerCount := len(rf.peers) 14 15\tvoteCount := 1 // self vote 16\tlastLogIndex := len(rf.log) - 1 17\tvar lastLogTerm int 18 19\tdone := make(chan struct{}) 20 21\tif lastLogIndex \u0026gt; 0 { 22\tlastLogTerm = rf.log[lastLogIndex].Term 23\t} else if rf.snapshotLastLogIndex \u0026gt; 0 { 24\tlastLogTerm = rf.snapshotLastLogTerm 25\t} 26 27\tlastLogIndex += rf.snapshotLastLogIndex 28 29\trf.persist(nil) 30 31\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Election timout! Initiating election for term %d, with lastLogIndex: %d \u0026amp; lastLogTerm: %d.\\n\u0026#34;, rf.me, rf.currentTerm, lastLogIndex, lastLogTerm) 32\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Election timeout reset to: %v.\\n\u0026#34;, rf.me, rf.electionTimeout) 33 34\targs := \u0026amp;RequestVoteArgs{ 35\tTerm: rf.currentTerm, 36\tCandidateId: rf.me, 37\tLastLogIndex: lastLogIndex, 38\tLastLogTerm: lastLogTerm, 39\t} 40\trequestVoteResponses := make(chan *RequestVoteReply) 41 42\tfor peerIndex, peer := range rf.peers { 43\tif peerIndex != rf.me { 44\tgo func(peer *labrpc.ClientEnd) { 45\tselect { 46\tcase \u0026lt;-done: 47\t// Either majority is achieved or candidate is stepping down as candidate 48\t// Dont wait for this peer\u0026#39;s RequestVote RPC response and exit this goroutine 49\t// to prevent goroutine leak 50\treturn 51\tdefault: 52\treply := \u0026amp;RequestVoteReply{} 53\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Requesting vote from peer: %d.\\n\u0026#34;, rf.me, peerIndex) 54\tok := peer.Call(\u0026#34;Raft.RequestVote\u0026#34;, args, reply) 55\tif ok { 56\tselect { 57\tcase requestVoteResponses \u0026lt;- reply: 58\tcase \u0026lt;-done: 59\treturn 60\t} 61\t} 62\t} 63\t}(peer) 64\t} 65\t} 66 67\t// Releasing the lock after making RPC calls 68\t// Each RPC call for RequestVote is in its own thread so its not blocking 69\t// We can release the lock after spawning RequestVote RPC thread for each peer 70\t// Before releasing the lock lets make copy of some state to verify sanity 71\t// After reacquiring the lock 72\telectionTerm := rf.currentTerm 73\trf.mu.Unlock() 74 75\tmajority := peerCount/2 + 1 76 77\tfor i := 0; i \u0026lt; peerCount-1; i++ { 78\tselect { 79\tcase res := \u0026lt;-requestVoteResponses: 80\tif rf.killed() { 81\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Candidate killed while waiting for peer RequestVote response. Aborting election process.\\n\u0026#34;, rf.me) 82\tclose(done) // Signal all other RequestVote goroutines to stop 83\treturn 84\t} 85 86\trf.mu.Lock() 87 88\t// State stale after RequestVote RPC 89\tif rf.currentTerm != electionTerm || rf.state != StateCandidate { 90\trf.mu.Unlock() 91\tclose(done) 92\treturn 93\t} 94 95\tif res.Term \u0026gt; rf.currentTerm { 96\t// A follower voted for someone else 97\t// If they voted for same term then we can ignore 98\t// But if term number is higher than our current term then 99\t// we should step from candidate to follower and update our term as well 100\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Stepping down as Candidate, Recieved RequestVoteReply with term value %d \u0026gt; %d - my currentTerm.\\n\u0026#34;, rf.me, res.Term, rf.currentTerm) 101 102\trf.currentTerm = res.Term 103\trf.state = StateFollower 104\trf.mu.Unlock() 105 106\trf.persist(nil) 107\tclose(done) 108\treturn 109\t} 110 111\tif res.VoteGranted { 112\tvoteCount++ 113\tif voteCount \u0026gt;= majority { 114\t// Won election 115\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Election won with %d/%d majority! New Leader:%d.\\n\u0026#34;, rf.me, voteCount, peerCount, rf.me) 116\trf.state = StateLeader 117 118\trf.mu.Unlock() 119\tclose(done) 120 121\trf.setupLeader() 122\treturn 123\t} 124\t} 125 126\trf.mu.Unlock() 127 128\tcase \u0026lt;-time.After(rf.electionTimeout): 129\trf.mu.Lock() 130\tfmt.Printf(\u0026#34;[Candidate: %d | Election Ticker]: Election timeout! Wrapping up election for term: %d. Got %d votes. Current state = %d. Current set term: %d.\\n\u0026#34;, rf.me, electionTerm, voteCount, rf.state, rf.currentTerm) 131\trf.mu.Unlock() 132 133\tclose(done) 134\treturn 135\t} 136 137\t} 138} Within startElection, we construct RequestVoteArgs and concurrently dispatch the RPC to every peer using separate goroutines for each call. A done channel is also provided to these goroutines to signal events such as:\nElection cancellation, occurring if the Candidate reverts to Follower status, potentially after sending initial RequestVote RPCs upon receiving a heartbeat OR a RequestVote RPC from another peer whose Term is at least the Candidate\u0026rsquo;s current Term. Cancellation of waiting for RequestVote RPC responses if a majority has been secured or the election timeout is reached. It\u0026rsquo;s worth noting that after sending RPC calls to each peer, we release the lock. This prevents us from blocking other incoming messages to that peer while awaiting RPC responses. This is why we use the done Go channel. It ensures that any concurrent request from elsewhere that modifies the candidate\u0026rsquo;s status is notified. This happens when the done channel is closed, causing the case \u0026lt;-done: statement to return first in the select block. 6.5840 Labs provide us with following test cases for leader election:\n1‚ùØ go test -run 3A 2Test (3A): initial election (reliable network)... 3 ... Passed -- time 5.5s #peers 3 #RPCs 36 #Ops 0 4Test (3A): election after network failure (reliable network)... 5 ... Passed -- time 8.4s #peers 3 #RPCs 50 #Ops 0 6Test (3A): multiple elections (reliable network)... 7 ... Passed -- time 16.9s #peers 7 #RPCs 382 #Ops 0 8PASS 9ok 6.5840/raft1 31.352s Conclusion In this part we understood how RAFT manages a replicated log across a cluster of machines, ensuring consistency and availability. The post details the roles of leader, follower, and candidate, along with the key concepts of terms, log entries, leader election, and log replication. Crucial mechanisms, such as checks on RequestVote and AppendEntries RPCs and random timeouts, guarantee leader accuracy and prevent split votes. The post lays the groundwork for understanding how RAFT ensures that committed log entries are never lost and how a valid leader is reliably elected.\nIn subsequent parts, we will see how we set up a leader when a candidate wins election and how we handle leader stepping down from leadership. Then we will see how log replication actually happens along with cases of log conflicts and log corrections by leader and how heartbeats helps to achieve that, In the end we will trace a client request to see the behaviour of this distributed cluster seen as a single machine from client\u0026rsquo;s point of view.\nReferences https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf Leader Election Visualization https://thesquareplanet.com/blog/students-guide-to-raft/ Implementation: https://github.com/harshrai654/6.5840/blob/master/src/raft1/raft.go ","permalink":"https://harshrai654.github.io/blogs/building-fault-tolerant-kv-storage-system---part-1/","summary":"\u003cp\u003eThis blog post is part of a series detailing my implementation of a fault-tolerant key-value server using the RAFT consensus protocol.\nBefore diving into RAFT and its mechanics, it\u0026rsquo;s crucial to grasp the concept of a replicated state machine and its significance in building systems that are both fault-tolerant and highly available.\u003c/p\u003e\n\u003ch2 id=\"replicated-state-machine\"\u003eReplicated State Machine\u003c/h2\u003e\n\u003cp\u003eA replicated state machine is essentially a collection of identical machines working together. One machine acts as the \u0026ldquo;master,\u0026rdquo; handling client requests and dictating the system\u0026rsquo;s operations. The other machines, the \u0026ldquo;replicas,\u0026rdquo; diligently copy the master\u0026rsquo;s \u003cem\u003estate\u003c/em\u003e. This \u0026ldquo;state\u0026rdquo; encompasses all the data necessary for the system to function correctly, remembering the effects of previous operations. Think of a key-value store: the \u003cem\u003estate\u003c/em\u003e would be the key-value pairs themselves, replicated across all machines to maintain consistency and resilience.\nHaving the master\u0026rsquo;s state copied across multiple machines enables us to use these replicas in several ways. They can take over as the new master if the original fails, or handle read operations to reduce the load on the master. Because the state is copied across machines, network issues like latency, packet loss, and packet reordering significantly affect how closely a replica\u0026rsquo;s state matches the master\u0026rsquo;s. The difference between the master\u0026rsquo;s (most up-to-date) state and a replica\u0026rsquo;s state is known as replication lag. Generally, we aim to minimize this lag, and different systems offer varying levels of consistency (which we will discuss later when covering replication in RAFT).\u003c/p\u003e","title":"Building Fault Tolerant KV Storage System - Part 1"},{"content":"This article shares learnings from Google\u0026rsquo;s influential MapReduce paper and explores the challenges encountered while implementing a simplified version. Our system uses multiple worker processes, running on a single¬†machine and communicating via RPC, to mimic key aspects of a distributed environment.\nWhat is Map-Reduce At its core, MapReduce is a programming model and an associated¬†framework for processing and generating massive datasets using a parallel, distributed algorithm, typically on a cluster of computers. You might already be familiar¬†with¬†map¬†and¬†reduce¬†operations from functional programming languages. For instance, in JavaScript,¬†array.map()¬†transforms¬†each element of an array independently based on a¬†mapper¬†function, while¬†array.reduce()¬†iterates through an array, applying a¬†reducer¬†function to accumulate its elements into a single output value (e.g., a sum, or¬†a new, aggregated object).\nThe MapReduce paradigm, brilliantly scales these fundamental¬†concepts to tackle data processing challenges that are orders of magnitude larger than what a single machine can handle. The general flow typically¬†involves several key stages:\nSplitting:¬†The vast input dataset is initially divided¬†into smaller, independent chunks. Each chunk will be processed by a Map task.\nMap Phase:¬†A user-defined¬†Map¬†function is applied to each input chunk in parallel across many worker machines. The¬†Map¬†function¬†takes an input pair (e.g., a document ID and its content) and produces a set of intermediate key/value pairs. For example, in a word count application, a¬†Map¬†function might take a line of text and output a¬†key/value pair for each word, like¬†(word, 1).\nShuffle and Sort¬†Phase:¬†This is a critical intermediate step. The framework gathers all intermediate key/value pairs produced by the Map tasks, sorts them¬†by key, and groups together all values associated with the same intermediate key. This ensures that all occurrences of¬†(word,¬†1)¬†for a specific \u0026lsquo;word\u0026rsquo; are brought to the same place for the next phase.\nReduce Phase:¬†A user-defined¬†Reduce¬†function then processes the grouped data for each unique key, also in parallel. The¬†Reduce¬†function takes an intermediate key and a list of all values associated with that key. It iterates through these values to¬†produce a final output, often zero or one output value. Continuing the word count example, the¬†Reduce¬†function for a¬†given¬†word¬†would receive¬†(word, [1, 1, 1, \u0026hellip;])¬†and sum these¬†ones to produce the total count, e.g.,¬†(word, total_count).\nThis distributed approach is¬†highly effective for several reasons:\nScalability:¬†It allows for horizontal scaling, you can process more data faster¬†by simply adding more machines to your cluster.\nParallelism:¬†It inherently parallelizes computation, significantly speeding up processing¬†times for large tasks.\nFault Tolerance:¬†The MapReduce framework is designed to handle machine failures automatically by¬†re-executing failed tasks, which is crucial when working with large clusters where failures are common.\nThis model simplifies¬†large-scale data processing by abstracting away the complexities of distributed programming, such as data distribution, parallelization, and fault tolerance, allowing developers to focus on the logic of their¬†Map¬†and¬†Reduce¬†functions.\nThe MapReduce Execution Flow To understand how MapReduce processes¬†vast amounts of data, let\u0026rsquo;s walk through the typical execution flow, as illustrated in the Google paper and its accompanying diagram¬†(Figure 1 from the paper, shown below). This flow is orchestrated by a central¬†Master¬†(or Coordinator, as in our lab implementation) and executed by multiple¬†Worker¬†processes.\nHere\u0026rsquo;s a breakdown of the key stages:\nInitialization¬†\u0026amp; Input Splitting (Diagram: User Program forks Master, Input files split):\nThe MapReduce library first divides the¬†input files¬†into¬†M¬†smaller, manageable pieces called¬†splits¬†(e.g.,¬†split 0¬†to¬†split¬†4¬†in the diagram). Each split is typically 16-64MB. The User¬†Program then starts multiple copies of the program on a cluster. One copy becomes the¬†Master, and the others become¬†Workers. Here the binary contains logic for master and worker as part of map-reduce library. Task Assignment by Master (Diagram: Master assigns map/reduce to workers):\nThe Master¬†is the central coordinator. It\u0026rsquo;s responsible for assigning tasks to idle workers. There are¬†M¬†map tasks¬†(one for each input split) and¬†R¬†reduce tasks (a number chosen by the user for the desired level of output parallelism). Map Phase - Processing Input Splits (Diagram: worker (3) reads split, (4) local write):\nA worker assigned a¬†map task¬†reads the content of its designated input split (e.g.,¬†split 2). It parses key/value pairs from this input data. For each pair, it executes the user-defined¬†Map¬†function. The¬†Map¬†function emits intermediate key/value¬†pairs. These intermediate pairs are initially buffered in the worker\u0026rsquo;s memory. Periodically, they are written¬†to the worker\u0026rsquo;s¬†local disk. Crucially, these locally written intermediate files are partitioned into¬†R¬†regions/files (one region/file for each eventual reduce task). This is typically done using a partitioning function (e.g.,¬†hash(intermediate_key) % R). The locations of these R partitioned files on the local disk¬†(shown as \u0026ldquo;Intermediate files (on local disks)\u0026rdquo; in the diagram) are then reported back to the Master. The Master now¬†knows where the intermediate data for each reduce task partition resides, spread across possibly many map workers. Reduce Phase¬†- Aggregating Intermediate Data (Diagram: worker (5) remote read, (6) write output):\nOnce the Master sees that map tasks are completing, it begins assigning¬†reduce tasks¬†to other (or the same) workers. When a reduce worker is assigned a partition (say, partition¬†j¬†out of¬†R), the Master provides it with the locations of all the relevant intermediate files (i.e., the¬†j-th region/file from¬†all¬†map workers that produced j-th intermediate file). The reduce worker then performs¬†remote reads¬†from the local disks of the map workers¬†to fetch this buffered intermediate data. After retrieving all necessary intermediate data for its assigned partition, the reduce worker¬†sorts¬†these key/value pairs by the intermediate key. This groups all occurrences of the same key together.¬†(If data is too large for memory, an external sort is used). The worker then iterates through the sorted data. For each unique intermediate key, it calls the user-defined¬†Reduce¬†function, passing the key and the list of all¬†associated intermediate values. The output of the¬†Reduce¬†function is appended to a final¬†output file¬†for¬†that specific reduce partition (e.g.,¬†output file 0,¬†output file 1). There will be¬†R¬†such output files. Job Completion:\nWhen all¬†M¬†map tasks and¬†R¬†reduce tasks have successfully completed, the Master signals the original User Program. The MapReduce call in the¬†user code returns, and the results are available in the¬†R¬†output files. Key Design Decisions:\nAbstraction:¬†Developers focus on¬†Map¬†and¬†Reduce¬†logic, while the framework manages distributed complexities like data partitioning, parallel execution, and shuffling. Inherent Fault Tolerance:¬†The system is designed for resilience against common failures: The Master detects¬†worker failures. If a worker assigned a¬†map task¬†fails, the task is re-assigned because its input split¬†is durable. More subtly, if a worker completes a map task (producing intermediate files on its local disk) but then fails¬†before¬†all necessary reduce tasks have read those intermediate files, those files are lost. The Master must¬†then reschedule that¬†original map task¬†on another worker to regenerate its intermediate output. If a worker¬†assigned a¬†reduce task¬†fails, that reduce task can be re-executed by another worker. However, once a¬†reduce task completes successfully¬†and writes its final output (e.g., to¬†mr-out-X), that output is considered final. The system aims to avoid re-executing successfully completed reduce tasks, relying on the durability of¬†their output. One important aspect to note¬†is that intermediate files are stored on the¬†local file system of the worker nodes that produce them.¬†This design choice is¬†deliberate: by keeping intermediate data local, the system significantly¬†reduces network bandwidth consumption and potential network congestion¬†that would arise¬†if all intermediate data had to be written to, and read from, a global file system. However, this means that crashes¬†in map worker nodes can result in the loss of their locally stored intermediate data, requiring the re-execution of those map tasks.\nIn contrast, the final outputs of worker processes executing the reduce operation are typically written to a¬†global, distributed file system¬†(like GFS in Google\u0026rsquo;s case).¬†Once a reduce task successfully¬†writes its output to this global system, it\u0026rsquo;s considered durable and generally does not need to be re-executed, even if the worker that produced it later fails.\nImplementing MapReduce in Go: The Coordinator and¬†Worker The Go implementation translates the conceptual MapReduce master-worker architecture into two main programs: a¬†Coordinator¬†and¬†multiple¬†Worker¬†processes, communicating via RPC. We\u0026rsquo;ll explore the key parts of their implementation, starting with the¬†Coordinator.\nThe Coordinator (mr/coordinator.go) The Coordinator is the central manager¬†of the MapReduce job. Its primary role is to distribute tasks to workers, track their progress, handle failures, and determine¬†when the overall job is complete.\nInitialization (MakeCoordinator) The¬†MakeCoordinator¬†function¬†initializes the Coordinator\u0026rsquo;s state. It\u0026rsquo;s called by¬†main/mrcoordinator.go¬†with the input files¬†and the number of reduce tasks (nReduce). 1// MakeCoordinator is called by main/mrcoordinator.go to create and initialize 2// the coordinator for a MapReduce job. 3// - files: A slice of input file paths for the map tasks. 4// - nReduce: The desired number of reduce tasks. 5func MakeCoordinator(files []string, nReduce int) *Coordinator { 6\t// Step 1: Initialize the list of ready Map tasks. 7\t// NewTaskList() creates a new instance of TaskList (wrapper around container/list). 8\treadyTaskList := NewTaskList() 9 10\t// For each input file, a Map task is created. 11\tfor index, file := range files { 12\treadyTaskList.AddTask(\u0026amp;Task{ // Task struct holds details for a single map or reduce operation. 13\tFilename: file, // Input file for this map task. 14\tStatus: StatusReady, // Initial status: ready to be assigned. 15\tType: MapType, // Task type is Map. 16\tId: TaskId(fmt.Sprintf(\u0026#34;m-%d\u0026#34;, index)), // Unique ID for the map task (e.g., \u0026#34;m-0\u0026#34;). 17\t}) 18\t} 19 20\t// Step 2: Initialize the Coordinator struct with its core state variables. 21\tc := Coordinator{ 22\t// --- Task Tracking --- 23\t// readyTasks: Holds tasks (initially all Map tasks, later Reduce tasks) that are 24\t// waiting to be assigned to a worker. 25\t// Managed by GetTask (removes) and ReportTask/checkWorkerStatus (adds back on failure). 26\treadyTasks: *readyTaskList, 27 28\t// runningTasks: A map from TaskId to *RunningTask. Tracks tasks currently assigned 29\t// to one or more workers. A RunningTask includes the Task details and a 30\t// list of WorkerIds processing it. 31\t// Managed by GetTask (adds) and ReportTask/checkWorkerStatus (modifies/removes). 32\trunningTasks: make(map[TaskId]*RunningTask), 33 34\t// successTasks: A map from TaskId to *Task. Stores tasks that have been successfully 35\t// completed by a worker. 36\t// Managed by ReportTask (adds on success). 37\tsuccessTasks: make(map[TaskId]*Task), 38 39\t// --- Job Parameters \u0026amp; Phase Control --- 40\t// nReduce: The target number of reduce partitions/tasks for the job. 41\t// Used by Map workers to partition intermediate data and by the Coordinator 42\t// to determine when all reduce tasks are done. 43\tnReduce: nReduce, 44 45\t// nMap: The total number of map tasks, simply the count of input files. 46\t// Used to determine when all map tasks are done. 47\tnMap: len(files), 48 49\t// pendingMappers: A counter for map tasks that are not yet successfully completed. 50\t// Crucially used in GetTask to gate the start of Reduce tasks ‚Äì 51\t// Reduce tasks cannot begin until pendingMappers is 0. 52\t// Decremented in ReportTask upon successful map task completion. 53\tpendingMappers: len(files), 54 55\t// --- Intermediate Data Management --- 56\t// intermediateFiles: An IntermediateFileMap (map[string]map[WorkerId][]string). 57\t// This is vital: maps a partition key (string, for a reduce task) 58\t// to another map. This inner map links a WorkerId (of a map worker) 59\t// to a list of filenames (intermediate files produced by that map worker 60\t// for that specific partition key). 61\t// Populated in ReportTask when a Map task succeeds. 62\t// Read by GetTask to provide Reduce workers with their input locations. 63\tintermediateFiles: make(IntermediateFileMap), 64 65\t// --- Worker Tracking --- 66\t// workers: A map from WorkerId to *WorkerMetdata. Stores metadata about each worker 67\t// that has interacted with the coordinator. WorkerMetdata includes: 68\t// - lastHeartBeat: Time of the worker\u0026#39;s last contact, used by checkWorkerStatus for timeouts. 69\t// - runningTask: TaskId of the task currently assigned to this worker. 70\t// - successfulTasks: A map of tasks this worker has completed (useful for debugging/optimizations, not strictly essential for basic fault tolerance in this lab\u0026#39;s context if tasks are just re-run). 71\t// Populated/updated in GetTask and ReportTask. 72\tworkers: make(map[WorkerId]*WorkerMetdata), 73 74\t// --- Coordinator Shutdown \u0026amp; Job Completion Signaling --- 75\t// finished: A boolean flag set to true when all map and reduce tasks are successfully 76\t// completed (checked in ReportTask). Signals the main job is done. 77\tfinished: false, 78 79\t// done: A channel of empty structs (chan struct{}). Used to signal background goroutines 80\t// (like checkWorkerStatus) to terminate gracefully when the job is `finished`. 81\t// Closed in the Done() method. 82\tdone: make(chan struct{}), 83 84\t// shutdownSignaled: A boolean flag, true after `done` channel is closed. Prevents 85\t// multiple closures or redundant shutdown logic. 86\tshutdownSignaled: false, 87 88\t// allGoroutinesDone: A boolean flag, true after `wg.Wait()` in `Done()` confirms all 89\t// background goroutines have exited. 90\tallGoroutinesDone: false, 91\t// wg (sync.WaitGroup): Used in conjunction with `done` to wait for background goroutines 92\t// to complete their cleanup before the Coordinator fully exits. 93\t// Incremented before launching a goroutine, Done called in goroutine\u0026#39;s defer. 94\t// (wg is part of the Coordinator struct, initialized implicitly here) 95\t} 96 97\tfmt.Printf(\u0026#34;Initialised ready tasklist of %d tasks\\n\u0026#34;, len(files)) 98 99\t// Step 3: Start Services 100\t// Start the RPC server so the coordinator can listen for requests from workers. 101\t// This makes methods like GetTask and ReportTask callable by workers. 102\tc.server() 103 104\t// Step 4: Launch Background Health Checker Goroutine 105\t// This goroutine is responsible for fault tolerance, specifically detecting 106\t// and handling timed-out (presumed crashed) workers. 107\tc.wg.Add(1) // Increment WaitGroup counter before launching the goroutine. 108\tgo func() { 109\tdefer c.wg.Done() // Decrement counter when the goroutine exits. 110\tfor { 111\tselect { 112\tcase \u0026lt;-c.done: // Listen for the shutdown signal from the main coordinator logic. 113\tfmt.Printf(\u0026#34;[Coordinator Shutdown]: Closing worker health check background thread.\\n\u0026#34;) 114\treturn // Exit the goroutine. 115\tdefault: 116\t// Periodically call checkWorkerStatus to handle unresponsive workers. 117\tc.checkWorkerStatus() 118\t// WORKER_TIMEOUT_SECONDS is 10s, so this checks every 5s. 119\ttime.Sleep(WORKER_TIMEOUT_SECONDS / 2) 120\t} 121\t} 122\t}() 123 124\treturn \u0026amp;c // Return the initialized Coordinator instance. 125} Initially,¬†M¬†map tasks are created (one for each input file) and added¬†to¬†readyTasks. Contrary to the paper we can only run reduce tasks only when all mapper tasks are finished as input for a reduce task may require intermediate file output(s) from more than one map task since a map task produces at max R intermediate partition files, each designated to one reduce task and reduce workers needs to fetch these intermediate files from each of the mapper worker\u0026rsquo;s local file system. An RPC server (c.server()) is started for worker¬†communication, and a background goroutine (checkWorkerStatus) is launched for fault tolerance. All shared state within the¬†Coordinator¬†(e.g., task lists, worker metadata) must be protected by mutexes (as seen in its¬†methods like¬†GetTask,¬†ReportTask) since the shared state can be accessed by multiple go routines handling RPC calls from various workers processes which may lead to race conditions. Assigning Tasks to Workers (GetTask¬†RPC Handler) Workers call the¬†GetTask¬†RPC handler to request jobs (either Map or Reduce tasks) from the Coordinator. 1// An RPC handler to find next available task (map or reduce) 2func (c *Coordinator) GetTask(args *GetTaskArgs, reply *GetTaskReply) error { 3\tc.mu.Lock() 4\tdefer c.mu.Unlock() 5 6\tworkerMetadata, ok := c.workers[args.WorkerId] 7 8\t// Requesting worker already processing a task 9\t// Skip task assignment 10\tif ok \u0026amp;\u0026amp; workerMetadata.runningTask != \u0026#34;\u0026#34; { 11\tfmt.Printf(\u0026#34;[GetTask]: Worker %d already processing task %s, rejecting task assignment request.\\n\u0026#34;, args.WorkerId, workerMetadata.runningTask) 12\treturn nil 13\t} 14 15\tif c.readyTasks.GetTaskCount() == 0 { 16\t// No tasks available 17\t// map reduce is complete if we also have len(runningTasks) == 0 18\t// Sending InvalidType task in such cases to worker 19\treply.Task = Task{ 20\tType: InvalidType, 21\t} 22\treturn nil 23\t} 24 25\ttask := c.readyTasks.RemoveTask() 26 27\t// Skipping tasks that are possible retrials with an instance already completed and part of success set 28\t// It is possible that a task here already has a status of `StatusRunning` we are not skipping such tasks in ready queue 29\t// This will result in multiple instances of same task execution, This case is possible if previous worker processing the task 30\t// failed/crashed (timeout of not reporting reached) and we added another instance of the same task. 31\t// Even if two workers report completion of same task only one of them will remove the task from running queue and add it to 32\t// success set, Reporting by slower worker will be skipped. 33 34\t// Only assing a reduce task when we are sure there is no pending map task left 35\t// Since then reduce task will surely fail because of unavailabiltiy of intermeidate fiel data 36\tfor task != nil { 37\tif task.Status == StatusSuccess || (task.Type == ReduceType \u0026amp;\u0026amp; c.pendingMappers \u0026gt; 0) { 38\tif task.Status == StatusSuccess { 39\tfmt.Printf(\u0026#34;[GetTask]: Skipping ready task %s since it is already successfully completed\\n\u0026#34;, task.Id) 40\t} else { 41\tfmt.Printf(\u0026#34;[GetTask]: Skipping reduce task %s since there are %d pending mappers\\n\u0026#34;, task.Id, c.pendingMappers) 42\t} 43\ttask = c.readyTasks.RemoveTask() 44\t} else { 45\tbreak 46\t} 47\t} 48 49\t// Either all tasks are completed (if len(runningTasks) == 0) 50\t// OR all tasks are currently being processed by some workers 51\tif task == nil { 52\treply.Task = Task{ 53\tType: InvalidType, 54\t} 55\tfmt.Printf(\u0026#34;[GetTask]: No task to assign to worker %d, # Tasks Running : %d, # Tasks Completed: %d\\n\u0026#34;, args.WorkerId, len(c.runningTasks), len(c.successTasks)) 56\treturn nil 57\t} 58 59\tfmt.Printf(\u0026#34;[GetTask]: Found a task with id %s for worker %d. Current Task Status: %v\\n\u0026#34;, task.Id, args.WorkerId, task.Status) 60 61\t// Found a task with Status as either `StatusError` or `StatusReady` or `StatusRunning` 62\t// If task\u0026#39;s status is: `StatusError`` -\u0026gt; Retrying failed task again 63\t// If task\u0026#39;s status is `StatusReady` -\u0026gt; First Attempt of processing of task 64\t// If task\u0026#39;s status is `StatusRunning` -\u0026gt; Retrying already running task due to delay from previous assigned worker 65\ttask.Worker = args.WorkerId 66\ttask.StartTime = time.Now() 67\ttask.Status = StatusRunning 68\treply.Task = *task 69 70\tif task.Type == ReduceType { 71\t// Add intermediate file locations collected from various map executions 72\treply.IntermediateFiles = c.intermediateFiles[task.Filename] 73\t} 74 75\treply.NR = c.nReduce 76 77\t// Update list of workers currently processing a taskId 78\trt := c.runningTasks[task.Id] 79 80\tif rt == nil { 81\tc.runningTasks[task.Id] = \u0026amp;RunningTask{} 82\t} 83\tc.runningTasks[task.Id].task = task 84 85\tc.runningTasks[task.Id].workers = append(c.runningTasks[task.Id].workers, args.WorkerId) 86 87\tif workerMetadata != nil { 88\tworkerMetadata.lastHeartBeat = time.Now() 89\tworkerMetadata.runningTask = task.Id 90\t} else { 91\tc.workers[args.WorkerId] = \u0026amp;WorkerMetdata{ 92\tlastHeartBeat: time.Now(), 93\trunningTask: task.Id, 94\tsuccessfulTasks: make(map[TaskId]*TaskOutput), 95\t} 96\t} 97 98\treturn nil 99} As defined in the paper\u0026rsquo;s step-2 of the execution flow this method is called by various workers to request task which are in readyTasks. It deals with scenarios like workers already being busy, no tasks¬†being available, or tasks being unsuitable for immediate assignment (e.g., reduce tasks when mappers are still active). If a valid task is found all necessary details to execute that task are populated in GetTaskReply struct. For Map tasks, it implicitly provides the input¬†file (via¬†task.Filename). For Reduce tasks, it explicitly provides the locations of all relevant intermediate files and the total¬†number of reducers (nR). Handling Task Completion/Failure (ReportTask¬†RPC Handler) Workers call¬†ReportTask¬†to inform the coordinator about the outcome of their assigned task. 1func (c *Coordinator) ReportTask(args *ReportTaskArgs, reply *ReportTaskReply) error { 2\tc.mu.Lock() 3\tdefer c.mu.Unlock() 4 5\treply.Status = true // optimistic reply 6 7\ttaskSuccessInstance := c.successTasks[args.Task.Id] 8\t// ... (validation: check if task already completed, if worker owns task) ... 9 10\t// Reported task is already in success set. 11\t// Possibly retried after timeout by another worker 12\t// One of the worker complted the task. 13\tif taskSuccessInstance != nil { 14\tfmt.Printf(\u0026#34;[ReportTask]: Task %s has already been completed by worker %d\\n\u0026#34;, taskSuccessInstance.Id, taskSuccessInstance.Worker) 15\t// ... (update worker metadata) ... 16\treturn nil 17\t} 18\t19\t// ... (check if worker lost ownership of the task) ... 20 21 22\tif args.Task.Status == StatusError { 23\tfmt.Printf(\u0026#34;[ReportTask]: Task %s reported with status %v by worker %d\\n\u0026#34;, args.Task.Id, args.Task.Status, args.Task.Worker) 24\t// ... (disown worker from task) ... 25\t// If no other worker is processing this task, add it back to readyTasks 26\tif len(c.runningTasks[args.Task.Id].workers) == 0 { 27\ttask := args.Task 28\ttask.Worker = 0 29\ttask.StartTime = time.Time{} 30\ttask.Status = StatusReady 31\tc.readyTasks.AddTask(\u0026amp;task) 32\t} 33\treturn nil 34\t} 35 36\tif args.Task.Status == StatusSuccess { 37\tswitch args.Task.Type { 38\tcase MapType: 39\tintermediateFiles := args.Task.Output 40\tfmt.Printf(\u0026#34;[ReportTask]: Mapper Task %s completed successfully by worker %d, produced following intermediate files: %v\\n\u0026#34;, args.Task.Id, args.Task.Worker, intermediateFiles) 41 42\tfor _, filename := range intermediateFiles { 43\tpartitionKey := strings.Split(filename, \u0026#34;-\u0026#34;)[4] // Assumes filename format like w-\u0026lt;workerId\u0026gt;/mr-m-\u0026lt;taskId\u0026gt;-\u0026lt;hash\u0026gt; 44\tparitionFiles, ok := c.intermediateFiles[partitionKey] 45\tif !ok || paritionFiles == nil { 46\tparitionFiles = make(map[WorkerId][]string) 47\t} 48\tparitionFiles[args.Task.Worker] = append(paritionFiles[args.Task.Worker], filename) 49\tc.intermediateFiles[partitionKey] = paritionFiles 50\t} 51\t// ... (update worker metadata, move task to successTasks, decrement pendingMappers) ... 52\tdelete(c.runningTasks, args.Task.Id) 53\tc.successTasks[args.Task.Id] = \u0026amp;args.Task 54\tc.pendingMappers-- 55 56\tif c.pendingMappers == 0 { 57\tfmt.Printf(\u0026#34;\\nAll map task ran successfully. Tasks Run Details: \\n %v \\n\u0026#34;, c.successTasks) 58\tc.addReduceTasks() // Trigger creation of reduce tasks 59\t} 60\tcase ReduceType: 61\t// ... (update worker metadata, move task to successTasks) ... 62\tdelete(c.runningTasks, args.Task.Id) 63\tc.successTasks[args.Task.Id] = \u0026amp;args.Task 64 65\tif len(c.successTasks) == c.nMap+c.nReduce { 66\tfmt.Printf(\u0026#34;\\nAll reduce tasks ran successfully. Tasks Run Details: \\n %v \\n\u0026#34;, c.successTasks) 67\tc.finished = true // Mark entire job as done 68\t} 69\tdefault: 70\t// ... 71\t} 72\t// ... (logging) ... 73\t} 74\treturn nil 75} 76 77// ... (helper function addReduceTasks) 78func (c *Coordinator) addReduceTasks() { 79\tindex := 0 80\tfor partition, v := range c.intermediateFiles { // Iterate over collected partitions 81\ttask := Task{ 82\tStatus: StatusReady, 83\tType: ReduceType, 84\tId: TaskId(fmt.Sprintf(\u0026#34;r-%d\u0026#34;, index)), 85\tFilename: partition, // Partition key becomes the \u0026#39;filename\u0026#39; for the reduce task 86\t} 87\tif c.successTasks[task.Id] == nil { // Avoid re-adding if already processed (e.g. due to retries) 88\tc.readyTasks.AddTask(\u0026amp;task) 89\tfmt.Printf(\u0026#34;Reduce Task with Id %s Added to ready queue (Intermediate partition %s with %d files)\\n\u0026#34;, task.Id, partition, len(v)) 90\t} 91\tindex++ 92\t} 93\tc.nReduce = index // Update nReduce to actual number of partitions, good for robustness 94} If a task is reported with¬†StatusError, and¬†it\u0026rsquo;s the only instance of that task running, it\u0026rsquo;s re-queued for a later attempt. This is¬†a core part of fault tolerance. Processes Successful Map Tasks: Collects and organizes¬†the locations of¬†intermediate files¬†(output of Map functions) based on their partition key. This information (c.intermediateFiles) is vital for the subsequent Reduce phase, as it tells Reduce workers where to fetch their input data. This aligns¬†with step 4 of the paper\u0026rsquo;s flow. Decrements¬†pendingMappers. When all mappers are done, it triggers¬†addReduceTasks. Once all Map tasks are complete,¬†addReduceTasks¬†is called. It iterates through all the unique partition keys¬†derived from the intermediate files and creates one Reduce task for each, adding them to the¬†readyTasks¬†queue. Processes Successful Reduce Tasks: Marks the reduce task as complete. Checks if all Map¬†and Reduce tasks for the entire job are finished. If so, it sets¬†c.finished = true, signaling that¬†the Coordinator can begin shutting down. By checking¬†c.successTasks¬†at the beginning, it¬†avoids reprocessing reports for tasks already marked as successful, which helps in scenarios with duplicate or late messages. Fault Tolerance (checkWorkerStatus) A background goroutine periodically checks for unresponsive workers. 1func (c *Coordinator) checkWorkerStatus() { 2\tc.mu.Lock() 3\tdefer c.mu.Unlock() 4 5\tfor workerId, metadata := range c.workers { 6\tlastHeartBeatDuration := time.Since(metadata.lastHeartBeat) 7 8\tif metadata.runningTask != \u0026#34;\u0026#34; \u0026amp;\u0026amp; lastHeartBeatDuration \u0026gt;= WORKER_TIMEOUT_SECONDS { 9\tfmt.Printf(\u0026#34;Worker %d have not reported in last %s\\n\u0026#34;, workerId, lastHeartBeatDuration) 10\ttaskToRetry := make([]*Task, 0) 11\t12\trunningTask := c.runningTasks[metadata.runningTask] 13\tif runningTask == nil { 14\t// This case should ideally not happen if state is consistent 15\tfmt.Printf(\u0026#34;[checkWorkerStatus]: Local worker state shows worker %d running rask %s whereas global running tasks state does not show any worker for the same task.\\n\u0026#34;, workerId, metadata.runningTask) 16\t// Potentially clear worker\u0026#39;s running task if inconsistent: metadata.runningTask = \u0026#34;\u0026#34; 17\tcontinue // or return, depending on desired error handling 18\t} 19 20\ttaskToRetry = append(taskToRetry, runningTask.task) 21\tmetadata.runningTask = \u0026#34;\u0026#34; // Worker is no longer considered running this task 22 23\t// Remove this worker from the list of workers for the task 24\trunningTask.workers = slices.DeleteFunc(runningTask.workers, func(w WorkerId) bool { 25\treturn w == workerId 26\t}) 27 28\t// If this was the only worker on this task, or if we want to aggressively reschedule 29\t// (The current code implies rescheduling if *any* assigned worker times out, which is fine for this lab) 30\tif len(taskToRetry) \u0026gt; 0 { // Will always be true if runningTask was not nil 31\tfor _, task := range taskToRetry { 32\tfmt.Printf(\u0026#34;[checkWorkerStatus]: Adding task %s of type %d with status %d back to the ready queue.\\n\u0026#34;, task.Id, task.Type, task.Status) 33\t34\t// Reset task for retry 35\ttask.Status = StatusReady 36\ttask.Worker = 0 // Clear previous worker assignment 37\ttask.Output = make([]string, 0) // Clear previous output 38 39\tc.readyTasks.AddTask(task) 40\t} 41\t} 42\t} 43\t// ... (logging for active workers) ... 44\t} 45}\tKey Decisions Upon¬†Detecting a Failed Worker:\nIdentify the Affected Task:¬†The primary task to consider is¬†metadata.runningTask, which the failed worker was supposed to be executing. The details of this task are retrieved from¬†c.runningTasks. Update Worker\u0026rsquo;s State:¬†The failed worker\u0026rsquo;s¬†metadata.runningTask¬†is cleared, indicating it\u0026rsquo;s no longer considered to be working on that task by the coordinator. Update Task\u0026rsquo;s Worker List:¬†The failed¬†workerId¬†is removed from the¬†runningTaskEntry.workers¬†list, which tracks¬†all workers assigned to that specific task ID. Reset Task for Re-execution:¬†The affected¬†taskInstanceToRetry¬†undergoes several state changes: Status¬†is set back to¬†StatusReady, making it available in the¬†c.readyTasks¬†queue. Worker¬†(assigned worker ID) is cleared. StartTime¬†is reset. Output¬†(list of output¬†files) is cleared, as any partial output is now suspect or irrelevant. Re-queue the Task:¬†The reset task is added back to¬†c.readyTasks.AddTask(task). This ensures another worker can pick it¬†up. Handling Lost Intermediate Data (Implicitly via Task Re-execution): A critical¬†aspect of fault tolerance in MapReduce, as highlighted by the paper, is managing the intermediate files produced by map tasks. These¬†are typically stored on the local disks of the map workers. If a map worker completes its task successfully but then crashes¬†before¬†all¬†relevant reduce tasks have consumed its intermediate output, those intermediate files are lost. Our current¬†checkWorkerStatus¬†implementation¬†primarily focuses on retrying the¬†actively running task¬†of a worker that times out.\n1// In checkWorkerStatus, when a worker (workerId) times out: 2// ... 3runningTask := c.runningTasks[metadata.runningTask] 4// ... 5taskToRetry = append(taskToRetry, runningTask.task) // The currently running task is added for retry 6metadata.runningTask = \u0026#34;\u0026#34; 7// ... task is reset and added back to c.readyTasks ... This handles cases where a worker fails mid-task. But what about its¬†previously¬†completed¬†map tasks whose outputs are now gone?\nThe Challenge of Retrying Previously Successful Map Tasks One might initially think that upon a worker\u0026rsquo;s crash, we should re-queue¬†all¬†map tasks that worker¬†had successfully completed. The following (commented-out) snippet from an earlier version of¬†checkWorkerStatus¬†attempted¬†this:\n1// Original (commented-out) consideration for retrying all successful map tasks of a crashed worker: 2 3// Adding successful map tasks of this worker for retrial 4for taskId, _ := range metadata.successfulTasks { 5\tif c.successTasks[taskId] != nil \u0026amp;\u0026amp; c.successTasks[taskId].Type == MapType { 6\t// If this task was indeed in the global success set and was a MapType: 7\ttaskToRetry = append(taskToRetry, c.successTasks[taskId]) // Add it for retrial 8\tdelete(c.successTasks, taskId) // Remove from global success set 9\t// CRITICAL: We would also need to increment c.pendingMappers here if it had been decremented 10\tc.pendingMappers++ 11\t} 12} 13 14// Tombstoning metadata of intermediate files produced by this worker 15// From global state so that downstream reduce workers get to know about the failure. 16// This would ideally cause reduce tasks that depend on this worker\u0026#39;s output to fail 17// or wait, and get re-added after the map tasks are re-run. 18for _, v := range c.intermediateFiles { 19\t// Mark intermediate files from this worker (workerId) as unavailable/invalid. 20\tdelete(v, workerId) // Or v[workerId] = nil if the structure supports it 21} When a map worker crashes after successfully writing its intermediate files, those files¬†(on its local disk) are lost in a true distributed system. Our lab setup, where all workers share the host\u0026rsquo;s filesystem, can sometimes mask this; a \u0026lsquo;crashed\u0026rsquo; worker\u0026rsquo;s files might still be accessible. This is a crucial difference¬†from a production environment. Simply re-queuing¬†all¬†successfully completed map tasks from a crashed worker can be inefficient:\nPerformance Hit:¬†It can lead to significant re-computation and potential test timeouts, especially if many¬†map tasks were already done by a worker which crashed. Complexity:¬†Managing¬†pendingMappers¬†and preventing reduce tasks from starting¬†prematurely adds complexity if many map tasks are suddenly re-added. A More Targeted Optimization (Future Scope): A more refined¬†approach is to¬†only re-run a successful map task from a crashed worker if its specific output intermediate partitions are actually needed by currently¬†pending (not yet completed) reduce tasks.\nThis involves:\nIdentifying which map tasks the crashed worker completed.\nDetermining if their output partitions are required by any¬†active or future¬†reduce tasks.\nOnly then, re-queueing those¬†specific¬†map tasks and invalidating their previous intermediate file locations.\nNot to prevent all reduce task from processing and maintain list of reduce task which should be skipped if scheduled based on lost intermediate files state from a crashed worker.\nThis smarter¬†retry avoids redundant work but increases coordinator complexity. For our lab, focusing on retrying the¬†currently running task¬†of a failed¬†worker proved sufficient to pass the tests, partly due to the shared filesystem behaviour making the storage of intermediate files also in some sense to global filesystem\nIn essence,¬†checkWorkerStatus¬†implements the \u0026ldquo;timeout¬†and retry\u0026rdquo; strategy. It ensures that work assigned to unresponsive workers is not indefinitely stalled and is eventually re-assigned, which is fundamental for making progress in a distributed system prone to failures.\nJob Completion (Done) main/mrcoordinator.go¬†periodically calls¬†Done()¬†to check¬†if the entire job is finished. 1// main/mrcoordinator.go calls Done() periodically to find out 2// if the entire job has finished. 3func (c *Coordinator) Done() bool { 4\tc.mu.Lock() 5 6\t// If the job is marked as finished and we haven\u0026#39;t started the shutdown sequence for goroutines yet 7\tif c.finished \u0026amp;\u0026amp; !c.shutdownSignaled { 8\tfmt.Printf(\u0026#34;[Coordinator Shutdown]: MR workflow completed. Signaling internal goroutines to stop.\\n\u0026#34;) 9\tclose(c.done) // Signal all listening goroutines 10\tc.shutdownSignaled = true // Mark that we\u0026#39;ve signaled them 11\t} 12 13\t// If we have signaled for shutdown, but haven\u0026#39;t yet confirmed all goroutines are done 14\tif c.shutdownSignaled \u0026amp;\u0026amp; !c.allGoroutinesDone { 15\tc.mu.Unlock() 16\tc.wg.Wait() // Wait for all goroutines (like the health checker) to call c.wg.Done() 17\tc.mu.Lock() 18\tc.allGoroutinesDone = true 19\tfmt.Printf(\u0026#34;[Coordinator Shutdown]: All internal goroutines have completed.\\n\u0026#34;) 20\t} 21 22\tisCompletelyDone := c.finished \u0026amp;\u0026amp; c.allGoroutinesDone 23\tc.mu.Unlock() 24\treturn isCompletelyDone 25} The Worker (mr/worker.go) The Worker process is responsible for executing the¬†actual¬†Map¬†and¬†Reduce¬†functions as directed by the Coordinator. Each worker operates independently, requesting tasks, performing them, and reporting back the results.\nWorker\u0026rsquo;s Main Loop (Worker¬†function) The¬†Worker¬†function, called by¬†main/mrworker.go, contains the main operational loop. 1var workerId WorkerId = WorkerId(os.Getpid()) // Unique ID for this worker process 2var dirName string = fmt.Sprintf(\u0026#34;w-%d\u0026#34;, workerId) // Worker-specific directory for temp files 3 4// ... (Log, ihash functions) ... 5 6// main/mrworker.go calls this function. 7func Worker(mapf func(string, string) []KeyValue, 8\treducef func(string, []string) string) { 9\tLog(\u0026#34;Started\u0026#34;) 10 11\t// Create a worker-specific directory if it doesn\u0026#39;t exist. 12\t// Used for storing temporary files before atomic rename. 13\tif _, err := os.Stat(dirName); os.IsNotExist(err) { 14\terr := os.Mkdir(dirName, 0755) 15\t// ... (error handling) ... 16\t} 17 18\tgetTaskargs := GetTaskArgs{ // Prepare args for GetTask RPC 19\tWorkerId: workerId, 20\t} 21 22\tfor { // Main loop: continuously ask for tasks 23\tgetTaskReply := GetTaskReply{} 24\tLog(\u0026#34;Fetching task from coordinator...\u0026#34;) 25\tok := call(\u0026#34;Coordinator.GetTask\u0026#34;, \u0026amp;getTaskargs, \u0026amp;getTaskReply) 26 27\tif ok { // Successfully contacted Coordinator 28\ttask := \u0026amp;getTaskReply.Task 29\tnReduce := getTaskReply.NR // Number of reduce partitions 30 31\tswitch task.Type { 32\tcase MapType: 33\tLog(fmt.Sprintf(\u0026#34;Assigned map job with task id: %s\u0026#34;, task.Id)) 34\tprocessMapTask(task, nReduce, mapf) // Execute the map task 35\tcase ReduceType: 36\tLog(fmt.Sprintf(\u0026#34;Assigned reduce job with task id: %s\u0026#34;, task.Id)) 37\tintermediateFiles := getTaskReply.IntermediateFiles // Get locations from Coordinator 38\tprocessReduceTask(task, intermediateFiles, reducef) // Execute reduce task 39\tdefault: // InvalidType or unknown 40\tLog(\u0026#34;Invalid task recieved or no tasks available. Sleeping.\u0026#34;) 41\t} 42 43\t// If a valid task was processed (not InvalidType), report its status 44\tif task.Type != InvalidType { 45\treportTaskArgs := ReportTaskArgs{ Task: *task } 46\treportTaskReply := ReportTaskReply{} 47\tok = call(\u0026#34;Coordinator.ReportTask\u0026#34;, \u0026amp;reportTaskArgs, \u0026amp;reportTaskReply) 48\tif !ok || !reportTaskReply.Status { 49\tLog(\u0026#34;Failed to report task or coordinator indicated an issue. Exiting.\u0026#34;) 50\t// The lab hints that if a worker can\u0026#39;t contact the coordinator, 51\t// it can assume the job is done and the coordinator has exited. 52\tremoveLocalWorkerDirectory() // Clean up worker-specific directory 53\treturn 54\t} 55\t} 56\t// Brief pause before asking for the next task. 57\ttime.Sleep(WORKER_SLEEP_DURATION) // WORKER_SLEEP_DURATION is 2s 58\t} else { // Failed to contact Coordinator 59\tLog(\u0026#34;Failed to call \u0026#39;Coordinator.GetTask\u0026#39;! Coordinator not found or exited. Exiting worker.\u0026#34;) 60\t// removeLocalWorkerDirectory() // Cleanup if needed, though not strictly required by lab on exit 61\treturn // Exit the worker process 62\t} 63\t} 64} Core Logic:¬†Continuously polls the Coordinator for tasks (Coordinator.GetTask). Based on the task type (MapType¬†or¬†ReduceType), it calls the respective processing function. After processing, it reports the outcome to the Coordinator (Coordinator.ReportTask). Exit Condition:¬†If communication with the Coordinator fails (e.g.,¬†GetTask¬†RPC fails), the worker assumes the job¬†is complete and the Coordinator has shut down, so the worker also exits. This is a simple shutdown mechanism compliant with the lab requirements. Local Directory:¬†Each worker maintains a local directory (dirName¬†like¬†w-workerId) for its¬†temporary files, ensuring isolation before final output naming. Processing Map Tasks (processMapTask) 1// Processes map task by fetching `Filename` from Task 2// Calls provided mapf function and stores intermediate files after 3// paritioninng them based on `ihash` function 4func processMapTask(task *Task, nReduce int, mapf func(string, string) []KeyValue) error { 5\tLog(fmt.Sprintf(\u0026#34;Processing map task with id %s and file: %s\u0026#34;, task.Id, task.Filename)) 6 7\tfile, err := os.Open(task.Filename) // Open the input split (file) 8\t// ... (error handling: set task.Status = StatusError, return) ... 9\tcontent, err := io.ReadAll(file) // Read the entire file content 10\t// ... (error handling: set task.Status = StatusError, return) ... 11\tfile.Close() 12 13\tintermediate := mapf(task.Filename, string(content)) // Execute the user-defined map function 14 15\t// Group intermediate key-value pairs by partition 16\tbuckets := make(map[int][]KeyValue) 17\tfor _, kv := range intermediate { 18\tpartition := ihash(kv.Key) % nReduce // Determine partition using ihash 19\tbuckets[partition] = append(buckets[partition], kv) 20\t} 21 22\ttask.Output = []string{} // Clear previous output, prepare for new output filenames 23 24\t// For each partition, sort and write to a temporary intermediate file 25\tfor partition, kva := range buckets { 26\t// In-memory sort for this partition\u0026#39;s KeyValue pairs. 27\t// The paper mentions external sort if data is too large, but here it\u0026#39;s in-memory. 28\tsort.Sort(ByKey(kva)) 29 30\t// Create a temporary file in the worker\u0026#39;s specific directory. 31\ttempFile, err := os.CreateTemp(dirName, \u0026#34;mwt-*\u0026#34;) // \u0026#34;mwt\u0026#34; for map worker temp 32\t// ... (error handling: set task.Status = StatusError, return) ... 33\t34\tenc := json.NewEncoder(tempFile) 35\tfor _, kv := range kva { // Write sorted KeyValue pairs to the temp file using JSON encoding 36\terr := enc.Encode(\u0026amp;kv) 37\t// ... (error handling: set task.Status = StatusError, tempFile.Close(), return) ... 38\t} 39\ttempFile.Close() // Close after writing 40 41\t// Atomically rename the temporary file to its final intermediate name. 42\t// Filename format: mr-\u0026lt;map_task_id\u0026gt;-\u0026lt;partition_number\u0026gt; (e.g., mr-m-0-1) 43\t// Stored within the worker\u0026#39;s directory: w-\u0026lt;workerId\u0026gt;/mr-m-0-1 44\tintermediateFilename := filepath.Join(dirName, fmt.Sprintf(\u0026#34;mr-%s-%d\u0026#34;, task.Id, partition)) 45\terr = os.Rename(tempFile.Name(), intermediateFilename) 46\t// ... (error handling: set task.Status = StatusError, return) ... 47\t48\ttask.Output = append(task.Output, intermediateFilename) // Add final filename to task\u0026#39;s output list 49\t} 50 51\ttask.Status = StatusSuccess // Mark task as successful 52\treturn nil 53} Core Logic:¬†Reads the assigned input file, applies the user-defined¬†mapf, partitions the output¬†KeyValue¬†pairs using¬†ihash() % nReduce, sorts each partition\u0026rsquo;s data in memory, and writes it to a uniquely named intermediate file within its local directory. Intermediate Files:¬†Output filenames¬†(e.g.,¬†w-workerId/mr-mapTaskID-partitionID) are collected in¬†task.Output. Atomic Rename:¬†Uses¬†os.Rename¬†to make intermediate files visible only once fully written, preventing¬†partial reads by reducers. This is crucial for consistency, especially if crashes occur. In-Memory Sort:¬†A simplification for the lab; a production system might use external sorting if intermediate data for a partition is too large for memory. Processing Reduce Tasks (processReduceTask) 1func processReduceTask(task *Task, intermediateFiles map[WorkerId][]string, reducef func(string, []string) string) error { 2\tLog(fmt.Sprintf(\u0026#34;Processing reduce task with id %s for partition key %s\u0026#34;, task.Id, task.Filename)) 3 4\t// Create a temporary output file in the worker\u0026#39;s directory 5\ttempReduceFile, err := os.CreateTemp(dirName, \u0026#34;mwt-*\u0026#34;) // \u0026#34;mwt\u0026#34; for map worker temp (could be \u0026#34;rwt\u0026#34;) 6\t// ... (error handling: set task.Status = StatusError, return) ... 7\tdefer tempReduceFile.Close() // Ensure temp file is closed 8 9\tvar kva []KeyValue // To store all KeyValue pairs for this reduce partition 10 11\t// Gather all intermediate data for this reduce task\u0026#39;s partition from various map workers. 12\t// `intermediateFiles` (map[WorkerId][]string) comes from the Coordinator, 13\t// mapping map worker IDs to the list of intermediate files they produced for *this specific partition*. 14\tfor mapWorkerId, filesFromMapWorker := range intermediateFiles { 15\tfor _, filename := range filesFromMapWorker { 16\tLog(fmt.Sprintf(\u0026#34;Processing intermediate file %s from map worker %d\u0026#34;, filename, mapWorkerId)) 17\tintermediateFile, err := os.Open(filename) 18\t// ... (error handling: set task.Status = StatusError, return) ... 19\t20\tdec := json.NewDecoder(intermediateFile) 21\tfor { // Read all KeyValue pairs from this intermediate file 22\tvar kv KeyValue 23\tif err := dec.Decode(\u0026amp;kv); err != nil { 24\tif err != io.EOF { // Handle actual decoding errors 25\tLog(fmt.Sprintf(\u0026#34;Error decoding KV from intermediate file %s: %v\u0026#34;, filename, err)) 26\ttask.Status = StatusError 27\tintermediateFile.Close() 28\treturn err 29\t} 30\tbreak // EOF reached 31\t} 32\tkva = append(kva, kv) 33\t} 34\tintermediateFile.Close() 35\t} 36\t} 37 38\t// Sort all collected KeyValue pairs by key. This groups identical keys together. 39\t// This is Step 5 of the paper: \u0026#34;When a reduce worker has read all intermediate data, 40\t// it sorts it by the intermediate keys...\u0026#34; 41\t// Again, this is an in-memory sort of all data for this partition. 42\tsort.Sort(ByKey(kva)) 43 44\t// Iterate over sorted data, apply reducef for each unique key 45\ti := 0 46\tfor i \u0026lt; len(kva) { 47\tj := i + 1 48\t// Find all values for the current key kva[i].Key 49\tfor j \u0026lt; len(kva) \u0026amp;\u0026amp; kva[j].Key == kva[i].Key { 50\tj++ 51\t} 52\tvalues := []string{} 53\tfor k := i; k \u0026lt; j; k++ { 54\tvalues = append(values, kva[k].Value) 55\t} 56\t57\toutput := reducef(kva[i].Key, values) // Execute user-defined reduce function 58 59\t// Write output in the format \u0026#34;key value\\n\u0026#34; to the temporary reduce file. 60\t// This matches main/mrsequential.go and the lab\u0026#39;s expected output format. 61\tfmt.Fprintf(tempReduceFile, \u0026#34;%v %v\\n\u0026#34;, kva[i].Key, output) 62\ti = j // Move to the next unique key 63\t} 64 65\t// Atomically rename the temporary output file to its final name (e.g., mr-out-0). 66\t// The final output file is placed in the current directory (mr-tmp/ during tests), 67\t// not the worker-specific one, as it\u0026#39;s global output. 68\tfinalOutputFileName := fmt.Sprintf(\u0026#34;mr-out-%s\u0026#34;, task.Filename) // task.Filename is the partition key (e.g., \u0026#34;0\u0026#34;, \u0026#34;1\u0026#34;) 69\terr = os.Rename(tempReduceFile.Name(), finalOutputFileName) 70\t// ... (error handling: set task.Status = StatusError, return) ... 71 72\ttask.Output = []string{finalOutputFileName} // Record final output filename 73\ttask.Status = StatusSuccess 74\treturn nil 75} Core Logic:¬†Gathers all intermediate¬†KeyValue¬†pairs for¬†its assigned partition (identified by¬†task.Filename) from the locations provided by the Coordinator (intermediateFiles). It then sorts¬†all these¬†KeyValue¬†pairs together, groups them by unique key, applies the user-defined¬†reducef¬†for each key and¬†its list of values, and writes the final output. Data Aggregation:¬†Reads from multiple intermediate files¬†(potentially from different map workers) that correspond to its specific partition. Global Sort (for the partition):¬†All¬†KeyValue pairs for the partition are sorted together in memory before reduction. This is essential for grouping values for the same key. Final Output:¬†Writes output to a temporary file and then atomically renames it to the final output file¬†name (e.g.,¬†mr-out-X), which is placed in the main job directory (not the worker\u0026rsquo;s specific temp directory). In-Memory Sort:¬†Similar to map tasks, all data for a reduce¬†partition is sorted in memory. Conclusion Working on this MapReduce project taught me a lot about Go‚Äôs concurrency features, how to use RPC for process communication, and how the MapReduce framework organizes big data jobs. Most importantly, I learned to think about what can go wrong in distributed systems and how to handle failures gracefully. It‚Äôs been a great hands-on way to understand the real challenges behind large-scale data processing.\nReferences https://pdos.csail.mit.edu/6.824/labs/lab-mr.html https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf https://github.com/harshrai654/6.5840/tree/lab0/src ","permalink":"https://harshrai654.github.io/blogs/map-reduce/","summary":"\u003cp\u003eThis article shares learnings from Google\u0026rsquo;s influential MapReduce paper and explores the challenges encountered while implementing a simplified version. Our system uses multiple worker processes, running on a single¬†machine and communicating via RPC, to mimic key aspects of a distributed environment.\u003c/p\u003e\n\u003ch1 id=\"what-is-map-reduce\"\u003eWhat is Map-Reduce\u003c/h1\u003e\n\u003cp\u003eAt its core, MapReduce is a programming model and an associated¬†framework for processing and generating massive datasets using a parallel, distributed algorithm, typically on a cluster of computers. You might already be familiar¬†with¬†\u003ccode\u003emap\u003c/code\u003e¬†and¬†\u003ccode\u003ereduce\u003c/code\u003e¬†operations from functional programming languages. For instance, in JavaScript,¬†\u003ccode\u003earray.map()\u003c/code\u003e¬†transforms¬†each element of an array independently based on a¬†mapper¬†function, while¬†\u003ccode\u003earray.reduce()\u003c/code\u003e¬†iterates through an array, applying a¬†reducer¬†function to accumulate its elements into a single output value (e.g., a sum, or¬†a new, aggregated object).\u003c/p\u003e","title":"Map Reduce"},{"content":"This article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.\nBackground After introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as ioredis for interacting with our Redis-compatible database. Once we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand why this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions ‚Äì options ranged from open-source SDKs (OpenTelemetry Node.js SDK) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted Grafana Tempo instance to test the setup and feasibility. (So far, the setup is working great, and I\u0026rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.\nAn example trace showing Redis commands executed while processing an HTTP request.\nOkay, back to the problem. After setting up tracing, we could visually inspect the Redis command spans, like in the example above. Correlating these trace timings with our earlier metrics confirmed the high latency numbers. Indeed, something wasn\u0026rsquo;t right with how our service was connecting to the cache server and executing commands.\nFinding the culprit Dragonfly is a Redis-compatible key-value database but with support for multithreading; Redis, on the other hand, follows a single-threaded, event-based model similar to Node.js.\nOur first step was to check if anything was wrong with the cache server deployment itself. We enabled Dragonfly\u0026rsquo;s slow query logs to check for commands taking longer than 100ms. Interestingly, we only saw SCAN commands in these logs. This didn\u0026rsquo;t immediately make sense because our high latency metrics were observed for commands like GET, SET, DELETE, and UNLINK. These are typically O(1) commands and should not take more than a few milliseconds, so we ruled out the possibility of these specific commands taking significant time to process on the cache server itself.\nTo further monitor command execution time directly on the Dragonfly server, we enabled its Prometheus metrics exporter. We looked at two metrics: \u0026ldquo;Pipeline Latency\u0026rdquo; and \u0026ldquo;Average Pipeline Length\u0026rdquo;. The \u0026ldquo;Average Pipeline Length\u0026rdquo; was always close to 0, and the \u0026ldquo;Pipeline Latency\u0026rdquo; was consistently under 10ms. While there wasn\u0026rsquo;t clear documentation from Dragonfly detailing these metrics precisely, going by the names, we assumed they represented the actual command execution time on the cache server.\nSo, the evidence suggested commands were executing quickly on the cache server (confirmed by both low Prometheus pipeline latency and the absence of GET/SET etc., in the slow query logs). But wait ‚Äì the slow query logs did show the SCAN command with execution times in the range of 50ms to 200ms. So, what exactly is the SCAN command, and why were we using it?\nThe SCAN Command and Frequent Use First, what is the SCAN command? SCAN is a cursor-based command in Redis used to iterate over the keyspace. It takes a cursor position and a glob pattern, matching the pattern against keys in the database without blocking the server for long periods (unlike its predecessor, KEYS).\nIn our system, we primarily use SCAN to invalidate cache entries for specific users. We publish cache invalidation events from various parts of our application. Depending on the event type, a process triggers that uses SCAN to find and delete cache keys matching a user-specific pattern. Since these invalidation events are very frequent in our workload, the SCAN command was executed much more often than we initially realized.\nThe Restart Clue and Initial Hypotheses During our investigation, we stumbled upon a curious behavior: if we restarted the DragonflyDB instance, the high command latency would drop back to normal levels for a few hours before inevitably climbing back up to the problematic 200ms range. This provided a temporary, albeit disruptive, fix during peak hours (the cost being the loss of cached data, although we later mitigated this by enabling snapshotting for restores).\nThis temporary \u0026ldquo;fix\u0026rdquo; from restarting was a significant clue. It strongly suggested the problem wasn\u0026rsquo;t necessarily the SCAN command\u0026rsquo;s execution on the server (which slow logs and metrics already indicated was fast most of the time, except for SCAN itself sometimes), but perhaps something related to the state of the connection or interaction between our Node.js services and DragonflyDB over time.\nThis led us to two main hypotheses related to connection handling:\nConnection Pooling: ioredis, the driver we were using, maintains a single connection to the Redis server. This is standard for single-threaded Redis, where multiple connections offer little benefit. However, DragonflyDB is multi-threaded. Could our single connection be a bottleneck when dealing with frequent commands, especially potentially long-running SCAN operations, under Dragonfly\u0026rsquo;s multi-threaded architecture? Perhaps connection pooling would allow better parallel execution. Long-Running TCP Connections: Could the TCP connections themselves, after being open for extended periods, degrade in performance or enter a state that caused delays in sending commands or receiving responses? Investigating Connection Pooling To test the connection pooling hypothesis, we considered adding a pooling library like generic-pool on top of ioredis. However, we noticed that node-redis, the official Redis client for Node.js, already included built-in connection pooling capabilities and had an API largely compatible with ioredis. So, as a direct way to test the effect of pooling, we replaced ioredis with node-redis in our service.\nUnfortunately, even with node-redis and its connection pooling configured, the behavior remained the same: high latencies persisted, only dropping temporarily after a DragonflyDB restart. This seemed to rule out simple connection pooling as the solution.\nInvestigating TCP Connection State With the pooling hypothesis proving unfruitful, we turned to the idea of issues with long-running TCP connections. We tried several approaches to detect problems here:\nCode Profiling: We profiled the Node.js service during periods of high latency, generating flame graphs to see if significant time was being spent within the Redis driver\u0026rsquo;s internal methods, specifically looking for delays in writing to or reading from the underlying network socket. Packet Tracing: We used tcpdump on the service instances to capture network traffic between the Node.js service and DragonflyDB, looking for signs of network-level latency, packet loss, or retransmissions that could explain the delays. Both of these efforts came up empty. The profiling data showed no unusual delays within the driver\u0026rsquo;s socket operations, and the tcpdump analysis indicated normal network communication without significant latency.\nWe had confirmed the high frequency of SCAN, observed the strange restart behavior, and ruled out both simple connection pooling and obvious TCP-level network issues as the root cause. We needed a new hypothesis.\nA Perfect Correlation and the Root Cause We refocused on the most reliable clue: why did restarting the cache server temporarily fix the latency? We had ruled out connection management issues. The other major effect of a restart was clearing the in-memory key-value store (remember, at this stage, we weren\u0026rsquo;t restoring snapshots immediately after restarts). Plotting the number of keys in DragonflyDB over time confirmed our suspicion. We saw the key count drop to zero after each restart and then steadily climb until the next restart. Correlating this key count graph with our latency metrics revealed a clear pattern: as the number of keys rose, so did the p99 latency for our Redis commands. Although Redis/DragonflyDB can handle millions of keys, we started seeing significant latency increases once the key count grew into the 100,000‚Äì200,000 range in our specific setup. Now, which of our commands would be most affected by the number of keys? Looking at our common operations (GET, SET, DEL, UNLINK, SCAN, EXISTS), SCAN stood out. While most Redis commands have O(1) complexity, SCAN\u0026rsquo;s performance is inherently tied to the number of keys it needs to iterate over. (More details on SCAN\u0026rsquo;s behavior can be found in the official Redis documentation). We were using SCAN extensively for cache invalidation, employing code similar to this:\n1const keysToDelete = []; 2 3for await (const key of this.client.scanIterator({ 4\tMATCH: pattern, 5\tCOUNT: count, 6})) { 7\tkeysToDelete.push(key); 8} Critically, for each cache invalidation event (which were frequent), we potentially ran multiple SCAN operations, and each scanIterator loop continued until the entire relevant portion of the keyspace was traversed to find all keys matching the pattern.\nBut how could SCAN, even if sometimes slow itself (as seen in the slow logs), cause delays for fast O(1) commands like GET or SET? Our server-side metrics (like Dragonfly\u0026rsquo;s Pipeline Latency) showed quick execution times for those O(1) commands. This led to a new hypothesis: the server metrics likely measured only the actual CPU time for command execution, not the total turnaround time experienced by the client, which includes any wait time before the command gets processed.\nEven though SCAN is non-blocking, issuing a large number of SCAN commands concurrently, especially when each needs to iterate over a growing keyspace (100k-200k+ keys), could potentially overwhelm the server\u0026rsquo;s connection-handling threads (even Dragonfly\u0026rsquo;s multiple threads). If threads were busy processing numerous, longer-running SCAN iterations, incoming GET, SET, etc., commands would have to wait longer before being picked up for execution, increasing their total observed latency from the client\u0026rsquo;s perspective. The performance degradation of SCAN with more keys, combined with its high frequency, created a bottleneck that impacted all other operations.\nThe Fix: Replacing SCAN with SETs Armed with this hypothesis, the fix became clear: we needed to drastically reduce or eliminate our reliance on SCAN for finding keys to invalidate.\nWe implemented an alternative approach:\nFor each user (or entity needing cache invalidation), maintain a Redis SET containing the keys associated with that user. When an invalidation event occurs for a user, instead of scanning the keyspace, retrieve the list of keys directly from the user\u0026rsquo;s specific SET using the SMEMBERS command (which is much faster for this purpose). Delete the keys obtained from the SET. This required some additional logic (housekeeping) to keep these SETs up-to-date as new keys were cached, but the performance benefits far outweighed the complexity.\nSo we opted for a more optimal way to invalidate cache where we also stored key names related to a user in a redis SET, so the use of SCAN was not moot because we do not need to scan the namespace every time to first prepare list of keys to delete now we can get that with just a SMEMBERS command which gives list of set elements. A little housekeeping was needed to maintain this set for each user, but it still outweighs the benefits.\nThe Results: Latency Tamed This change dramatically solved the high latency issue.\nFirst, the frequency of the SCAN command dropped to nearly zero, as expected. Consequently, the latency spikes across all commands disappeared. The overall p99 latency stabilized at a much healthier level. Interestingly, even the server-side execution time reported by Dragonfly showed improvement, suggesting the reduced load from SCAN allowed other commands to be processed more efficiently internally as well. The final result was a significant drop in p99 latency, bringing it down from peaks often exceeding 200ms (and sometimes reaching ~500ms as shown below) to consistently around ~40ms. p99 latency comparison showing reduction from peaks around ~500ms down to ~40ms after the fix\nConclusion: Trust the Clues Looking back at how we tackled our high Redis command latency (~200ms+ p99), the journey involved ramping up the setup for observability, and exploring several potential culprits. While investigating connection pooling, profiling code execution, and even analyzing network packets with tcpdump were valuable exercises, they ultimately didn\u0026rsquo;t lead us to the root cause in this case.\nThe most significant clue, in hindsight, was the temporary fix we got from restarting the DragonflyDB instance. If we had focused more intently from the start on why that restart helped ‚Äì realizing it pointed directly to the state accumulated within the database (specifically, the key count) ‚Äì and correlated that with our application\u0026rsquo;s command usage patterns, we likely would have identified the high-frequency, full-keyspace SCAN operations as the bottleneck much sooner.\nThe real issue wasn\u0026rsquo;t low-level network glitches or basic connection handling, but an application-level pattern: frequent SCANs over a growing keyspace were overwhelming the server, increasing wait times for all commands. Switching our invalidation logic to use Redis SETs (SMEMBERS) eliminated this problematic pattern, finally bringing our p99 latency down to a stable ~40ms. Although optimizing the SCAN operation itself using [[https://redis.io/docs/latest/operate/oss_and_stack/reference/cluster-spec/#hash-tags]] was another interesting possibility (ensuring keys with the same tag land in the same hash slot to potentially limit scan scope), we didn\u0026rsquo;t opt for this solution since this required a rethink of our cache key nomenclature and would have involved substantial changes. Ultimately, the most direct path to a solution often lies in understanding the application\u0026rsquo;s behavior and trusting the most obvious clues, rather than immediately reaching for the deepest diagnostic tools.\n","permalink":"https://harshrai654.github.io/blogs/debugging-redis-latency/","summary":"\u003cp\u003eThis article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.\u003c/p\u003e\n\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eAfter introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as \u003ccode\u003eioredis\u003c/code\u003e for interacting with our Redis-compatible database.\nOnce we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand \u003cem\u003ewhy\u003c/em\u003e this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions ‚Äì options ranged from open-source SDKs (\u003ca href=\"https://opentelemetry.io/docs/languages/js/\"\u003eOpenTelemetry Node.js SDK\u003c/a\u003e) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted \u003ca href=\"https://grafana.com/oss/tempo/\"\u003eGrafana Tempo\u003c/a\u003e instance to test the setup and feasibility. (So far, the setup is working great, and I\u0026rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.\u003c/p\u003e","title":"Debugging Redis Latency"},{"content":"Socket File Descriptors and Their Kernel Structures A socket is a special type of file descriptor (FD) in Linux, represented as socket:[inode]. Unlike regular file FDs, socket FDs point to in-memory kernel structures, not disk inodes. The /proc/\u0026lt;pid\u0026gt;/fd directory lists all FDs for a process, including sockets. The inode number of a socket can be used to inspect its details via tools like ss and /proc/net/tcp. Example: Checking Open FDs for Process 216 ls -l /proc/216/fd Output:\nlrwx------. 1 root root 64 Mar 2 09:01 0 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 1 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 2 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 3 -\u0026gt; \u0026#39;socket:[35587]\u0026#39; Here, FD 3 is a socket pointing to inode 35587. Checking FD Details cat /proc/216/fdinfo/3 Output: pos: 0 flags: 02 mnt_id: 10 ino: 35587 How Data Flows Through a Socket (User Space to Kernel Space) When a process writes data to a socket, it is copied from user-space memory to kernel-space buffers (using syscall write()). The kernel then processes and forwards the data to the network interface card (NIC). This copying introduces overhead, which can be mitigated using zero-copy techniques like sendfile() and io_uring. (A tweet which might recall this) TCP 3-Way Handshake (How a Connection is Established) A TCP connection is established through a 3-way handshake between the client and server:\nClient ‚Üí SYN (Initial sequence number) Server ‚Üí SYN-ACK (Acknowledges client‚Äôs SYN, sends its own) Client ‚Üí ACK (Acknowledges server‚Äôs SYN-ACK) Checking a Listening TCP Port ss -aep | grep 35587 Output:\ntcp LISTEN 0 0 0.0.0.0:41555 0.0.0.0:* users:((\u0026#34;nc\u0026#34;,pid=216,fd=3)) ino:35587 sk:53f53fa7 Port 41555 is in the LISTEN state, bound to nc (netcat). It corresponds to socket inode 35587. TCP Connection Queues in the Kernel Once a TCP connection request arrives, it goes through two queues managed by the kernel:\n1Ô∏è] SYN Queue (Incomplete Connection Queue) Holds half-open connections (received SYN but not yet fully established). If this queue overflows, new SYN requests may be dropped (SYN flood attack risk). 2]Accept Queue (Backlog Queue, Fully Established Connections) Holds connections that have completed the handshake and are waiting for accept(). Controlled by listen(sockfd, backlog), where backlog defines max queue size If full, new connections are dropped. Checking Connection Queues ss -ltni Output:\nState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 5 0.0.0.0:8080 0.0.0.0:* Recv-Q (Accept Queue, Backlog Queue) ‚Üí Number of connections waiting in the backlog. Send-Q (Not relevant here) ‚Üí Usually for outbound data. Checking Kernel TCP Queues via **/proc/net/tcp** cat /proc/net/tcp Output:\nsl local_address rem_address st tx_queue rx_queue tr tm-\u0026gt;when retrnsmt uid timeout inode 0: 00000000:A253 00000000:0000 0A 00000000:00000000 00:00000000 00000000 0 0 35587 1 0000000053f53fa7 100 0 0 10 0 tx_queue ‚Üí Data waiting to be sent. rx_queue ‚Üí Data waiting to be read. The Role of the Kernel in TCP Connections The Linux kernel manages the entire TCP stack:\nHandshaking, sequencing, retransmissions, timeouts. Maintaining connection queues \u0026amp; buffering. Interacting with the NIC for packet transmission. Applications don‚Äôt deal with raw packets directly‚Äîthey only read/write to sockets, while the kernel handles the rest.\nFlow Diagram: TCP Connection Journey with Kernel Involvement Client (User Space) Kernel (Server) Application (User Space) | | | | 1. SYN | | |---------------------------\u0026gt;| | | | | | 2. SYN-ACK | | |\u0026lt;---------------------------| | | | | | 3. ACK | | |---------------------------\u0026gt;| | | | Connection Added to SYN Queue | | |-----------------------------\u0026gt;| | | | | | Connection Moved to Accept Queue | | |-----------------------------\u0026gt;| | | | | | Application Calls `accept()` | | |-----------------------------\u0026gt;| | | | | | Data Transfer Begins | Why Each Connection Needs a Separate FD When a server listens on a port, it creates a listening socket FD. When a client initiates a connection: The kernel accepts the connection using the 3-way handshake. The kernel creates a new socket structure for this connection. The server application calls accept(), which returns a new FD. Why is a New FD Required? Each TCP connection requires its own state:\nSequence numbers (to track packets in order) Receive and send buffers Connection state (e.g., established, closed) Does the Communication Happen on the Same Port?\nYes, all connections still use the same local port (the port used for listening for connection on the server side). But, each accepted connection is a unique socket with a different remote IP/port pair. The kernel distinguishes connections by:\n(Local IP, Local Port) \u0026lt;\u0026ndash;\u0026gt; (Remote IP, Remote Port). Think of it like this:\nThe listening socket is like a front desk at a hotel. Every guest (client) gets their own room (new socket), but the front desk (listening socket) stays the same. Multiple Sockets on the Same Port (SO_REUSEPORT) Allows multiple FDs bound to the same port. Kernel load-balances connections across them. Used in: Nginx, HAProxy. Example: Multi-Threaded Server with SO_REUSEPORT int sock1 = socket(AF_INET, SOCK_STREAM, 0); int sock2 = socket(AF_INET, SOCK_STREAM, 0); setsockopt(sock1, SOL_SOCKET, SO_REUSEPORT, \u0026amp;opt, sizeof(opt)); setsockopt(sock2, SOL_SOCKET, SO_REUSEPORT, \u0026amp;opt, sizeof(opt)); bind(sock1, ...); bind(sock2, ...); listen(sock1, BACKLOG); listen(sock2, BACKLOG); References https://chatgpt.com/c/67c414f3-4830-8013-a058-0fd2596e3c07 ","permalink":"https://harshrai654.github.io/blogs/socket-file-descriptor-and-tcp-connections/","summary":"\u003ch2 id=\"socket-file-descriptors-and-their-kernel-structures\"\u003eSocket File Descriptors and Their Kernel Structures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003esocket\u003c/strong\u003e is a special type of file descriptor (FD) in Linux, represented as \u003ccode\u003esocket:[inode]\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eUnlike regular file FDs, socket FDs point to \u003cstrong\u003ein-memory kernel structures\u003c/strong\u003e, not disk inodes.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003e/proc/\u0026lt;pid\u0026gt;/fd\u003c/code\u003e directory lists all FDs for a process, including sockets.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003einode number\u003c/strong\u003e of a socket can be used to inspect its details via tools like \u003ccode\u003ess\u003c/code\u003e and \u003ccode\u003e/proc/net/tcp\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"example-checking-open-fds-for-process-216\"\u003eExample: Checking Open FDs for Process \u003ccode\u003e216\u003c/code\u003e\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003els -l /proc/216/fd\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eOutput:\u003c/strong\u003e\u003c/p\u003e","title":"Socket File Descriptor and TCP connections"},{"content":"Overall Organization Of Data In Disks Assuming we have a 256KB disk.\nDisk Blocks: The basic units of storage on the disk, each 4 KB in size. The disk is divided into these blocks, numbered from 0 to N-1 (where N is the total number of blocks). Inode Bitmap (i): Block 1; a bitmap tracking which inodes are free (0) or in-use (1). Data Bitmap (d): Block 2; a bitmap tracking which data blocks are free (0) or allocated (1). Inode Table (I): Blocks 3-7; an array of inodes, where each inode (256 bytes) holds metadata about a file, like size, permissions, and pointers to data blocks. 5 blocks of 4KB will contain 80 256 byte inode strutures. Data Region (D): Blocks 8-63; the largest section, storing the actual contents of files and directories. Inode Every inode has a unique identifier called an inode number (or i-number). This number acts like a file‚Äôs address in the file system, allowing the operating system to quickly locate its inode. For example:\nIn a system with 80 inodes, numbers might range from 0 to 79. Conventionally, the root directory is assigned inode number 2 (numbers 0 and 1 are often reserved or used for special purposes). The inode number is the key to finding a file‚Äôs metadata on the disk, and it‚Äôs stored in directory entries alongside the file‚Äôs name.\nHow Do We Jump to the Disk Block for a Specific Inode Number? In a file system like vsfs, inodes are stored consecutively in an inode table, a reserved area of the disk (e.g., spanning blocks 3 to 7). Each inode has a fixed size‚Äîlet‚Äôs say 256 bytes‚Äîand the disk is divided into blocks (e.g., 4096 bytes each). To locate a specific inode given its i-number, we calculate its exact position on the disk. Here‚Äôs how:\nIdentify the Inode Table‚Äôs Starting Block: Suppose the inode table starts at block 3. Calculate the Block Containing the Inode: Formula: block = (i-number * inode_size) / block_size + start_block Example: For inode 10, inode_size = 256 bytes, block_size = 4096 bytes, start_block = 3 (10 * 256) / 4096 = 2560 / 4096 = 0.625 ‚Üí integer part is 0. block = 0 + 3 = 3. So, inode 10 is in block 3. Calculate the Offset Within the Block: Formula: offset = (i-number * inode_size) % block_size Example: (10 * 256) % 4096 = 2560 % 4096 = 2560 bytes. Inode 10 starts 2560 bytes into block 3. Result: The operating system reads block 3 from the disk and jumps to offset 2560 bytes to access inode 10‚Äôs metadata. This process allows the file system to efficiently retrieve an inode‚Äôs information and, from there, its data blocks. Since multiple processes may have file descriptors for the same file opened with their own data of offset to read from, multiple processes will be accessing the same inode structure to read about file and modify its data (Here inodes data would mean modifying things like last accessed time or adding another entry for list of data blocks etc), So i-node needs some sort of concurrency control in place. (More on this)\nHow does inode know the data it owns An inode is a data structure in a file system that stores information about a file, including where its data is located on the disk. Instead of holding the file‚Äôs data itself, the inode contains pointers that reference the disk blocks where the data is stored.\nDisk Blocks: These are fixed-size chunks of storage on the disk (e.g., 4 KB each) that hold the actual file content. Pointers: These are entries in the inode that specify the locations of these disk blocks. The inode uses these pointers to keep track of all the blocks that make up a file, allowing the file system to retrieve the data when needed.\nWhat Are Disk Addresses? Disk addresses are the identifiers that tell the file system the exact physical locations of data blocks on the disk. Think of them as a map: each address corresponds to a specific block, such as block number 100, which might map to a particular sector and track on a hard drive.\nFor example, if a file is 8 KB and the block size is 4 KB, the inode might have two pointers with disk addresses like \u0026ldquo;block 50\u0026rdquo; and \u0026ldquo;block 51,\u0026rdquo; pointing to the two blocks that hold the file‚Äôs data. How Does an Inode Manage Disk Blocks? The inode organizes its pointers in a way that can handle files of different sizes efficiently. It uses a combination of direct pointers and indirect pointers, forming a multi-level indexing structure.\n1. Direct Pointers The inode starts with a set of direct pointers, which point straight to the disk addresses of data blocks. Example: If the block size is 4 KB and the inode has 12 direct pointers, it can directly address 12 √ó 4 KB = 48 KB of data. 2. Indirect Pointers (Multi-Level Indexing) For files too big for direct pointers alone, the inode uses indirect pointers, which point to special blocks that themselves contain more pointers. This creates a hierarchical, or multi-level, structure.\nSingle Indirect Pointer\nThis pointer points to a block (called an indirect block) that contains a list of disk addresses to data blocks.\nExample: If a block is 4 KB and each pointer is 4 bytes, the indirect block can hold 4 KB / 4 bytes = 1024 pointers. That‚Äôs 1024 √ó 4 KB = 4 MB of additional data.\nTotal with Direct: With 12 direct pointers and 1 single indirect pointer, the file can reach (12 + 1024) √ó 4 KB = 4,096 KB (about 4 MB).\nDouble Indirect Pointer\nThis pointer points to a block that contains pointers to other single indirect blocks, each of which points to data blocks.\nExample: The double indirect block might hold 1024 pointers to single indirect blocks. Each of those holds 1024 pointers to data blocks, so that‚Äôs 1024 √ó 1024 = 1,048,576 data blocks, or about 4 GB with 4 KB blocks.\nTotal with Direct and Single: (12 + 1024 + 1,048,576) √ó 4 KB ‚âà 4 GB.\nThis structure acts like an imbalanced tree: small files use only direct pointers, while larger files use additional levels of indirection as needed.\nWhy Use Multi-Level Indexing? The multi-level indexing structure is designed to balance efficiency and scalability:\nSmall Files: Most files are small, so direct pointers handle them quickly without extra overhead. Large Files: Indirect pointers allow the inode to scale up to support massive files by adding more layers of pointers. How It Works in Practice When the file system needs to find a specific block in a file:\nIt checks the inode‚Äôs direct pointers first. If the block number is beyond the direct pointers‚Äô range, it follows the single indirect pointer to the indirect block and looks up the address there. For even larger block numbers, it traverses the double indirect or triple indirect pointers, following the hierarchy until it finds the right disk address. This process ensures the file system can efficiently locate any block, no matter how big the file is.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-implementation.pdf https://grok.com/chat/d429de0e-556f-426c-84bd-21ff1b0c4002 (Contains reference to actual inode structure implementation along with locks it uses on Linux) https://grok.com/chat/c86e3040-2f86-4aa9-a317-1c0a464564a3?referrer=website ","permalink":"https://harshrai654.github.io/blogs/file-system-implementation/","summary":"\u003ch2 id=\"overall-organization-of-data-in-disks\"\u003eOverall Organization Of Data In Disks\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eAssuming we have a 256KB disk\u003c/em\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDisk Blocks\u003c/strong\u003e: The basic units of storage on the disk, \u003cem\u003eeach 4 KB in size.\u003c/em\u003e The disk is divided into these blocks, numbered from 0 to N-1 (where N is the total number of blocks).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInode Bitmap (i)\u003c/strong\u003e: Block 1; a bitmap tracking which inodes are free (0) or in-use (1).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Bitmap (d)\u003c/strong\u003e: Block 2; a bitmap tracking which data blocks are free (0) or allocated (1).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInode Table (I)\u003c/strong\u003e: Blocks 3-7; an array of inodes, where each inode (256 bytes) holds metadata about a file, like size, permissions, and pointers to data blocks.\n5 blocks of 4KB will contain 80 256 byte inode strutures.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Region (D)\u003c/strong\u003e: Blocks 8-63; the largest section, storing the actual contents of files and directories.\n\u003cimg alt=\"Pasted image 20250301204506.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20250301204506.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"inode\"\u003eInode\u003c/h2\u003e\n\u003cp\u003eEvery inode has a unique identifier called an \u003cstrong\u003einode number\u003c/strong\u003e (or \u003cstrong\u003ei-number\u003c/strong\u003e). This number acts like a file‚Äôs address in the file system, allowing the operating system to quickly locate its inode. For example:\u003c/p\u003e","title":"Understanding Inodes and Disk Layout"},{"content":"Files and directories File systems virtualize persistent storage (e.g., hard drives, SSDs) into user-friendly files and directories, adding a third pillar to OS abstractions (processes for CPU, address spaces for memory).\nFile Paths and System Calls Files are organized in a tree-like directory structure, starting from the root (/). A file‚Äôs location is identified by its pathname (e.g., /home/user/file.txt). To interact with files, processes use system calls:\nopen(path, flags): Opens a file and returns a file descriptor (fd). read(fd, buffer, size): Reads data from the file into a buffer using the fd. write(fd, buffer, size): Writes data to the file via the fd. close(fd): Closes the file, freeing the fd. File Descriptors A file descriptor is a small integer, unique to each process, that identifies an open file. When a process calls open(), the operating system assigns it the next available fd (e.g., 3, 4, etc.). Every process starts with three default fds:\n0: Standard input (stdin) 1: Standard output (stdout) 2: Standard error (stderr) Quick Note: File Descriptors \u0026amp; Terminals Every process starts with three standard file descriptors:\n0 (stdin), 1 (stdout), 2 (stderr). Where They Point By default they link to a terminal device (e.g., /dev/pts/0 for Terminal 1, /dev/pts/1 for Terminal 2). These are character devices with a major number (e.g., 136) for the tty driver and a unique minor number (e.g., 0 or 1) for each instance.\nCommand Output (Terminal 1):\n1ls -l /proc/self/fd 2lrwx------ 1 runner runner 64 Feb 23 11:47 0 -\u0026gt; /dev/pts/0 3lrwx------ 1 runner runner 64 Feb 23 11:47 1 -\u0026gt; /dev/pts/0 4lrwx------ 1 runner runner 64 Feb 23 11:47 2 -\u0026gt; /dev/pts/0 Device Details:\n1ls -l /dev/pts/0 2crw--w---- 1 runner tty 136, 0 Feb 23 11:53 /dev/pts/0 3 4~/workspace$ stat /dev/pts/0 5File: /dev/pts/0 6Size: 0 Blocks: 0 IO Block: 1024 character special file 7Device: 0,1350 Inode: 3 Links: 1 Device type: 136,0 8Access: (0620/crw--w----) Uid: ( 1000/ runner) Gid: ( 5/ tty) 9Access: 2025-02-23 12:13:52.419852946 +0000 10Modify: 2025-02-23 12:13:52.419852946 +0000 11Change: 2025-02-23 11:36:28.419852946 +0000 12 Birth: - Versus a File A regular file descriptor (e.g., for test.txt) points to a disk inode with data blocks, tied to a filesystem device (e.g., /dev/sda1), not a driver.\nExample:\n1ls -li test.txt 212345 -rw-r--r-- 1 runner runner 5 Feb 23 12:00 test.txt 3 4~/workspace$ stat test.txt 5 File: test.txt 6 Size: 288 Blocks: 8 IO Block: 4096 regular file 7Device: 0,375 Inode: 1111 Links: 1 8Access: (0644/-rw-r--r--) Uid: ( 1000/ runner) Gid: ( 1000/ runner) 9Access: 2025-02-23 11:40:57.014322027 +0000 10Modify: 2025-02-23 11:41:41.800233516 +0000 11Change: 2025-02-23 11:41:41.800233516 +0000 12 Birth: 2025-02-23 11:40:57.014322027 +0000 Fun Test Redirected Terminal 1‚Äôs output to Terminal 2:\nCommand: echo \u0026quot;Hello\u0026quot; \u0026gt; /dev/pts/1 Result: \u0026ldquo;Hello\u0026rdquo; appeared in Terminal 2 (/dev/pts/1, minor 1)! Open File Table The Open File Table (OFT) is a system-wide structure in kernel memory that tracks all open files. Each entry in the OFT includes:\nThe current offset (position for the next read/write). Permissions (e.g., read, write). A reference count (if multiple processes share the file). Each process has its own array of file descriptors, where each fd maps to an entry in the OFT. For example, process A‚Äôs fd 3 and process B‚Äôs fd 4 might point to the same OFT entry if they‚Äôve opened the same file.\nExample: Showing entries of a PID in open file table\n1~/workspace$ lsof -p 113 2COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME 3bash 113 runner cwd DIR 0,375 394 256 /home/runner/workspace 4bash 113 runner rtd DIR 0,1324 4096 1562007 / 5bash 113 runner 0u CHR 136,0 0t0 3 /dev/pts/0 6bash 113 runner 1u CHR 136,0 0t0 3 /dev/pts/0 7bash 113 runner 2u CHR 136,0 0t0 3 /dev/pts/0 8bash 113 runner 255u CHR 136,0 0t0 3 /dev/pts/0 As you see above that the process had 0,1 and 2 FDs as well as FD for a directory as well.\nReference counting in OFT Reference counting is a technique used in operating systems to manage entries in the Open File Table (OFT), which stores information about open files shared by processes or file descriptors.\nHow It Works Each OFT entry has a reference count that tracks how many file descriptors (from one or more processes) refer to it. When a file is opened, the reference count increases by 1. If the same file is reused (e.g., via fork() or dup()), the count increments without creating a new entry. When a file descriptor is closed (e.g., with close(fd)), the count decreases by 1. The OFT entry is removed only when the reference count reaches 0, meaning no process is using it. How It Helps Remove Entries Reference counting ensures an OFT entry is deleted only when it‚Äôs no longer needed. For example, after a fork(), both parent and child processes share the same OFT entry (reference count = 2). Closing the file in one process lowers the count to 1, but the entry persists until the second process closes it, bringing the count to 0.\nWhy It‚Äôs Useful This method efficiently manages shared file resources, preventing premature removal of file metadata (like the current offset) while any process still needs it.\nINode What is an Inode? An inode (short for \u0026ldquo;index node\u0026rdquo;) is a fundamental data structure in UNIX-based file systems. It stores essential metadata about a file, enabling the system to manage and locate files efficiently. Each file in the system is uniquely identified by its inode number (also called i-number). The metadata stored in an inode includes:\nFile size: The total size of the file in bytes.\nPermissions: Access rights defining who can read, write, or execute the file.\nOwnership: User ID (UID) and group ID (GID) of the file\u0026rsquo;s owner.\nTimestamps:\nLast access time (when the file was last read). Last modification time (when the file\u0026rsquo;s content was last changed). Last status change time (when the file\u0026rsquo;s metadata, like permissions, was last modified). Pointers to data blocks: Locations on disk where the file\u0026rsquo;s actual content is stored.\nThe inode does not store the file\u0026rsquo;s name or its content; these are managed separately. The inode\u0026rsquo;s role is to provide a compact and efficient way to access a file\u0026rsquo;s metadata. Each inode is stored in inode block in disk.\nHow the stat System Call Works The stat system call is used to retrieve metadata about a file without accessing its actual content. It provides a way for programs to query information like file size, permissions, and timestamps. Here\u0026rsquo;s how it works:\nInput: The stat system call takes a file path as input. Locate the inode: The file system uses the file path to find the corresponding inode number. (Insert link to FS implemetation note here explaining how FS searches inode and data block from given file path) Retrieve metadata: The inode is fetched from disk (or from a cache, if available, for faster access). Populate the struct stat buffer: The metadata stored in the inode is copied into a struct stat buffer, which contains fields for file size, permissions, ownership, timestamps, and more. Return to the user: The struct stat buffer is returned to the calling program, providing all the metadata for the file. Because the stat system call only accesses the inode and not the file\u0026rsquo;s content, it is a fast and efficient operation. This separation of metadata (stored in the inode) and content (stored in data blocks) allows the system to quickly retrieve file information without unnecessary disk I/O.\nDirectories ls -al Output for Directories\n1total 152 2drwxr-xr-x 1 runner runner 394 Feb 23 11:40 . 3drwxrwxrwx 1 runner runner 46 Feb 23 11:36 .. 4-rw-r--r-- 1 runner runner 174 Feb 15 11:31 .breakpoints 5drwxr-xr-x 1 runner runner 34 Feb 15 11:32 .cache 6drwxr-x--- 1 runner runner 260 Feb 15 11:31 .ccls-cache 7-rw-r--r-- 1 runner runner 1564 Dec 21 11:44 common_threads.h 8-rwxr-xr-x 1 runner runner 16128 Dec 21 11:54 deadlock 9-rw-r--r-- 1 runner runner 647 Dec 21 11:51 deadlock.c 10-rwxr-xr-x 1 runner runner 16160 Dec 21 11:57 deadlock-global 11-rw-r--r-- 1 runner runner 748 Dec 21 11:57 deadlock-global.c 12-rw-r--r-- 1 runner runner 6320 Feb 23 11:36 generated-icon.png 13-rw-r--r-- 1 runner runner 429 Mar 8 2024 .gitignore 14-rwxr-xr-x 1 runner runner 15984 Dec 21 11:51 main 15-rw-r--r-- 1 runner runner 415 Dec 21 11:51 main.c 16-rwxr-xr-x 1 runner runner 16816 Aug 16 2024 main-debug 17-rw-r--r-- 1 runner runner 411 Dec 12 2023 Makefile 18-rwxr-xr-x 1 runner runner 16136 Dec 14 11:04 mem 19-rw-r--r-- 1 runner runner 1437 Aug 16 2024 .replit 20-rw-r--r-- 1 runner runner 134 Feb 23 13:54 replit.nix 21-rwxr-xr-x 1 runner runner 15936 Dec 21 11:59 signal 22-rw-r--r-- 1 runner runner 329 Dec 21 11:59 signal.c 23-rw-r--r-- 1 runner runner 288 Feb 23 11:41 test.txt 24drwxr-xr-x 1 runner runner 42 Feb 15 11:32 wcat Size of a directory only means storage needed to store each directory entry which basically comprise entry name and its inode number along with some other metdata\nDirectory Data Block Contents The data blocks of a directory contain:\nDirectory entries: Mapping of file/subdirectory names to their inode numbers. Special entries: . (current directory) and .. (parent directory). Optimization for Small Directories For small directories, some file systems store directory entries directly in the inode itself, avoiding separate data blocks. This optimization saves space and speeds up access.\nReading Directory Entries with System Calls To programmatically read directory contents following sys calls are used:\nopendir(path): Opens the directory. readdir(dir): Reads one entry at a time (returns a struct dirent with name and inode number. closedir(dir): Closes the directory. Permission bits and their octal representation Format: ls -l displays permissions as rwxr-xr-x:\nFirst character: d (directory), - (file), or l (symlink). Next 9 bits: Three groups of rwx for owner, group, and others: r = read, w = write, x = execute (run for files, enter for directories). Converting a Full 9-Bit Permission to Numeric (Octal) Representation Example: rwxr-xr-x\nBreakdown: Split into three groups (owner, group, others): Owner: rwx. Group: r-x. Others: r-x. Step-by-Step Conversion\nOwner: rwx: r = 1, w = 1, x = 1 ‚Üí Binary: 111. Decimal: 1√ó4 + 1√ó2 + 1√ó1 = 4 + 2 + 1 = 7. Group: r-x: r = 1, w = 0, x = 1 ‚Üí Binary: 101. Decimal: 1√ó4 + 0√ó2 + 1√ó1 = 4 + 1 = 5. Others: r-x: r = 1, w = 0, x = 1 ‚Üí Binary: 101. Decimal: 1√ó4 + 0√ó2 + 1√ó1 = 4 + 1 = 5. Final Octal Representation\nCombine the three digits: 7 5 5. Result: rwxr-xr-x = 755. Quick Recall: Split rwx into 3 groups, convert each to binary (1s/0s), sum (4, 2, 1), get octal (e.g., 755). Hard Links, Symbolic Links (Including Size), and File Deletion (Based on OSTEP PDF) Hard Links and unlink Definition: A hard link (PDF p. 18-19) is an extra directory entry pointing to the same inode, created with link(old, new).. When a directory reference its parent directory (with ..) it increases hard link count by 1. For a new empty directory its hard count is always 2 since it is referred by itself (with .) and also referred by its parent\u0026rsquo;s directory entry (with ..) With unlink: Removes a name-to-inode link, decrements the inode‚Äôs link count. When count hits 0 (and no processes use it), the inode and data blocks are freed from disk. Symbolic Links and Dangling References Definition: A symbolic (soft) link (PDF p. 20-21) is a distinct file storing a pathname to another file, created with ln -s old new. Size: Its size equals the length of the stored pathname (e.g., 4 bytes for \u0026ldquo;file\u0026rdquo;, 15 bytes for \u0026ldquo;alongerfilename\u0026rdquo;; PDF p. 21). How It Works: References the path, not the inode directly; accessing it resolves the path. Dangling Reference: If the target is deleted (e.g., unlink file), the symbolic link persists but points to nothing, causing errors (e.g., \u0026ldquo;No such file or directory\u0026rdquo;). Disk, Partition, Volume, File System Hierarchy, and Mounting Distinction and Hierarchy Disk: Physical storage device (e.g., hard drive, SSD) holding raw data blocks. Partition: A subdivided section of a disk (e.g., /dev/sda1), treated as a separate unit. Volume: A logical abstraction, often a partition or group of partitions, formatted with a file system (FS). File System: Software structure (e.g., ext3, AFS) organizing data into files and directories on a volume. Hierarchy: Disk ‚Üí Partitions ‚Üí Volumes ‚Üí File Systems.\nMounting Process Creation: Use mkfs to format a partition with a file system (e.g., mkfs -t ext3 /dev/sda1 creates an empty FS). Mounting: The mount command (PDF p. 24) attaches a file system to the directory tree at a mount point (e.g., mount -t ext3 /dev/sda1 /home/users), making its contents accessible under that path. Multiple File Systems on One Machine A single machine can host multiple file systems by mounting them at different points in the tree (PDF p. 26). Example Output (from mount): 1/dev/sda1 on / type ext3 (rw) # Root FS 2/dev/sda5 on /tmp type ext3 (rw) # Separate tmp FS 3AFS on /afs type afs (rw) # Distributed FS How: Each partition or volume gets its own FS type and mount point, unified under one tree (e.g., /). Quick Recall: Disk splits into partitions; volumes get FS; mount glues them into a tree; multiple FS coexist at different paths.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-intro.pdf https://grok.com/share/bGVnYWN5_47ab49d6-aa1d-4de1-9bbe-0d47332e12fe https://grok.com/share/bGVnYWN5_7db77c97-d33d-4543-9993-d5aa362c8b2b ","permalink":"https://harshrai654.github.io/blogs/files-and-directories/","summary":"\u003ch1 id=\"files-and-directories\"\u003eFiles and directories\u003c/h1\u003e\n\u003cp\u003eFile systems virtualize persistent storage (e.g., hard drives, SSDs) into user-friendly files and directories, adding a third pillar to OS abstractions (processes for CPU, address spaces for memory).\u003c/p\u003e\n\u003ch2 id=\"file-paths-and-system-calls\"\u003eFile Paths and System Calls\u003c/h2\u003e\n\u003cp\u003eFiles are organized in a \u003cstrong\u003etree-like directory structure\u003c/strong\u003e, starting from the root (/). A file‚Äôs location is identified by its \u003cstrong\u003epathname\u003c/strong\u003e (e.g., /home/user/file.txt). To interact with files, processes use \u003cstrong\u003esystem calls\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eopen(path, flags)\u003c/strong\u003e: Opens a file and returns a \u003cstrong\u003efile descriptor\u003c/strong\u003e (fd).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eread(fd, buffer, size)\u003c/strong\u003e: Reads data from the file into a buffer using the fd.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ewrite(fd, buffer, size)\u003c/strong\u003e: Writes data to the file via the fd.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eclose(fd)\u003c/strong\u003e: Closes the file, freeing the fd.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"file-descriptors\"\u003eFile Descriptors\u003c/h2\u003e\n\u003cp\u003eA \u003cstrong\u003efile descriptor\u003c/strong\u003e is a small integer, unique to each process, that identifies an open file. When a process calls open(), the operating system assigns it the next available fd (e.g., 3, 4, etc.). Every process starts with three default fds:\u003c/p\u003e","title":"Files And Directories"},{"content":"RAID Disks Three axes on which disks are analysed\nCapacity - How much capacity is needed to store X bytes of data Reliability - How much fault-tolerant is the disk Performance - Read and write speeds (Sequential and random) To make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification) When we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database In a way this set of disks also have challenges similar to distributes databases.\nThere are different types of data arrangement in set of physical disks which results in different types/levels of RAID\nRAID Level 0 - Striping Disk 0 Disk 1 Disk 2 Disk 3 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Here each cell is a disk block which will be of fixed size (for example 4KB) A vertical column shows blocks stored by a single disk\nStriping: Writing out chunks (in multiple of disk block size) to each disk, one at a time so that we have the data spread uniformly across the disk. When read or write requests comes up to disk it comes in the form of stripe number (row number in above illustration) and based on RAID level disk controller knows which block it has to access and which disk to read it from.\nTradeoffs:\nThis level has no reliability as a disk failure always means loss of some data. This arrangement is good for capacity as we are utilizing all disks for storage. RAID Level 1 - Mirroring Disk 0 Disk 1 Disk 2 Disk 3 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 Copies of blocks made to two disks, tradeoff of reliability over capacity. Consistent update problem Imagine the write is issued to the RAID, and then the RAID decides that it must be written to two disks, disk 0 and disk 1. The RAID then issues the write to disk 0, but just before the RAID can issue the request to disk 1, a power loss (or system crash) occurs. In this unfortunate case, let us assume that the request to disk 0 completed (but clearly the request to disk 1 did not, as it was never issued). The result of this untimely power loss is that the two copies of the block are now inconsistent; the copy on disk 0 is the new version, and the copy on disk 1 is the old. What we would like to happen is for the state of both disks to change atomically, i.e., either both should end up as the new version or neither. The general way to solve this problem is to use a write-ahead log of some kind to first record what the RAID is about to do (i.e., update two disks with a certain piece of data) before doing it. By taking this approach, we can ensure that in the presence of a crash, the right thing will happen; by running a recovery procedure that replays all pending transactions to the RAID, we can ensure that no two mirrored copies (in the RAID-1 case) are out of sync. One last note: because logging to disk on every write is prohibitively expensive, most RAID hardware includes a small amount of non-volatile RAM (e.g., battery-backed) where it performs this type of logging. Thus, consistent update is provided without the high cost of logging to disk.\nTradeoffs:\nThis level can tolerate up to N/2 disk failures. This arrangement is good for reliability over cost of capacity being half Even though updates for a block needs to happen at two separate disks, The write would be parallel but still slower than updating a single disk (If we consider different seek and rotational time for both disks) RAID Level 4 - Parity Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 4 5 6 7 P1 8 9 10 11 P2 12 13 14 15 P3 N - 1 Disks follow striping with the Nth disk containing parity block for each strip row Concept: RAID 4 adds redundancy to a disk array using parity, which consumes less storage space than mirroring. How it works:\nA single parity block is added to each stripe of data blocks. The parity block stores redundant information calculated from the data blocks in its stripe. The XOR function is used to calculate parity. XOR returns 0 if there are an even number of 1s in the bits, and 1 if there are an odd number of 1s. The parity bit ensures that the number of 1s in any row, including the parity bit, is always even. Example of Parity Calculation: Imagine a RAID 4 system with 4-bit data blocks. Let\u0026rsquo;s say we have the following data in one stripe:\nBlock 0: 0010 Block 1: 1001 Block 2: 0110 Block 3: 1011 To calculate the parity block, we perform a bitwise XOR across the corresponding bits of each data block:\nBit 1 (from left): 0 XOR 1 XOR 0 XOR 1 = 0 Bit 2: 0 XOR 0 XOR 1 XOR 0 = 1 Bit 3: 1 XOR 0 XOR 1 XOR 1 = 1 Bit 4: 0 XOR 1 XOR 0 XOR 1 = 0 Therefore, the parity block would be 0110.\nData recovery:\nIf a data block is lost, the remaining blocks in the stripe, including the parity block, are read. The XOR function is applied to these blocks to reconstruct the missing data. For example, if Block 2 (0110) was lost, we would XOR Block 0, Block 1, Block 3, and the parity block: 0010 XOR 1001 XOR 1011 XOR 0110 = 0110. Performance:\nRAID 4 has a performance cost due to the overhead of parity calculation. Crucially, all write operations must update the parity disk. Write Bottleneck: If multiple random write requests comes for various blocks at the same time then they all will all require to update different parity blocks but all parity blocks are in one disk so multiple updates to different parity block will be done one after the other because all are in the same disk which eventually makes concurrent random write requests sequential in nature this is also known as small-write problem RAID Level 5 - Rotating Parity Instead of having one dedicated disk for parity blocks for each stripe, distribute the parity blocks in rotating manner to all disks for each stripe.\nDisk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 4 5 6 P1 7 8 9 P2 10 11 12 P3 13 14 15 Small-write: Concurrent random write requests to blocks of different blocks of different stripes can now be done parallelly since parity block for each stripe will be in different disk. It is still possible that blocks of different stripes need to update parity blocks which are lying in same disk (due to rotating nature of parity block) write requests to blocks of same stripes will still be sequential since parity block will be on same disk for all the blocks of same stripe. In summary: While RAID 5 significantly improves random write performance compared to RAID 4, it doesn\u0026rsquo;t completely eliminate the possibility of parity-related bottlenecks. The rotated parity distribution reduces the likelihood of contention, but it doesn\u0026rsquo;t guarantee that parity updates will always be fully parallel. The chance of multiple stripes\u0026rsquo; parity residing on the same disk is still there, leading to potential performance degradation.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf How Data recovery happens with parity drive in RAID: https://blogs.oracle.com/solaris/post/understanding-raid-5-recovery-with-elementary-school-math#:~:text=We%20need%20to%20read%20all%20date%20from%20other%20drives%20to%20recovery%20parity.\n","permalink":"https://harshrai654.github.io/blogs/raid-redundant-array-of-inexpensive-disk/","summary":"\u003ch1 id=\"raid-disks\"\u003eRAID Disks\u003c/h1\u003e\n\u003cp\u003eThree axes on which disks are analysed\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCapacity - How much capacity is needed to store X bytes of data\u003c/li\u003e\n\u003cli\u003eReliability - How much fault-tolerant is the disk\u003c/li\u003e\n\u003cli\u003ePerformance - Read and write speeds (Sequential and random)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification)\nWhen we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database\nIn a way this set of disks also have challenges similar to distributes databases.\u003c/p\u003e","title":"RAID (Redundant array of inexpensive disk)"},{"content":"Segmented Page Table Page table can grow large for a 32-bit address space and 4 KB page size we will be using 20 bits for virtual page number resulting in 2^20 bytes (i.e. 4MB of page table) for a single page table and each process will have its own page table so it is possible that we will be storing ~100sMB for page table alone which is not good. For above page table with 4 bits for VPN (Virtual page number) we can see that only VPN 0,4,14 and 15 are valid i.e. pointing to a PFN (Physical Frame Number) other PTEs (Page table entry) are just taking up space which is not used. We can use segmentation here with base and bound registers for each page table to only store valid PTE in the table. This will again split the virtual address to also contain the segment bits to identify which segment the address belongs to (code, heap or stack). Instead of using Base Page Table Register to query page table we will now be using Base Page Table Register [Segment] to get page table physical address for a given segment.\nSN = (VirtualAddress \u0026amp; SEG_MASK) \u0026gt;\u0026gt; SN_SHIFT VPN = (VirtualAddress \u0026amp; VPN_MASK) \u0026gt;\u0026gt; VPN_SHIFT AddressOfPTE = Base[SN] + (VPN * sizeof(PTE)) ^3c2fde\nThis way we can place contents of a process\u0026rsquo;s page table at different locations in memory for different segments and avoiding storing of invalid PTEs.\nMultilevel Page Table Segmented page table still can suffer from space wastage if we have sparse usage of heap and can cause external fragmentation since now size of a page table segment can be different (multiple of PTE size) and finding free space for variable sized page table can be difficult.\nIdea of Multilevel page table is to group page table entries into size of a page and ignore those group of PTE where each entry is invalid Like in above figure PFN 202 and 203 contain all entries as invalid and with multilevel page table we do not require to store PTE inside such pages. Now we would have an indirection where we will first refer page directory and then the actual page of the page table to get physical frame number of a given virtual address. So in a way we now have page table for the actual page table called page directory Lets assume we have 14 bit address space with 4 byte PTE with 64 Byte page size. VPN will be of 8 bits, which means a linear page table will contain 256 PTE each of size 4 byte resulting in 1KB page table (4 * 256). A 1KB page table requires 16 pages of size of 32 bytes so we can group this page table intro 16 different segments which will be now addressed by page table directory. To address 16 segments we need 4 bits so now we will be using first 4 bits of virtual address for page directory index.\nYou can see the similarity with segmentation here with the only difference being we are now creating segments of page size, so everything will be in multiple of page sizes so we won\u0026rsquo;t be facing external fragmentation + We do not need to know the number of segments beforehand like in segmentation with code, heap and stack. Similar to how we were calculating AddressOfPTE [[#^3c2fde]] in linear page table now we will first calculate AddressOfPDE (Page Directory Entry) as\nPDEAddr = PageDirBase + (PDIndex * sizeof(PDE)). Now we are storing page directory base address in register\nOnce we get PDE Address from there we can get the page address of required PTE. Similar to concatenating page offset in linear page table now we will be concatenating rest of the bits after page directory index on virtual address (Page Table Index or Page offset of page-table\u0026rsquo;s page) with PDEAddress to get the physical address of the page table entry.\nPTEAddr = (PDE.PFN \u0026lt;\u0026lt; SHIFT) + (PTIndex * sizeof(PTE)) Once we get the PTE address we can concatenate the physical address inside the entry with the page offset in virtual address to finally get the physical address of the given virtual address.\nSummary As you can see the tradeoff here is indirection to save memory space. Indirection basically means more number of memory access. For a given virtual address now we need following memory accesses:\nAccess memory to fetch page directory address Access memory to fetch page table entry address from page directory index Finally access memory for given overall virtual address As you can see with a TLB miss [[2- Source Materials/Books/OSTEP/TLB#^3bbded]] now we have 2 extra memory access overhead but after this miss we will have cache hit for this virtual address next time, and we will translate virtual address directly into physical address without any memory access. Overall A bigger table such as linear page table means faster service in case of TLB miss (lesser memory access) and reducing page table with indirection means slower TLB miss service. References https://pages.cs.wisc.edu/~remzi/OSTEP/vm-smalltables.pdf\n","permalink":"https://harshrai654.github.io/blogs/multilevel-page-table/","summary":"\u003ch1 id=\"segmented-page-table\"\u003eSegmented Page Table\u003c/h1\u003e\n\u003cp\u003ePage table can grow large for a 32-bit address space and 4 KB page size we will be using 20 bits for virtual page number resulting in 2^20 bytes (i.e. 4MB of page table) for a single page table and each process will have its own page table so it is possible that we will be storing ~100sMB for page table alone which is not good.\n\u003cimg alt=\"Pasted image 20241127093849.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241127093849.png\"\u003e\nFor above page table with 4 bits for VPN (Virtual page number) we can see that only VPN 0,4,14 and 15 are valid i.e. pointing to a PFN (Physical Frame Number) other PTEs (Page table entry) are just taking up space which is not used.\nWe can use segmentation here with base and bound registers for each page table to only store valid PTE in the table.\n\u003cimg alt=\"Pasted image 20241127094506.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241127094506.png\"\u003e\nThis will again split the virtual address to also contain the segment bits to identify which segment the address belongs to (code, heap or stack). Instead of using \u003cem\u003eBase Page Table Register\u003c/em\u003e to query page table we will now be using \u003cem\u003eBase Page Table Register [Segment]\u003c/em\u003e to get page table physical address for a given segment.\u003c/p\u003e","title":"Multilevel Page table"},{"content":"TLB Translation look-aside buffer is a CPU cache which is generally small but since it is closer to CPU a TLB hit results in address translation to happen in 1-5 CPU cycles.\nCPU Cycle Time taken by CPU to fully execute an instruction, while CPU frequency refers to the number of these cycles that occur per second\nA TLB hit means for given virtual address the physical frame number was found in the TLB cache. A TLB hit will benefit all the address that lie on the same page. In the above given image page size is 16 bytes, so 4 INT variables can be saved in a single page, so a TLB hit of VPN 07 will serve address translation for VPN = 07 + page of offset of 0, 4,8 and 12 byte. This type of caching is benefitted from spatial locality of data where a cache hit results in cache hits for surrounding data as well. If we cache data and other data points which are more probable to get accessed in the same time frame (like loop variables etc) then such caching is benefitted from Temporal locality.\nSoftware (OS) handled TLB miss ^3bbded\nWhen a TLB miss happens\nGenerate a trap Trap handler for this trap is OS code. This trap handler will find the translation of virtual address from page table stored in memory A privileged operation to update the TLB Return to trap with PC updated to same instruction which generated the trap. (Usually return to trap updates PC to next instruction address) Context Switch and TLB A naive approach is to flush the TLB (by setting valid bit of the page table entry to 0) so that the next yet to run process has a clean TLB, but this can slow things down since after every context switch new process will have few TLB misses, and we have high frequency of context switches then the performance may degrade. Another approach is to use same TLB for multiple processes with a Address space identifier (ASID, similar to PID but small) in the page table entry signifying the process to which a page table entry belongs Using cache replacement policies like LRU for page table entries References https://pages.cs.wisc.edu/~remzi/OSTEP/vm-tlbs.pdf\n","permalink":"https://harshrai654.github.io/blogs/tlb/","summary":"\u003ch1 id=\"tlb\"\u003eTLB\u003c/h1\u003e\n\u003cp\u003eTranslation look-aside buffer is a CPU cache which is generally small but since it is closer to CPU a TLB hit results in address translation to happen in 1-5 CPU cycles.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCPU Cycle\nTime taken by CPU to fully execute an instruction, while CPU frequency refers to the number of these cycles that occur per second\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eA TLB hit means for given virtual address the physical frame number was found in the TLB cache. A TLB hit will benefit all the address that lie on the same page.\n\u003cimg alt=\"Pasted image 20241120223520.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241120223520.png\"\u003e\nIn the above given image page size is 16 bytes, so 4 INT variables can be saved in a single page, so a TLB hit of VPN 07 will serve address translation for VPN = 07 + page of offset of 0, 4,8 and 12 byte.\nThis type of caching is benefitted from \u003cem\u003e\u003cstrong\u003espatial locality\u003c/strong\u003e\u003c/em\u003e of data where a cache hit results in cache hits for surrounding data as well.\nIf we cache data and other data points which are more probable to get accessed in the same time frame (like loop variables etc) then such caching is benefitted from \u003cem\u003e\u003cstrong\u003eTemporal locality.\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e","title":"TLB"},{"content":"Page Tables Page table contains the translation information of virtual page number to physical frame number. For an address space of 32 bits and page size of 4 KB (i.e. memory of 2^32 is divided into segments of 4 KB where each segment is called a memory page) , The virtual address will be of size 32 bits of which 12 bits (2^12 = 4 KB) will be used as offset inside a single page whereas remaining 20 bits will be used as virtual page number\nExample Memory Access As it can be seen from the above memory access flow, translating and accessing a virtual page address to actual physical frame address requires 2 memory access which can be slow when the process assumed that it is making only one memory access to fetch data or instruction from memory.\nPage table size is directly proportional to address space size Page table size is inversely proportional to a page size References OSTEP\n","permalink":"https://harshrai654.github.io/blogs/page-tables/","summary":"\u003ch1 id=\"page-tables\"\u003ePage Tables\u003c/h1\u003e\n\u003cp\u003ePage table contains the translation information of virtual page number to physical frame number.\nFor an address space of 32 bits and page size of 4 KB \u003cem\u003e(i.e. memory of 2^32 is divided into segments of 4 KB where each segment is called a memory page)\u003c/em\u003e , The virtual address will be of size 32 bits of which 12 bits (2^12 = 4 KB) will be used as offset inside a single page whereas remaining 20 bits will be used as virtual page number\u003c/p\u003e","title":"Page Tables"},{"content":"References 5.6 Problem Generally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:\nIf the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading If the operation is a write operation then there are again two possibilities: Optimistic Locking If the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it\u0026rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify ^ab3c53 Pessimistic Locking But if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking Pessimistic locking involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf\u0026rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff\nNow there is a glaring problem with pessimistic locking, even if the restructuring is limited to the leaf node and its direct parent or neighbours only then also we are taking a write latch on the root restricting potential read operations resulting in slow reads\n8.0 Solution Introduction to SX Lock (Write lock is called X lock - Exclusive lock | Read lock is called S lock - Shared lock)\n- SX LOCK does not conflict with S LOCK but does conflict with X LOCK. SX Locks also conflict with each other. - The purpose of an SX LOCK is to indicate the intention to modify the protected area, but the modification has not yet started. Therefore, the resource is still accessible, but once the modification begins, access will no longer be allowed. Since an intention to modify exists, no other modifications can occur, so it conflicts with X LOCKs.\nSX locks are kind of like X lock among themselves but are like S locks when used with S locks, Now with SX locks are held in following manner.\nREAD OPERATION: We take S lock as before on the root node, but we also take S locks on the internal nodes till the leaf nodes WRITE OPERATION: If the write operation is just going to modify the leaf page we still use Optimistic Locking [[#^ab3c53]] But if the write operation involves restructuring of the tree then instead of taking a write lock on the root as discussed in Pessimistic locking [[#^17a3ff]] We take SX lock on the root and same X locks on leaf node and its direct parent and neighbour, This still allows S locks to be taken in root node to allow other read operation which are not having the same path as the ongoing write operation But still prevents another SX lock on root node by some other write operation. So we can see that SX lock\u0026rsquo;s introduction helps in increasing the read throughput but still has the problem of global contention on write operation even if the writes are not going to happen in the ongoing another write operation. I think a root level latch is under the pessimistic guess of modification of write bubbling up to root which can conflict with another write operation even if not in the same path as bubbling up to the root will impact all the branches, butt the question is do all write operations bubble up to root and if not is it wise to take a root level of SX lock also to prevent other write operations. The answer lies in another type of lock mechanism called Latch Coupling or Latch Crabbing.\nReferences Inno DB B-Tree Latch Optimisation\n","permalink":"https://harshrai654.github.io/blogs/b-tree-latch-optimisation/","summary":"\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\u003ch2 id=\"56-problem\"\u003e5.6 Problem\u003c/h2\u003e\n\u003cp\u003eGenerally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading\u003c/li\u003e\n\u003cli\u003eIf the operation is a write operation then there are again two possibilities:\n\u003cul\u003e\n\u003cli\u003e\n\u003ch3 id=\"optimistic-locking\"\u003eOptimistic Locking\u003c/h3\u003e\nIf the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it\u0026rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify\n\u003cimg alt=\"Pasted image 20241117123300.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241117123300.png\"\u003e ^ab3c53\u003c/li\u003e\n\u003cli\u003e\n\u003ch3 id=\"pessimistic-locking\"\u003ePessimistic Locking\u003c/h3\u003e\nBut if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking\n\u003cimg alt=\"Pasted image 20241117123407.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241117123407.png\"\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003ePessimistic locking\u003c/strong\u003e involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf\u0026rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff\u003c/p\u003e","title":"B-Tree Latch Optimisation"},{"content":"This is a test post to verify Mermaid diagram rendering in both light and dark modes.\nSequence Diagram sequenceDiagram participant Alice participant Bob Alice-\u0026gt;\u0026gt;Bob: Hello Bob, how are you? Bob--\u0026gt;\u0026gt;Alice: Great! Alice-\u0026gt;\u0026gt;Bob: See you later! Flowchart graph TD A[Start] --\u0026gt; B{Is it?} B --\u0026gt;|Yes| C[OK] C --\u0026gt; D[Rethink] D --\u0026gt; B B ----\u0026gt;|No| E[End] Gantt Chart gantt title A Gantt Diagram dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d State Diagram stateDiagram-v2 [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] Class Diagram classDiagram class Animal { +String name +int age +makeSound() } class Dog { +String breed +bark() } class Cat { +String color +meow() } Animal \u0026lt;|-- Dog Animal \u0026lt;|-- Cat ","permalink":"https://harshrai654.github.io/blogs/mermaid-dark-test/","summary":"\u003cp\u003eThis is a test post to verify Mermaid diagram rendering in both light and dark modes.\u003c/p\u003e\n\u003ch2 id=\"sequence-diagram\"\u003eSequence Diagram\u003c/h2\u003e\n\u003cpre class=\"mermaid\"\u003e\n  sequenceDiagram\n    participant Alice\n    participant Bob\n    Alice-\u0026gt;\u0026gt;Bob: Hello Bob, how are you?\n    Bob--\u0026gt;\u0026gt;Alice: Great!\n    Alice-\u0026gt;\u0026gt;Bob: See you later!\n\u003c/pre\u003e\n\u003ch2 id=\"flowchart\"\u003eFlowchart\u003c/h2\u003e\n\u003cpre class=\"mermaid\"\u003e\n  graph TD\n    A[Start] --\u0026gt; B{Is it?}\n    B --\u0026gt;|Yes| C[OK]\n    C --\u0026gt; D[Rethink]\n    D --\u0026gt; B\n    B ----\u0026gt;|No| E[End]\n\u003c/pre\u003e\n\u003ch2 id=\"gantt-chart\"\u003eGantt Chart\u003c/h2\u003e\n\u003cpre class=\"mermaid\"\u003e\n  gantt\n    title A Gantt Diagram\n    dateFormat  YYYY-MM-DD\n    section Section\n    A task           :a1, 2014-01-01, 30d\n    Another task     :after a1  , 20d\n    section Another\n    Task in sec      :2014-01-12  , 12d\n    another task      : 24d\n\u003c/pre\u003e\n\u003ch2 id=\"state-diagram\"\u003eState Diagram\u003c/h2\u003e\n\u003cpre class=\"mermaid\"\u003e\n  stateDiagram-v2\n    [*] --\u0026gt; Still\n    Still --\u0026gt; [*]\n    Still --\u0026gt; Moving\n    Moving --\u0026gt; Still\n    Moving --\u0026gt; Crash\n    Crash --\u0026gt; [*]\n\u003c/pre\u003e\n\u003ch2 id=\"class-diagram\"\u003eClass Diagram\u003c/h2\u003e\n\u003cpre class=\"mermaid\"\u003e\n  classDiagram\n    class Animal {\n        +String name\n        +int age\n        +makeSound()\n    }\n    class Dog {\n        +String breed\n        +bark()\n    }\n    class Cat {\n        +String color\n        +meow()\n    }\n    Animal \u0026lt;|-- Dog\n    Animal \u0026lt;|-- Cat\n\u003c/pre\u003e","title":"Mermaid Dark Mode Test"},{"content":"","permalink":"https://harshrai654.github.io/blogs/mermaid-test/","summary":"","title":""}]