[{"content":"This article shares learnings from Google\u0026rsquo;s influential MapReduce paper and explores the challenges encountered while implementing a simplified version. Our system uses multiple worker processes, running on a single machine and communicating via RPC, to mimic key aspects of a distributed environment.\nWhat is Map-Reduce At its core, MapReduce is a programming model and an associated framework for processing and generating massive datasets using a parallel, distributed algorithm, typically on a cluster of computers. You might already be familiar with map and reduce operations from functional programming languages. For instance, in JavaScript, array.map() transforms each element of an array independently based on a mapper function, while array.reduce() iterates through an array, applying a reducer function to accumulate its elements into a single output value (e.g., a sum, or a new, aggregated object).\nThe MapReduce paradigm, brilliantly scales these fundamental concepts to tackle data processing challenges that are orders of magnitude larger than what a single machine can handle. The general flow typically involves several key stages:\nSplitting: The vast input dataset is initially divided into smaller, independent chunks. Each chunk will be processed by a Map task.\nMap Phase: A user-defined Map function is applied to each input chunk in parallel across many worker machines. The Map function takes an input pair (e.g., a document ID and its content) and produces a set of intermediate key/value pairs. For example, in a word count application, a Map function might take a line of text and output a key/value pair for each word, like (word, 1).\nShuffle and Sort Phase: This is a critical intermediate step. The framework gathers all intermediate key/value pairs produced by the Map tasks, sorts them by key, and groups together all values associated with the same intermediate key. This ensures that all occurrences of (word, 1) for a specific \u0026lsquo;word\u0026rsquo; are brought to the same place for the next phase.\nReduce Phase: A user-defined Reduce function then processes the grouped data for each unique key, also in parallel. The Reduce function takes an intermediate key and a list of all values associated with that key. It iterates through these values to produce a final output, often zero or one output value. Continuing the word count example, the Reduce function for a given word would receive (word, [1, 1, 1, \u0026hellip;]) and sum these ones to produce the total count, e.g., (word, total_count).\nThis distributed approach is highly effective for several reasons:\nScalability: It allows for horizontal scaling, you can process more data faster by simply adding more machines to your cluster.\nParallelism: It inherently parallelizes computation, significantly speeding up processing times for large tasks.\nFault Tolerance: The MapReduce framework is designed to handle machine failures automatically by re-executing failed tasks, which is crucial when working with large clusters where failures are common.\nThis model simplifies large-scale data processing by abstracting away the complexities of distributed programming, such as data distribution, parallelization, and fault tolerance, allowing developers to focus on the logic of their Map and Reduce functions.\nThe MapReduce Execution Flow To understand how MapReduce processes vast amounts of data, let\u0026rsquo;s walk through the typical execution flow, as illustrated in the Google paper and its accompanying diagram (Figure 1 from the paper, shown below). This flow is orchestrated by a central Master (or Coordinator, as in our lab implementation) and executed by multiple Worker processes.\nHere\u0026rsquo;s a breakdown of the key stages:\nInitialization \u0026amp; Input Splitting (Diagram: User Program forks Master, Input files split):\nThe MapReduce library first divides the input files into M smaller, manageable pieces called splits (e.g., split 0 to split 4 in the diagram). Each split is typically 16-64MB. The User Program then starts multiple copies of the program on a cluster. One copy becomes the Master, and the others become Workers. Here the binary contains logic for master and worker. Task Assignment by Master (Diagram: Master assigns map/reduce to workers):\nThe Master is the central coordinator. It\u0026rsquo;s responsible for assigning tasks to idle workers. There are M map tasks (one for each input split) and R reduce tasks (a number chosen by the user for the desired level of output parallelism). Map Phase - Processing Input Splits (Diagram: worker (3) reads split, (4) local write):\nA worker assigned a map task reads the content of its designated input split (e.g., split 2). It parses key/value pairs from this input data. For each pair, it executes the user-defined Map function. The Map function emits intermediate key/value pairs. These intermediate pairs are initially buffered in the worker\u0026rsquo;s memory. Periodically, they are written to the worker\u0026rsquo;s local disk. Crucially, these locally written intermediate files are partitioned into R regions/files (one region/file for each eventual reduce task). This is typically done using a partitioning function (e.g., hash(intermediate_key) % R). The locations of these R partitioned files on the local disk (shown as \u0026ldquo;Intermediate files (on local disks)\u0026rdquo; in the diagram) are then reported back to the Master. The Master now knows where the intermediate data for each reduce task partition resides, spread across possibly many map workers. Reduce Phase - Aggregating Intermediate Data (Diagram: worker (5) remote read, (6) write output):\nOnce the Master sees that map tasks are completing, it begins assigning reduce tasks to other (or the same) workers. When a reduce worker is assigned a partition (say, partition j out of R), the Master provides it with the locations of all the relevant intermediate files (i.e., the j-th region/file from all map workers that produced j-th intermediate file). The reduce worker then performs remote reads from the local disks of the map workers to fetch this buffered intermediate data. After retrieving all necessary intermediate data for its assigned partition, the reduce worker sorts these key/value pairs by the intermediate key. This groups all occurrences of the same key together. (If data is too large for memory, an external sort is used). The worker then iterates through the sorted data. For each unique intermediate key, it calls the user-defined Reduce function, passing the key and the list of all associated intermediate values. The output of the Reduce function is appended to a final output file for that specific reduce partition (e.g., output file 0, output file 1). There will be R such output files. Job Completion:\nWhen all M map tasks and R reduce tasks have successfully completed, the Master signals the original User Program. The MapReduce call in the user code returns, and the results are available in the R output files. Key Design Decisions:\nAbstraction: Developers focus on Map and Reduce logic, while the framework manages distributed complexities like data partitioning, parallel execution, and shuffling. Inherent Fault Tolerance: The system is designed for resilience against common failures: The Master detects worker failures. If a worker assigned a map task fails, the task is re-assigned because its input split is durable. More subtly, if a worker completes a map task (producing intermediate files on its local disk) but then fails before all necessary reduce tasks have read those intermediate files, those files are lost. The Master must then reschedule that original map task on another worker to regenerate its intermediate output. If a worker assigned a reduce task fails, that reduce task can be re-executed by another worker. However, once a reduce task completes successfully and writes its final output (e.g., to mr-out-X), that output is considered final. The system aims to avoid re-executing successfully completed reduce tasks, relying on the durability of their output. One important aspect to note is that intermediate files are stored on the local file system of the worker nodes that produce them. This design choice is deliberate: by keeping intermediate data local, the system significantly reduces network bandwidth consumption and potential network congestion that would arise if all intermediate data had to be written to, and read from, a global file system. However, this means that crashes in map worker nodes can result in the loss of their locally stored intermediate data, necessitating the re-execution of those map tasks.\nIn contrast, the final outputs of worker processes executing the reduce operation are typically written to a global, distributed file system (like GFS in Google\u0026rsquo;s case). Once a reduce task successfully writes its output to this global system, it\u0026rsquo;s considered durable and generally does not need to be re-executed, even if the worker that produced it later fails.\nImplementing MapReduce in Go: The Coordinator and Worker The Go implementation translates the conceptual MapReduce master-worker architecture into two main programs: a Coordinator and multiple Worker processes, communicating via RPC. We\u0026rsquo;ll explore the key parts of their implementation, starting with the Coordinator.\nThe Coordinator (mr/coordinator.go) The Coordinator is the central manager of the MapReduce job. Its primary role is to distribute tasks to workers, track their progress, handle failures, and determine when the overall job is complete.\nInitialization (MakeCoordinator) The MakeCoordinator function initializes the Coordinator\u0026rsquo;s state. It\u0026rsquo;s called by main/mrcoordinator.go with the input files and the number of reduce tasks (nReduce). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 // MakeCoordinator is called by main/mrcoordinator.go to create and initialize // the coordinator for a MapReduce job. // - files: A slice of input file paths for the map tasks. // - nReduce: The desired number of reduce tasks. func MakeCoordinator(files []string, nReduce int) *Coordinator { // Step 1: Initialize the list of ready Map tasks. // NewTaskList() creates a new instance of TaskList (wrapper around container/list). readyTaskList := NewTaskList() // For each input file, a Map task is created. for index, file := range files { readyTaskList.AddTask(\u0026amp;Task{ // Task struct holds details for a single map or reduce operation. Filename: file, // Input file for this map task. Status: StatusReady, // Initial status: ready to be assigned. Type: MapType, // Task type is Map. Id: TaskId(fmt.Sprintf(\u0026#34;m-%d\u0026#34;, index)), // Unique ID for the map task (e.g., \u0026#34;m-0\u0026#34;). }) } // Step 2: Initialize the Coordinator struct with its core state variables. c := Coordinator{ // --- Task Tracking --- // readyTasks: Holds tasks (initially all Map tasks, later Reduce tasks) that are // waiting to be assigned to a worker. // Managed by GetTask (removes) and ReportTask/checkWorkerStatus (adds back on failure). readyTasks: *readyTaskList, // runningTasks: A map from TaskId to *RunningTask. Tracks tasks currently assigned // to one or more workers. A RunningTask includes the Task details and a // list of WorkerIds processing it. // Managed by GetTask (adds) and ReportTask/checkWorkerStatus (modifies/removes). runningTasks: make(map[TaskId]*RunningTask), // successTasks: A map from TaskId to *Task. Stores tasks that have been successfully // completed by a worker. // Managed by ReportTask (adds on success). successTasks: make(map[TaskId]*Task), // --- Job Parameters \u0026amp; Phase Control --- // nReduce: The target number of reduce partitions/tasks for the job. // Used by Map workers to partition intermediate data and by the Coordinator // to determine when all reduce tasks are done. nReduce: nReduce, // nMap: The total number of map tasks, simply the count of input files. // Used to determine when all map tasks are done. nMap: len(files), // pendingMappers: A counter for map tasks that are not yet successfully completed. // Crucially used in GetTask to gate the start of Reduce tasks – // Reduce tasks cannot begin until pendingMappers is 0. // Decremented in ReportTask upon successful map task completion. pendingMappers: len(files), // --- Intermediate Data Management --- // intermediateFiles: An IntermediateFileMap (map[string]map[WorkerId][]string). // This is vital: maps a partition key (string, for a reduce task) // to another map. This inner map links a WorkerId (of a map worker) // to a list of filenames (intermediate files produced by that map worker // for that specific partition key). // Populated in ReportTask when a Map task succeeds. // Read by GetTask to provide Reduce workers with their input locations. intermediateFiles: make(IntermediateFileMap), // --- Worker Tracking --- // workers: A map from WorkerId to *WorkerMetdata. Stores metadata about each worker // that has interacted with the coordinator. WorkerMetdata includes: // - lastHeartBeat: Time of the worker\u0026#39;s last contact, used by checkWorkerStatus for timeouts. // - runningTask: TaskId of the task currently assigned to this worker. // - successfulTasks: A map of tasks this worker has completed (useful for debugging/optimizations, not strictly essential for basic fault tolerance in this lab\u0026#39;s context if tasks are just re-run). // Populated/updated in GetTask and ReportTask. workers: make(map[WorkerId]*WorkerMetdata), // --- Coordinator Shutdown \u0026amp; Job Completion Signaling --- // finished: A boolean flag set to true when all map and reduce tasks are successfully // completed (checked in ReportTask). Signals the main job is done. finished: false, // done: A channel of empty structs (chan struct{}). Used to signal background goroutines // (like checkWorkerStatus) to terminate gracefully when the job is `finished`. // Closed in the Done() method. done: make(chan struct{}), // shutdownSignaled: A boolean flag, true after `done` channel is closed. Prevents // multiple closures or redundant shutdown logic. shutdownSignaled: false, // allGoroutinesDone: A boolean flag, true after `wg.Wait()` in `Done()` confirms all // background goroutines have exited. allGoroutinesDone: false, // wg (sync.WaitGroup): Used in conjunction with `done` to wait for background goroutines // to complete their cleanup before the Coordinator fully exits. // Incremented before launching a goroutine, Done called in goroutine\u0026#39;s defer. // (wg is part of the Coordinator struct, initialized implicitly here) } fmt.Printf(\u0026#34;Initialised ready tasklist of %d tasks\\n\u0026#34;, len(files)) // Step 3: Start Services // Start the RPC server so the coordinator can listen for requests from workers. // This makes methods like GetTask and ReportTask callable by workers. c.server() // Step 4: Launch Background Health Checker Goroutine // This goroutine is responsible for fault tolerance, specifically detecting // and handling timed-out (presumed crashed) workers. c.wg.Add(1) // Increment WaitGroup counter before launching the goroutine. go func() { defer c.wg.Done() // Decrement counter when the goroutine exits. for { select { case \u0026lt;-c.done: // Listen for the shutdown signal from the main coordinator logic. fmt.Printf(\u0026#34;[Coordinator Shutdown]: Closing worker health check background thread.\\n\u0026#34;) return // Exit the goroutine. default: // Periodically call checkWorkerStatus to handle unresponsive workers. c.checkWorkerStatus() // WORKER_TIMEOUT_SECONDS is 10s, so this checks every 5s. time.Sleep(WORKER_TIMEOUT_SECONDS / 2) } } }() return \u0026amp;c // Return the initialized Coordinator instance. } Initially, M map tasks are created (one for each input file) and added to readyTasks. Contrary to the paper we can only run reduce tasks only when all mapper tasks are finished as input for a reduce task may require intermediate file output(s) from more than one map task since a map task produces at max R intermediate partition files, each designated to one reduce task and reduce workers needs to fetch these intermediate files from each of the mapper worker\u0026rsquo;s local file system. An RPC server (c.server()) is started for worker communication, and a background goroutine (checkWorkerStatus) is launched for fault tolerance. All shared state within the Coordinator (e.g., task lists, worker metadata) must be protected by mutexes (as seen in its methods like GetTask, ReportTask) since the shared state can be accessed by multiple go routines handling RPC calls from various worker processed which may lead to race conditions. Assigning Tasks to Workers (GetTask RPC Handler) Workers call the GetTask RPC handler to request jobs (either Map or Reduce tasks) from the Coordinator. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 // An RPC handler to find next available task (map or reduce) func (c *Coordinator) GetTask(args *GetTaskArgs, reply *GetTaskReply) error { c.mu.Lock() defer c.mu.Unlock() workerMetadata, ok := c.workers[args.WorkerId] // Requesting worker already processing a task // Skip task assignment if ok \u0026amp;\u0026amp; workerMetadata.runningTask != \u0026#34;\u0026#34; { fmt.Printf(\u0026#34;[GetTask]: Worker %d already processing task %s, rejecting task assignment request.\\n\u0026#34;, args.WorkerId, workerMetadata.runningTask) return nil } if c.readyTasks.GetTaskCount() == 0 { // No tasks available // map reduce is complete if we also have len(runningTasks) == 0 // Sending InvalidType task in such cases to worker reply.Task = Task{ Type: InvalidType, } return nil } task := c.readyTasks.RemoveTask() // Skipping tasks that are possible retrials with an instance already completed and part of success set // It is possible that a task here already has a status of `StatusRunning` we are not skipping such tasks in ready queue // This will result in multiple instances of same task execution, This case is possible if previous worker processing the task // failed/crashed (timeout of not reporting reached) and we added another instance of the same task. // Even if two workers report completion of same task only one of them will remove the task from running queue and add it to // success set, Reporting by slower worker will be skipped. // Only assing a reduce task when we are sure there is no pending map task left // Since then reduce task will surely fail because of unavailabiltiy of intermeidate fiel data for task != nil { if task.Status == StatusSuccess || (task.Type == ReduceType \u0026amp;\u0026amp; c.pendingMappers \u0026gt; 0) { if task.Status == StatusSuccess { fmt.Printf(\u0026#34;[GetTask]: Skipping ready task %s since it is already successfully completed\\n\u0026#34;, task.Id) } else { fmt.Printf(\u0026#34;[GetTask]: Skipping reduce task %s since there are %d pending mappers\\n\u0026#34;, task.Id, c.pendingMappers) } task = c.readyTasks.RemoveTask() } else { break } } // Either all tasks are completed (if len(runningTasks) == 0) // OR all tasks are currently being processed by some workers if task == nil { reply.Task = Task{ Type: InvalidType, } fmt.Printf(\u0026#34;[GetTask]: No task to assign to worker %d, # Tasks Running : %d, # Tasks Completed: %d\\n\u0026#34;, args.WorkerId, len(c.runningTasks), len(c.successTasks)) return nil } fmt.Printf(\u0026#34;[GetTask]: Found a task with id %s for worker %d. Current Task Status: %v\\n\u0026#34;, task.Id, args.WorkerId, task.Status) // Found a task with Status as either `StatusError` or `StatusReady` or `StatusRunning` // If task\u0026#39;s status is: `StatusError`` -\u0026gt; Retrying failed task again // If task\u0026#39;s status is `StatusReady` -\u0026gt; First Attempt of processing of task // If task\u0026#39;s status is `StatusRunning` -\u0026gt; Retrying already running task due to delay from previous assigned worker task.Worker = args.WorkerId task.StartTime = time.Now() task.Status = StatusRunning reply.Task = *task if task.Type == ReduceType { // Add intermediate file locations collected from various map executions reply.IntermediateFiles = c.intermediateFiles[task.Filename] } reply.NR = c.nReduce // Update list of workers currently processing a taskId rt := c.runningTasks[task.Id] if rt == nil { c.runningTasks[task.Id] = \u0026amp;RunningTask{} } c.runningTasks[task.Id].task = task c.runningTasks[task.Id].workers = append(c.runningTasks[task.Id].workers, args.WorkerId) if workerMetadata != nil { workerMetadata.lastHeartBeat = time.Now() workerMetadata.runningTask = task.Id } else { c.workers[args.WorkerId] = \u0026amp;WorkerMetdata{ lastHeartBeat: time.Now(), runningTask: task.Id, successfulTasks: make(map[TaskId]*TaskOutput), } } return nil } As defined in the paper\u0026rsquo;s step-2 of the execution flow this method is called by various workers to request task which are in readyTasks. It deals with scenarios like workers already being busy, no tasks being available, or tasks being unsuitable for immediate assignment (e.g., reduce tasks when mappers are still active). If a valid task is found all necessary details to execute that task are populated in GetTaskReply struct. For Map tasks, it implicitly provides the input file (via task.Filename). For Reduce tasks, it explicitly provides the locations of all relevant intermediate files and the total number of reducers (NR). Handling Task Completion/Failure (ReportTask RPC Handler) Workers call ReportTask to inform the coordinator about the outcome of their assigned task. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 func (c *Coordinator) ReportTask(args *ReportTaskArgs, reply *ReportTaskReply) error { c.mu.Lock() defer c.mu.Unlock() reply.Status = true // optimistic reply taskSuccessInstance := c.successTasks[args.Task.Id] // ... (validation: check if task already completed, if worker owns task) ... // Reported task is already in success set. // Possibly retried after timeout by another worker // One of the worker complted the task. if taskSuccessInstance != nil { fmt.Printf(\u0026#34;[ReportTask]: Task %s has already been completed by worker %d\\n\u0026#34;, taskSuccessInstance.Id, taskSuccessInstance.Worker) // ... (update worker metadata) ... return nil } // ... (check if worker lost ownership of the task) ... if args.Task.Status == StatusError { fmt.Printf(\u0026#34;[ReportTask]: Task %s reported with status %v by worker %d\\n\u0026#34;, args.Task.Id, args.Task.Status, args.Task.Worker) // ... (disown worker from task) ... // If no other worker is processing this task, add it back to readyTasks if len(c.runningTasks[args.Task.Id].workers) == 0 { task := args.Task task.Worker = 0 task.StartTime = time.Time{} task.Status = StatusReady c.readyTasks.AddTask(\u0026amp;task) } return nil } if args.Task.Status == StatusSuccess { switch args.Task.Type { case MapType: intermediateFiles := args.Task.Output fmt.Printf(\u0026#34;[ReportTask]: Mapper Task %s completed successfully by worker %d, produced following intermediate files: %v\\n\u0026#34;, args.Task.Id, args.Task.Worker, intermediateFiles) for _, filename := range intermediateFiles { partitionKey := strings.Split(filename, \u0026#34;-\u0026#34;)[4] // Assumes filename format like w-\u0026lt;workerId\u0026gt;/mr-m-\u0026lt;taskId\u0026gt;-\u0026lt;hash\u0026gt; paritionFiles, ok := c.intermediateFiles[partitionKey] if !ok || paritionFiles == nil { paritionFiles = make(map[WorkerId][]string) } paritionFiles[args.Task.Worker] = append(paritionFiles[args.Task.Worker], filename) c.intermediateFiles[partitionKey] = paritionFiles } // ... (update worker metadata, move task to successTasks, decrement pendingMappers) ... delete(c.runningTasks, args.Task.Id) c.successTasks[args.Task.Id] = \u0026amp;args.Task c.pendingMappers-- if c.pendingMappers == 0 { fmt.Printf(\u0026#34;\\nAll map task ran successfully. Tasks Run Details: \\n %v \\n\u0026#34;, c.successTasks) c.addReduceTasks() // Trigger creation of reduce tasks } case ReduceType: // ... (update worker metadata, move task to successTasks) ... delete(c.runningTasks, args.Task.Id) c.successTasks[args.Task.Id] = \u0026amp;args.Task if len(c.successTasks) == c.nMap+c.nReduce { fmt.Printf(\u0026#34;\\nAll reduce tasks ran successfully. Tasks Run Details: \\n %v \\n\u0026#34;, c.successTasks) c.finished = true // Mark entire job as done } default: // ... } // ... (logging) ... } return nil } // ... (helper function addReduceTasks) func (c *Coordinator) addReduceTasks() { index := 0 for partition, v := range c.intermediateFiles { // Iterate over collected partitions task := Task{ Status: StatusReady, Type: ReduceType, Id: TaskId(fmt.Sprintf(\u0026#34;r-%d\u0026#34;, index)), Filename: partition, // Partition key becomes the \u0026#39;filename\u0026#39; for the reduce task } if c.successTasks[task.Id] == nil { // Avoid re-adding if already processed (e.g. due to retries) c.readyTasks.AddTask(\u0026amp;task) fmt.Printf(\u0026#34;Reduce Task with Id %s Added to ready queue (Intermediate partition %s with %d files)\\n\u0026#34;, task.Id, partition, len(v)) } index++ } c.nReduce = index // Update nReduce to actual number of partitions, good for robustness } If a task is reported with StatusError, and it\u0026rsquo;s the only instance of that task running, it\u0026rsquo;s re-queued for a later attempt. This is a core part of fault tolerance. Processes Successful Map Tasks: Collects and organizes the locations of intermediate files (output of Map functions) based on their partition key. This information (c.intermediateFiles) is vital for the subsequent Reduce phase, as it tells Reduce workers where to fetch their input data. This aligns with step 4 of the paper\u0026rsquo;s flow. Decrements pendingMappers. When all mappers are done, it triggers addReduceTasks. Once all Map tasks are complete, addReduceTasks is called. It iterates through all the unique partition keys derived from the intermediate files and creates one Reduce task for each, adding them to the readyTasks queue. Processes Successful Reduce Tasks: Marks the reduce task as complete. Checks if all Map and Reduce tasks for the entire job are finished. If so, it sets c.finished = true, signaling that the Coordinator can begin shutting down. By checking c.successTasks at the beginning, it avoids reprocessing reports for tasks already marked as successful, which helps in scenarios with duplicate or late messages. Fault Tolerance (checkWorkerStatus) A background goroutine periodically checks for unresponsive workers. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 func (c *Coordinator) checkWorkerStatus() { c.mu.Lock() defer c.mu.Unlock() for workerId, metadata := range c.workers { lastHeartBeatDuration := time.Since(metadata.lastHeartBeat) if metadata.runningTask != \u0026#34;\u0026#34; \u0026amp;\u0026amp; lastHeartBeatDuration \u0026gt;= WORKER_TIMEOUT_SECONDS { fmt.Printf(\u0026#34;Worker %d have not reported in last %s\\n\u0026#34;, workerId, lastHeartBeatDuration) taskToRetry := make([]*Task, 0) runningTask := c.runningTasks[metadata.runningTask] if runningTask == nil { // This case should ideally not happen if state is consistent fmt.Printf(\u0026#34;[checkWorkerStatus]: Local worker state shows worker %d running rask %s whereas global running tasks state does not show any worker for the same task.\\n\u0026#34;, workerId, metadata.runningTask) // Potentially clear worker\u0026#39;s running task if inconsistent: metadata.runningTask = \u0026#34;\u0026#34; continue // or return, depending on desired error handling } taskToRetry = append(taskToRetry, runningTask.task) metadata.runningTask = \u0026#34;\u0026#34; // Worker is no longer considered running this task // Remove this worker from the list of workers for the task runningTask.workers = slices.DeleteFunc(runningTask.workers, func(w WorkerId) bool { return w == workerId }) // If this was the only worker on this task, or if we want to aggressively reschedule // (The current code implies rescheduling if *any* assigned worker times out, which is fine for this lab) if len(taskToRetry) \u0026gt; 0 { // Will always be true if runningTask was not nil for _, task := range taskToRetry { fmt.Printf(\u0026#34;[checkWorkerStatus]: Adding task %s of type %d with status %d back to the ready queue.\\n\u0026#34;, task.Id, task.Type, task.Status) // Reset task for retry task.Status = StatusReady task.Worker = 0 // Clear previous worker assignment task.Output = make([]string, 0) // Clear previous output c.readyTasks.AddTask(task) } } } // ... (logging for active workers) ... } }\tKey Decisions Upon Detecting a Failed Worker:\nIdentify the Affected Task: The primary task to consider is metadata.runningTask, which the failed worker was supposed to be executing. The details of this task are retrieved from c.runningTasks. Update Worker\u0026rsquo;s State: The failed worker\u0026rsquo;s metadata.runningTask is cleared, indicating it\u0026rsquo;s no longer considered to be working on that task by the coordinator. Update Task\u0026rsquo;s Worker List: The failed workerId is removed from the runningTaskEntry.workers list, which tracks all workers assigned to that specific task ID. Reset Task for Re-execution: The affected taskInstanceToRetry undergoes several state changes: Status is set back to StatusReady, making it available in the c.readyTasks queue. Worker (assigned worker ID) is cleared. StartTime is reset. Output (list of output files) is cleared, as any partial output is now suspect or irrelevant. Re-queue the Task: The reset task is added back to c.readyTasks.AddTask(task). This ensures another worker can pick it up. Handling Lost Intermediate Data (Implicitly via Task Re-execution): A critical aspect of fault tolerance in MapReduce, as highlighted by the paper, is managing the intermediate files produced by map tasks. These are typically stored on the local disks of the map workers. If a map worker completes its task successfully but then crashes before all relevant reduce tasks have consumed its intermediate output, those intermediate files are lost. Our current checkWorkerStatus implementation primarily focuses on retrying the actively running task of a worker that times out.\n1 2 3 4 5 6 7 // In checkWorkerStatus, when a worker (workerId) times out: // ... runningTask := c.runningTasks[metadata.runningTask] // ... taskToRetry = append(taskToRetry, runningTask.task) // The currently running task is added for retry metadata.runningTask = \u0026#34;\u0026#34; // ... task is reset and added back to c.readyTasks ... This handles cases where a worker fails mid-task. But what about its previously completed map tasks whose outputs are now gone?\nThe Challenge of Retrying Previously Successful Map Tasks One might initially think that upon a worker\u0026rsquo;s crash, we should re-queue all map tasks that worker had successfully completed. The following (commented-out) snippet from an earlier version of checkWorkerStatus attempted this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Original (commented-out) consideration for retrying all successful map tasks of a crashed worker: // Adding successful map tasks of this worker for retrial for taskId, _ := range metadata.successfulTasks { if c.successTasks[taskId] != nil \u0026amp;\u0026amp; c.successTasks[taskId].Type == MapType { // If this task was indeed in the global success set and was a MapType: taskToRetry = append(taskToRetry, c.successTasks[taskId]) // Add it for retrial delete(c.successTasks, taskId) // Remove from global success set // CRITICAL: We would also need to increment c.pendingMappers here if it had been decremented c.pendingMappers++ } } // Tombstoning metadata of intermediate files produced by this worker // From global state so that downstream reduce workers get to know about the failure. // This would ideally cause reduce tasks that depend on this worker\u0026#39;s output to fail // or wait, and get re-added after the map tasks are re-run. for _, v := range c.intermediateFiles { // Mark intermediate files from this worker (workerId) as unavailable/invalid. delete(v, workerId) // Or v[workerId] = nil if the structure supports it } When a map worker crashes after successfully writing its intermediate files, those files (on its local disk) are lost in a true distributed system. Our lab setup, where all workers share the host\u0026rsquo;s filesystem, can sometimes mask this; a \u0026lsquo;crashed\u0026rsquo; worker\u0026rsquo;s files might still be accessible. This is a crucial difference from a production environment. Simply re-queuing all successfully completed map tasks from a crashed worker can be inefficient:\nPerformance Hit: It can lead to significant re-computation and potential test timeouts, especially if many map tasks were already done by a worker which crashed. Complexity: Managing pendingMappers and preventing reduce tasks from starting prematurely adds complexity if many map tasks are suddenly re-added. A More Targeted Optimization: A more refined approach is to only re-run a successful map task from a crashed worker if its specific output intermediate partitions are actually needed by currently pending (not yet completed) reduce tasks.\nThis involves:\nIdentifying which map tasks the crashed worker completed.\nDetermining if their output partitions are required by any active or future reduce tasks.\nOnly then, re-queueing those specific map tasks and invalidating their previous intermediate file locations.\nThis smarter retry avoids redundant work but increases coordinator complexity. For our lab, focusing on retrying the currently running task of a failed worker proved sufficient to pass the tests, partly due to the shared filesystem behaviour making the storage of intermediate files also in some sense to global filesystem\nIn essence, checkWorkerStatus implements the \u0026ldquo;timeout and retry\u0026rdquo; strategy. It ensures that work assigned to unresponsive workers is not indefinitely stalled and is eventually re-assigned, which is fundamental for making progress in a distributed system prone to failures.\nJob Completion (Done) main/mrcoordinator.go periodically calls Done() to check if the entire job is finished. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // main/mrcoordinator.go calls Done() periodically to find out // if the entire job has finished. func (c *Coordinator) Done() bool { c.mu.Lock() // If the job is marked as finished and we haven\u0026#39;t started the shutdown sequence for goroutines yet if c.finished \u0026amp;\u0026amp; !c.shutdownSignaled { fmt.Printf(\u0026#34;[Coordinator Shutdown]: MR workflow completed. Signaling internal goroutines to stop.\\n\u0026#34;) close(c.done) // Signal all listening goroutines c.shutdownSignaled = true // Mark that we\u0026#39;ve signaled them } // If we have signaled for shutdown, but haven\u0026#39;t yet confirmed all goroutines are done if c.shutdownSignaled \u0026amp;\u0026amp; !c.allGoroutinesDone { c.mu.Unlock() c.wg.Wait() // Wait for all goroutines (like the health checker) to call c.wg.Done() c.mu.Lock() c.allGoroutinesDone = true fmt.Printf(\u0026#34;[Coordinator Shutdown]: All internal goroutines have completed.\\n\u0026#34;) } isCompletelyDone := c.finished \u0026amp;\u0026amp; c.allGoroutinesDone c.mu.Unlock() return isCompletelyDone } The Worker (mr/worker.go) The Worker process is responsible for executing the actual Map and Reduce functions as directed by the Coordinator. Each worker operates independently, requesting tasks, performing them, and reporting back the results.\nWorker\u0026rsquo;s Main Loop (Worker function) The Worker function, called by main/mrworker.go, contains the main operational loop. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 var workerId WorkerId = WorkerId(os.Getpid()) // Unique ID for this worker process var dirName string = fmt.Sprintf(\u0026#34;w-%d\u0026#34;, workerId) // Worker-specific directory for temp files // ... (Log, ihash functions) ... // main/mrworker.go calls this function. func Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { Log(\u0026#34;Started\u0026#34;) // Create a worker-specific directory if it doesn\u0026#39;t exist. // Used for storing temporary files before atomic rename. if _, err := os.Stat(dirName); os.IsNotExist(err) { err := os.Mkdir(dirName, 0755) // ... (error handling) ... } getTaskargs := GetTaskArgs{ // Prepare args for GetTask RPC WorkerId: workerId, } for { // Main loop: continuously ask for tasks getTaskReply := GetTaskReply{} Log(\u0026#34;Fetching task from coordinator...\u0026#34;) ok := call(\u0026#34;Coordinator.GetTask\u0026#34;, \u0026amp;getTaskargs, \u0026amp;getTaskReply) if ok { // Successfully contacted Coordinator task := \u0026amp;getTaskReply.Task nReduce := getTaskReply.NR // Number of reduce partitions switch task.Type { case MapType: Log(fmt.Sprintf(\u0026#34;Assigned map job with task id: %s\u0026#34;, task.Id)) processMapTask(task, nReduce, mapf) // Execute the map task case ReduceType: Log(fmt.Sprintf(\u0026#34;Assigned reduce job with task id: %s\u0026#34;, task.Id)) intermediateFiles := getTaskReply.IntermediateFiles // Get locations from Coordinator processReduceTask(task, intermediateFiles, reducef) // Execute reduce task default: // InvalidType or unknown Log(\u0026#34;Invalid task recieved or no tasks available. Sleeping.\u0026#34;) } // If a valid task was processed (not InvalidType), report its status if task.Type != InvalidType { reportTaskArgs := ReportTaskArgs{ Task: *task } reportTaskReply := ReportTaskReply{} ok = call(\u0026#34;Coordinator.ReportTask\u0026#34;, \u0026amp;reportTaskArgs, \u0026amp;reportTaskReply) if !ok || !reportTaskReply.Status { Log(\u0026#34;Failed to report task or coordinator indicated an issue. Exiting.\u0026#34;) // The lab hints that if a worker can\u0026#39;t contact the coordinator, // it can assume the job is done and the coordinator has exited. removeLocalWorkerDirectory() // Clean up worker-specific directory return } } // Brief pause before asking for the next task. time.Sleep(WORKER_SLEEP_DURATION) // WORKER_SLEEP_DURATION is 2s } else { // Failed to contact Coordinator Log(\u0026#34;Failed to call \u0026#39;Coordinator.GetTask\u0026#39;! Coordinator not found or exited. Exiting worker.\u0026#34;) // removeLocalWorkerDirectory() // Cleanup if needed, though not strictly required by lab on exit return // Exit the worker process } } } Core Logic: Continuously polls the Coordinator for tasks (Coordinator.GetTask). Based on the task type (MapType or ReduceType), it calls the respective processing function. After processing, it reports the outcome to the Coordinator (Coordinator.ReportTask). Exit Condition: If communication with the Coordinator fails (e.g., GetTask RPC fails), the worker assumes the job is complete and the Coordinator has shut down, so the worker also exits. This is a simple shutdown mechanism compliant with the lab requirements. Local Directory: Each worker maintains a local directory (dirName like w-workerId) for its temporary files, ensuring isolation before final output naming. Processing Map Tasks (processMapTask) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 // Processes map task by fetching `Filename` from Task // Calls provided mapf function and stores intermediate files after // paritioninng them based on `ihash` function func processMapTask(task *Task, nReduce int, mapf func(string, string) []KeyValue) error { Log(fmt.Sprintf(\u0026#34;Processing map task with id %s and file: %s\u0026#34;, task.Id, task.Filename)) file, err := os.Open(task.Filename) // Open the input split (file) // ... (error handling: set task.Status = StatusError, return) ... content, err := io.ReadAll(file) // Read the entire file content // ... (error handling: set task.Status = StatusError, return) ... file.Close() intermediate := mapf(task.Filename, string(content)) // Execute the user-defined map function // Group intermediate key-value pairs by partition buckets := make(map[int][]KeyValue) for _, kv := range intermediate { partition := ihash(kv.Key) % nReduce // Determine partition using ihash buckets[partition] = append(buckets[partition], kv) } task.Output = []string{} // Clear previous output, prepare for new output filenames // For each partition, sort and write to a temporary intermediate file for partition, kva := range buckets { // In-memory sort for this partition\u0026#39;s KeyValue pairs. // The paper mentions external sort if data is too large, but here it\u0026#39;s in-memory. sort.Sort(ByKey(kva)) // Create a temporary file in the worker\u0026#39;s specific directory. tempFile, err := os.CreateTemp(dirName, \u0026#34;mwt-*\u0026#34;) // \u0026#34;mwt\u0026#34; for map worker temp // ... (error handling: set task.Status = StatusError, return) ... enc := json.NewEncoder(tempFile) for _, kv := range kva { // Write sorted KeyValue pairs to the temp file using JSON encoding err := enc.Encode(\u0026amp;kv) // ... (error handling: set task.Status = StatusError, tempFile.Close(), return) ... } tempFile.Close() // Close after writing // Atomically rename the temporary file to its final intermediate name. // Filename format: mr-\u0026lt;map_task_id\u0026gt;-\u0026lt;partition_number\u0026gt; (e.g., mr-m-0-1) // Stored within the worker\u0026#39;s directory: w-\u0026lt;workerId\u0026gt;/mr-m-0-1 intermediateFilename := filepath.Join(dirName, fmt.Sprintf(\u0026#34;mr-%s-%d\u0026#34;, task.Id, partition)) err = os.Rename(tempFile.Name(), intermediateFilename) // ... (error handling: set task.Status = StatusError, return) ... task.Output = append(task.Output, intermediateFilename) // Add final filename to task\u0026#39;s output list } task.Status = StatusSuccess // Mark task as successful return nil } Core Logic: Reads the assigned input file, applies the user-defined mapf, partitions the output KeyValue pairs using ihash() % nReduce, sorts each partition\u0026rsquo;s data in memory, and writes it to a uniquely named intermediate file within its local directory. Intermediate Files: Output filenames (e.g., w-workerId/mr-mapTaskID-partitionID) are collected in task.Output. Atomic Rename: Uses os.Rename to make intermediate files visible only once fully written, preventing partial reads by reducers. This is crucial for consistency, especially if crashes occur. In-Memory Sort: A simplification for the lab; a production system might use external sorting if intermediate data for a partition is too large for memory. Processing Reduce Tasks (processReduceTask) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 func processReduceTask(task *Task, intermediateFiles map[WorkerId][]string, reducef func(string, []string) string) error { Log(fmt.Sprintf(\u0026#34;Processing reduce task with id %s for partition key %s\u0026#34;, task.Id, task.Filename)) // Create a temporary output file in the worker\u0026#39;s directory tempReduceFile, err := os.CreateTemp(dirName, \u0026#34;mwt-*\u0026#34;) // \u0026#34;mwt\u0026#34; for map worker temp (could be \u0026#34;rwt\u0026#34;) // ... (error handling: set task.Status = StatusError, return) ... defer tempReduceFile.Close() // Ensure temp file is closed var kva []KeyValue // To store all KeyValue pairs for this reduce partition // Gather all intermediate data for this reduce task\u0026#39;s partition from various map workers. // `intermediateFiles` (map[WorkerId][]string) comes from the Coordinator, // mapping map worker IDs to the list of intermediate files they produced for *this specific partition*. for mapWorkerId, filesFromMapWorker := range intermediateFiles { for _, filename := range filesFromMapWorker { Log(fmt.Sprintf(\u0026#34;Processing intermediate file %s from map worker %d\u0026#34;, filename, mapWorkerId)) intermediateFile, err := os.Open(filename) // ... (error handling: set task.Status = StatusError, return) ... dec := json.NewDecoder(intermediateFile) for { // Read all KeyValue pairs from this intermediate file var kv KeyValue if err := dec.Decode(\u0026amp;kv); err != nil { if err != io.EOF { // Handle actual decoding errors Log(fmt.Sprintf(\u0026#34;Error decoding KV from intermediate file %s: %v\u0026#34;, filename, err)) task.Status = StatusError intermediateFile.Close() return err } break // EOF reached } kva = append(kva, kv) } intermediateFile.Close() } } // Sort all collected KeyValue pairs by key. This groups identical keys together. // This is Step 5 of the paper: \u0026#34;When a reduce worker has read all intermediate data, // it sorts it by the intermediate keys...\u0026#34; // Again, this is an in-memory sort of all data for this partition. sort.Sort(ByKey(kva)) // Iterate over sorted data, apply reducef for each unique key i := 0 for i \u0026lt; len(kva) { j := i + 1 // Find all values for the current key kva[i].Key for j \u0026lt; len(kva) \u0026amp;\u0026amp; kva[j].Key == kva[i].Key { j++ } values := []string{} for k := i; k \u0026lt; j; k++ { values = append(values, kva[k].Value) } output := reducef(kva[i].Key, values) // Execute user-defined reduce function // Write output in the format \u0026#34;key value\\n\u0026#34; to the temporary reduce file. // This matches main/mrsequential.go and the lab\u0026#39;s expected output format. fmt.Fprintf(tempReduceFile, \u0026#34;%v %v\\n\u0026#34;, kva[i].Key, output) i = j // Move to the next unique key } // Atomically rename the temporary output file to its final name (e.g., mr-out-0). // The final output file is placed in the current directory (mr-tmp/ during tests), // not the worker-specific one, as it\u0026#39;s global output. finalOutputFileName := fmt.Sprintf(\u0026#34;mr-out-%s\u0026#34;, task.Filename) // task.Filename is the partition key (e.g., \u0026#34;0\u0026#34;, \u0026#34;1\u0026#34;) err = os.Rename(tempReduceFile.Name(), finalOutputFileName) // ... (error handling: set task.Status = StatusError, return) ... task.Output = []string{finalOutputFileName} // Record final output filename task.Status = StatusSuccess return nil } Core Logic: Gathers all intermediate KeyValue pairs for its assigned partition (identified by task.Filename) from the locations provided by the Coordinator (intermediateFiles). It then sorts all these KeyValue pairs together, groups them by unique key, applies the user-defined reducef for each key and its list of values, and writes the final output. Data Aggregation: Reads from multiple intermediate files (potentially from different map workers) that correspond to its specific partition. Global Sort (for the partition): All KeyValue pairs for the partition are sorted together in memory before reduction. This is essential for grouping values for the same key. Final Output: Writes output to a temporary file and then atomically renames it to the final output file name (e.g., mr-out-X), which is placed in the main job directory (not the worker\u0026rsquo;s specific temp directory). In-Memory Sort: Similar to map tasks, all data for a reduce partition is sorted in memory. Conclusion Working on this MapReduce project taught me a lot about Go’s concurrency features, how to use RPC for process communication, and how the MapReduce framework organizes big data jobs. Most importantly, I learned to think about what can go wrong in distributed systems and how to handle failures gracefully. It’s been a great hands-on way to understand the real challenges behind large-scale data processing.\nReferences https://pdos.csail.mit.edu/6.824/labs/lab-mr.html https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf https://github.com/harshrai654/6.5840/tree/lab0/src ","permalink":"http://localhost:1313/blogs/map-reduce/","summary":"\u003cp\u003eThis article shares learnings from Google\u0026rsquo;s influential MapReduce paper and explores the challenges encountered while implementing a simplified version. Our system uses multiple worker processes, running on a single machine and communicating via RPC, to mimic key aspects of a distributed environment.\u003c/p\u003e\n\u003ch1 id=\"what-is-map-reduce\"\u003eWhat is Map-Reduce\u003c/h1\u003e\n\u003cp\u003eAt its core, MapReduce is a programming model and an associated framework for processing and generating massive datasets using a parallel, distributed algorithm, typically on a cluster of computers. You might already be familiar with map and reduce operations from functional programming languages. For instance, in JavaScript, array.map() transforms each element of an array independently based on a mapper function, while array.reduce() iterates through an array, applying a reducer function to accumulate its elements into a single output value (e.g., a sum, or a new, aggregated object).\u003c/p\u003e","title":"Map Reduce"},{"content":"This article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.\nBackground After introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as ioredis for interacting with our Redis-compatible database. Once we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand why this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions – options ranged from open-source SDKs (OpenTelemetry Node.js SDK) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted Grafana Tempo instance to test the setup and feasibility. (So far, the setup is working great, and I\u0026rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.\nAn example trace showing Redis commands executed while processing an HTTP request.\nOkay, back to the problem. After setting up tracing, we could visually inspect the Redis command spans, like in the example above. Correlating these trace timings with our earlier metrics confirmed the high latency numbers. Indeed, something wasn\u0026rsquo;t right with how our service was connecting to the cache server and executing commands.\nFinding the culprit Dragonfly is a Redis-compatible key-value database but with support for multithreading; Redis, on the other hand, follows a single-threaded, event-based model similar to Node.js.\nOur first step was to check if anything was wrong with the cache server deployment itself. We enabled Dragonfly\u0026rsquo;s slow query logs to check for commands taking longer than 100ms. Interestingly, we only saw SCAN commands in these logs. This didn\u0026rsquo;t immediately make sense because our high latency metrics were observed for commands like GET, SET, DELETE, and UNLINK. These are typically O(1) commands and should not take more than a few milliseconds, so we ruled out the possibility of these specific commands taking significant time to process on the cache server itself.\nTo further monitor command execution time directly on the Dragonfly server, we enabled its Prometheus metrics exporter. We looked at two metrics: \u0026ldquo;Pipeline Latency\u0026rdquo; and \u0026ldquo;Average Pipeline Length\u0026rdquo;. The \u0026ldquo;Average Pipeline Length\u0026rdquo; was always close to 0, and the \u0026ldquo;Pipeline Latency\u0026rdquo; was consistently under 10ms. While there wasn\u0026rsquo;t clear documentation from Dragonfly detailing these metrics precisely, going by the names, we assumed they represented the actual command execution time on the cache server.\nSo, the evidence suggested commands were executing quickly on the cache server (confirmed by both low Prometheus pipeline latency and the absence of GET/SET etc., in the slow query logs). But wait – the slow query logs did show the SCAN command with execution times in the range of 50ms to 200ms. So, what exactly is the SCAN command, and why were we using it?\nThe SCAN Command and Frequent Use First, what is the SCAN command? SCAN is a cursor-based command in Redis used to iterate over the keyspace. It takes a cursor position and a glob pattern, matching the pattern against keys in the database without blocking the server for long periods (unlike its predecessor, KEYS).\nIn our system, we primarily use SCAN to invalidate cache entries for specific users. We publish cache invalidation events from various parts of our application. Depending on the event type, a process triggers that uses SCAN to find and delete cache keys matching a user-specific pattern. Since these invalidation events are very frequent in our workload, the SCAN command was executed much more often than we initially realized.\nThe Restart Clue and Initial Hypotheses During our investigation, we stumbled upon a curious behavior: if we restarted the DragonflyDB instance, the high command latency would drop back to normal levels for a few hours before inevitably climbing back up to the problematic 200ms range. This provided a temporary, albeit disruptive, fix during peak hours (the cost being the loss of cached data, although we later mitigated this by enabling snapshotting for restores).\nThis temporary \u0026ldquo;fix\u0026rdquo; from restarting was a significant clue. It strongly suggested the problem wasn\u0026rsquo;t necessarily the SCAN command\u0026rsquo;s execution on the server (which slow logs and metrics already indicated was fast most of the time, except for SCAN itself sometimes), but perhaps something related to the state of the connection or interaction between our Node.js services and DragonflyDB over time.\nThis led us to two main hypotheses related to connection handling:\nConnection Pooling: ioredis, the driver we were using, maintains a single connection to the Redis server. This is standard for single-threaded Redis, where multiple connections offer little benefit. However, DragonflyDB is multi-threaded. Could our single connection be a bottleneck when dealing with frequent commands, especially potentially long-running SCAN operations, under Dragonfly\u0026rsquo;s multi-threaded architecture? Perhaps connection pooling would allow better parallel execution. Long-Running TCP Connections: Could the TCP connections themselves, after being open for extended periods, degrade in performance or enter a state that caused delays in sending commands or receiving responses? Investigating Connection Pooling To test the connection pooling hypothesis, we considered adding a pooling library like generic-pool on top of ioredis. However, we noticed that node-redis, the official Redis client for Node.js, already included built-in connection pooling capabilities and had an API largely compatible with ioredis. So, as a direct way to test the effect of pooling, we replaced ioredis with node-redis in our service.\nUnfortunately, even with node-redis and its connection pooling configured, the behavior remained the same: high latencies persisted, only dropping temporarily after a DragonflyDB restart. This seemed to rule out simple connection pooling as the solution.\nInvestigating TCP Connection State With the pooling hypothesis proving unfruitful, we turned to the idea of issues with long-running TCP connections. We tried several approaches to detect problems here:\nCode Profiling: We profiled the Node.js service during periods of high latency, generating flame graphs to see if significant time was being spent within the Redis driver\u0026rsquo;s internal methods, specifically looking for delays in writing to or reading from the underlying network socket. Packet Tracing: We used tcpdump on the service instances to capture network traffic between the Node.js service and DragonflyDB, looking for signs of network-level latency, packet loss, or retransmissions that could explain the delays. Both of these efforts came up empty. The profiling data showed no unusual delays within the driver\u0026rsquo;s socket operations, and the tcpdump analysis indicated normal network communication without significant latency.\nWe had confirmed the high frequency of SCAN, observed the strange restart behavior, and ruled out both simple connection pooling and obvious TCP-level network issues as the root cause. We needed a new hypothesis.\nA Perfect Correlation and the Root Cause We refocused on the most reliable clue: why did restarting the cache server temporarily fix the latency? We had ruled out connection management issues. The other major effect of a restart was clearing the in-memory key-value store (remember, at this stage, we weren\u0026rsquo;t restoring snapshots immediately after restarts). Plotting the number of keys in DragonflyDB over time confirmed our suspicion. We saw the key count drop to zero after each restart and then steadily climb until the next restart. Correlating this key count graph with our latency metrics revealed a clear pattern: as the number of keys rose, so did the p99 latency for our Redis commands. Although Redis/DragonflyDB can handle millions of keys, we started seeing significant latency increases once the key count grew into the 100,000–200,000 range in our specific setup. Now, which of our commands would be most affected by the number of keys? Looking at our common operations (GET, SET, DEL, UNLINK, SCAN, EXISTS), SCAN stood out. While most Redis commands have O(1) complexity, SCAN\u0026rsquo;s performance is inherently tied to the number of keys it needs to iterate over. (More details on SCAN\u0026rsquo;s behavior can be found in the official Redis documentation). We were using SCAN extensively for cache invalidation, employing code similar to this:\n1 2 3 4 5 6 7 8 const keysToDelete = []; for await (const key of this.client.scanIterator({ MATCH: pattern, COUNT: count, })) { keysToDelete.push(key); } Critically, for each cache invalidation event (which were frequent), we potentially ran multiple SCAN operations, and each scanIterator loop continued until the entire relevant portion of the keyspace was traversed to find all keys matching the pattern.\nBut how could SCAN, even if sometimes slow itself (as seen in the slow logs), cause delays for fast O(1) commands like GET or SET? Our server-side metrics (like Dragonfly\u0026rsquo;s Pipeline Latency) showed quick execution times for those O(1) commands. This led to a new hypothesis: the server metrics likely measured only the actual CPU time for command execution, not the total turnaround time experienced by the client, which includes any wait time before the command gets processed.\nEven though SCAN is non-blocking, issuing a large number of SCAN commands concurrently, especially when each needs to iterate over a growing keyspace (100k-200k+ keys), could potentially overwhelm the server\u0026rsquo;s connection-handling threads (even Dragonfly\u0026rsquo;s multiple threads). If threads were busy processing numerous, longer-running SCAN iterations, incoming GET, SET, etc., commands would have to wait longer before being picked up for execution, increasing their total observed latency from the client\u0026rsquo;s perspective. The performance degradation of SCAN with more keys, combined with its high frequency, created a bottleneck that impacted all other operations.\nThe Fix: Replacing SCAN with SETs Armed with this hypothesis, the fix became clear: we needed to drastically reduce or eliminate our reliance on SCAN for finding keys to invalidate.\nWe implemented an alternative approach:\nFor each user (or entity needing cache invalidation), maintain a Redis SET containing the keys associated with that user. When an invalidation event occurs for a user, instead of scanning the keyspace, retrieve the list of keys directly from the user\u0026rsquo;s specific SET using the SMEMBERS command (which is much faster for this purpose). Delete the keys obtained from the SET. This required some additional logic (housekeeping) to keep these SETs up-to-date as new keys were cached, but the performance benefits far outweighed the complexity.\nSo we opted for a more optimal way to invalidate cache where we also stored key names related to a user in a redis SET, so the use of SCAN was not moot because we do not need to scan the namespace every time to first prepare list of keys to delete now we can get that with just a SMEMBERS command which gives list of set elements. A little housekeeping was needed to maintain this set for each user, but it still outweighs the benefits.\nThe Results: Latency Tamed This change dramatically solved the high latency issue.\nFirst, the frequency of the SCAN command dropped to nearly zero, as expected. Consequently, the latency spikes across all commands disappeared. The overall p99 latency stabilized at a much healthier level. Interestingly, even the server-side execution time reported by Dragonfly showed improvement, suggesting the reduced load from SCAN allowed other commands to be processed more efficiently internally as well. The final result was a significant drop in p99 latency, bringing it down from peaks often exceeding 200ms (and sometimes reaching ~500ms as shown below) to consistently around ~40ms. p99 latency comparison showing reduction from peaks around ~500ms down to ~40ms after the fix\nConclusion: Trust the Clues Looking back at how we tackled our high Redis command latency (~200ms+ p99), the journey involved ramping up the setup for observability, and exploring several potential culprits. While investigating connection pooling, profiling code execution, and even analyzing network packets with tcpdump were valuable exercises, they ultimately didn\u0026rsquo;t lead us to the root cause in this case.\nThe most significant clue, in hindsight, was the temporary fix we got from restarting the DragonflyDB instance. If we had focused more intently from the start on why that restart helped – realizing it pointed directly to the state accumulated within the database (specifically, the key count) – and correlated that with our application\u0026rsquo;s command usage patterns, we likely would have identified the high-frequency, full-keyspace SCAN operations as the bottleneck much sooner.\nThe real issue wasn\u0026rsquo;t low-level network glitches or basic connection handling, but an application-level pattern: frequent SCANs over a growing keyspace were overwhelming the server, increasing wait times for all commands. Switching our invalidation logic to use Redis SETs (SMEMBERS) eliminated this problematic pattern, finally bringing our p99 latency down to a stable ~40ms. Although optimizing the SCAN operation itself using [[https://redis.io/docs/latest/operate/oss_and_stack/reference/cluster-spec/#hash-tags]] was another interesting possibility (ensuring keys with the same tag land in the same hash slot to potentially limit scan scope), we didn\u0026rsquo;t opt for this solution since this required a rethink of our cache key nomenclature and would have involved substantial changes. Ultimately, the most direct path to a solution often lies in understanding the application\u0026rsquo;s behavior and trusting the most obvious clues, rather than immediately reaching for the deepest diagnostic tools.\n","permalink":"http://localhost:1313/blogs/debugging-redis-latency/","summary":"\u003cp\u003eThis article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.\u003c/p\u003e\n\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eAfter introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as \u003ccode\u003eioredis\u003c/code\u003e for interacting with our Redis-compatible database.\nOnce we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand \u003cem\u003ewhy\u003c/em\u003e this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions – options ranged from open-source SDKs (\u003ca href=\"https://opentelemetry.io/docs/languages/js/\"\u003eOpenTelemetry Node.js SDK\u003c/a\u003e) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted \u003ca href=\"https://grafana.com/oss/tempo/\"\u003eGrafana Tempo\u003c/a\u003e instance to test the setup and feasibility. (So far, the setup is working great, and I\u0026rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.\u003c/p\u003e","title":"Debugging Redis Latency"},{"content":"Socket File Descriptors and Their Kernel Structures A socket is a special type of file descriptor (FD) in Linux, represented as socket:[inode]. Unlike regular file FDs, socket FDs point to in-memory kernel structures, not disk inodes. The /proc/\u0026lt;pid\u0026gt;/fd directory lists all FDs for a process, including sockets. The inode number of a socket can be used to inspect its details via tools like ss and /proc/net/tcp. Example: Checking Open FDs for Process 216 ls -l /proc/216/fd Output:\nlrwx------. 1 root root 64 Mar 2 09:01 0 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 1 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 2 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 3 -\u0026gt; \u0026#39;socket:[35587]\u0026#39; Here, FD 3 is a socket pointing to inode 35587. Checking FD Details cat /proc/216/fdinfo/3 Output: pos: 0 flags: 02 mnt_id: 10 ino: 35587 How Data Flows Through a Socket (User Space to Kernel Space) When a process writes data to a socket, it is copied from user-space memory to kernel-space buffers (using syscall write()). The kernel then processes and forwards the data to the network interface card (NIC). This copying introduces overhead, which can be mitigated using zero-copy techniques like sendfile() and io_uring. (A tweet which might recall this) TCP 3-Way Handshake (How a Connection is Established) A TCP connection is established through a 3-way handshake between the client and server:\nClient → SYN (Initial sequence number) Server → SYN-ACK (Acknowledges client’s SYN, sends its own) Client → ACK (Acknowledges server’s SYN-ACK) Checking a Listening TCP Port ss -aep | grep 35587 Output:\ntcp LISTEN 0 0 0.0.0.0:41555 0.0.0.0:* users:((\u0026#34;nc\u0026#34;,pid=216,fd=3)) ino:35587 sk:53f53fa7 Port 41555 is in the LISTEN state, bound to nc (netcat). It corresponds to socket inode 35587. TCP Connection Queues in the Kernel Once a TCP connection request arrives, it goes through two queues managed by the kernel:\n1️] SYN Queue (Incomplete Connection Queue) Holds half-open connections (received SYN but not yet fully established). If this queue overflows, new SYN requests may be dropped (SYN flood attack risk). 2]Accept Queue (Backlog Queue, Fully Established Connections) Holds connections that have completed the handshake and are waiting for accept(). Controlled by listen(sockfd, backlog), where backlog defines max queue size If full, new connections are dropped. Checking Connection Queues ss -ltni Output:\nState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 5 0.0.0.0:8080 0.0.0.0:* Recv-Q (Accept Queue, Backlog Queue) → Number of connections waiting in the backlog. Send-Q (Not relevant here) → Usually for outbound data. Checking Kernel TCP Queues via **/proc/net/tcp** cat /proc/net/tcp Output:\nsl local_address rem_address st tx_queue rx_queue tr tm-\u0026gt;when retrnsmt uid timeout inode 0: 00000000:A253 00000000:0000 0A 00000000:00000000 00:00000000 00000000 0 0 35587 1 0000000053f53fa7 100 0 0 10 0 tx_queue → Data waiting to be sent. rx_queue → Data waiting to be read. The Role of the Kernel in TCP Connections The Linux kernel manages the entire TCP stack:\nHandshaking, sequencing, retransmissions, timeouts. Maintaining connection queues \u0026amp; buffering. Interacting with the NIC for packet transmission. Applications don’t deal with raw packets directly—they only read/write to sockets, while the kernel handles the rest.\nFlow Diagram: TCP Connection Journey with Kernel Involvement Client (User Space) Kernel (Server) Application (User Space) | | | | 1. SYN | | |---------------------------\u0026gt;| | | | | | 2. SYN-ACK | | |\u0026lt;---------------------------| | | | | | 3. ACK | | |---------------------------\u0026gt;| | | | Connection Added to SYN Queue | | |-----------------------------\u0026gt;| | | | | | Connection Moved to Accept Queue | | |-----------------------------\u0026gt;| | | | | | Application Calls `accept()` | | |-----------------------------\u0026gt;| | | | | | Data Transfer Begins | Why Each Connection Needs a Separate FD When a server listens on a port, it creates a listening socket FD. When a client initiates a connection: The kernel accepts the connection using the 3-way handshake. The kernel creates a new socket structure for this connection. The server application calls accept(), which returns a new FD. Why is a New FD Required? Each TCP connection requires its own state:\nSequence numbers (to track packets in order) Receive and send buffers Connection state (e.g., established, closed) Does the Communication Happen on the Same Port?\nYes, all connections still use the same local port (the port used for listening for connection on the server side). But, each accepted connection is a unique socket with a different remote IP/port pair. The kernel distinguishes connections by:\n(Local IP, Local Port) \u0026lt;\u0026ndash;\u0026gt; (Remote IP, Remote Port). Think of it like this:\nThe listening socket is like a front desk at a hotel. Every guest (client) gets their own room (new socket), but the front desk (listening socket) stays the same. Multiple Sockets on the Same Port (SO_REUSEPORT) Allows multiple FDs bound to the same port. Kernel load-balances connections across them. Used in: Nginx, HAProxy. Example: Multi-Threaded Server with SO_REUSEPORT int sock1 = socket(AF_INET, SOCK_STREAM, 0); int sock2 = socket(AF_INET, SOCK_STREAM, 0); setsockopt(sock1, SOL_SOCKET, SO_REUSEPORT, \u0026amp;opt, sizeof(opt)); setsockopt(sock2, SOL_SOCKET, SO_REUSEPORT, \u0026amp;opt, sizeof(opt)); bind(sock1, ...); bind(sock2, ...); listen(sock1, BACKLOG); listen(sock2, BACKLOG); References https://chatgpt.com/c/67c414f3-4830-8013-a058-0fd2596e3c07 ","permalink":"http://localhost:1313/blogs/socket-file-descriptor-and-tcp-connections/","summary":"\u003ch2 id=\"socket-file-descriptors-and-their-kernel-structures\"\u003eSocket File Descriptors and Their Kernel Structures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003esocket\u003c/strong\u003e is a special type of file descriptor (FD) in Linux, represented as \u003ccode\u003esocket:[inode]\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eUnlike regular file FDs, socket FDs point to \u003cstrong\u003ein-memory kernel structures\u003c/strong\u003e, not disk inodes.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003e/proc/\u0026lt;pid\u0026gt;/fd\u003c/code\u003e directory lists all FDs for a process, including sockets.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003einode number\u003c/strong\u003e of a socket can be used to inspect its details via tools like \u003ccode\u003ess\u003c/code\u003e and \u003ccode\u003e/proc/net/tcp\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"example-checking-open-fds-for-process-216\"\u003eExample: Checking Open FDs for Process \u003ccode\u003e216\u003c/code\u003e\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003els -l /proc/216/fd\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eOutput:\u003c/strong\u003e\u003c/p\u003e","title":"Socket File Descriptor and TCP connections"},{"content":"Overall Organization Of Data In Disks Assuming we have a 256KB disk.\nDisk Blocks: The basic units of storage on the disk, each 4 KB in size. The disk is divided into these blocks, numbered from 0 to N-1 (where N is the total number of blocks). Inode Bitmap (i): Block 1; a bitmap tracking which inodes are free (0) or in-use (1). Data Bitmap (d): Block 2; a bitmap tracking which data blocks are free (0) or allocated (1). Inode Table (I): Blocks 3-7; an array of inodes, where each inode (256 bytes) holds metadata about a file, like size, permissions, and pointers to data blocks. 5 blocks of 4KB will contain 80 256 byte inode strutures. Data Region (D): Blocks 8-63; the largest section, storing the actual contents of files and directories. Inode Every inode has a unique identifier called an inode number (or i-number). This number acts like a file’s address in the file system, allowing the operating system to quickly locate its inode. For example:\nIn a system with 80 inodes, numbers might range from 0 to 79. Conventionally, the root directory is assigned inode number 2 (numbers 0 and 1 are often reserved or used for special purposes). The inode number is the key to finding a file’s metadata on the disk, and it’s stored in directory entries alongside the file’s name.\nHow Do We Jump to the Disk Block for a Specific Inode Number? In a file system like vsfs, inodes are stored consecutively in an inode table, a reserved area of the disk (e.g., spanning blocks 3 to 7). Each inode has a fixed size—let’s say 256 bytes—and the disk is divided into blocks (e.g., 4096 bytes each). To locate a specific inode given its i-number, we calculate its exact position on the disk. Here’s how:\nIdentify the Inode Table’s Starting Block: Suppose the inode table starts at block 3. Calculate the Block Containing the Inode: Formula: block = (i-number * inode_size) / block_size + start_block Example: For inode 10, inode_size = 256 bytes, block_size = 4096 bytes, start_block = 3 (10 * 256) / 4096 = 2560 / 4096 = 0.625 → integer part is 0. block = 0 + 3 = 3. So, inode 10 is in block 3. Calculate the Offset Within the Block: Formula: offset = (i-number * inode_size) % block_size Example: (10 * 256) % 4096 = 2560 % 4096 = 2560 bytes. Inode 10 starts 2560 bytes into block 3. Result: The operating system reads block 3 from the disk and jumps to offset 2560 bytes to access inode 10’s metadata. This process allows the file system to efficiently retrieve an inode’s information and, from there, its data blocks. Since multiple processes may have file descriptors for the same file opened with their own data of offset to read from, multiple processes will be accessing the same inode structure to read about file and modify its data (Here inodes data would mean modifying things like last accessed time or adding another entry for list of data blocks etc), So i-node needs some sort of concurrency control in place. (More on this)\nHow does inode know the data it owns An inode is a data structure in a file system that stores information about a file, including where its data is located on the disk. Instead of holding the file’s data itself, the inode contains pointers that reference the disk blocks where the data is stored.\nDisk Blocks: These are fixed-size chunks of storage on the disk (e.g., 4 KB each) that hold the actual file content. Pointers: These are entries in the inode that specify the locations of these disk blocks. The inode uses these pointers to keep track of all the blocks that make up a file, allowing the file system to retrieve the data when needed.\nWhat Are Disk Addresses? Disk addresses are the identifiers that tell the file system the exact physical locations of data blocks on the disk. Think of them as a map: each address corresponds to a specific block, such as block number 100, which might map to a particular sector and track on a hard drive.\nFor example, if a file is 8 KB and the block size is 4 KB, the inode might have two pointers with disk addresses like \u0026ldquo;block 50\u0026rdquo; and \u0026ldquo;block 51,\u0026rdquo; pointing to the two blocks that hold the file’s data. How Does an Inode Manage Disk Blocks? The inode organizes its pointers in a way that can handle files of different sizes efficiently. It uses a combination of direct pointers and indirect pointers, forming a multi-level indexing structure.\n1. Direct Pointers The inode starts with a set of direct pointers, which point straight to the disk addresses of data blocks. Example: If the block size is 4 KB and the inode has 12 direct pointers, it can directly address 12 × 4 KB = 48 KB of data. 2. Indirect Pointers (Multi-Level Indexing) For files too big for direct pointers alone, the inode uses indirect pointers, which point to special blocks that themselves contain more pointers. This creates a hierarchical, or multi-level, structure.\nSingle Indirect Pointer\nThis pointer points to a block (called an indirect block) that contains a list of disk addresses to data blocks.\nExample: If a block is 4 KB and each pointer is 4 bytes, the indirect block can hold 4 KB / 4 bytes = 1024 pointers. That’s 1024 × 4 KB = 4 MB of additional data.\nTotal with Direct: With 12 direct pointers and 1 single indirect pointer, the file can reach (12 + 1024) × 4 KB = 4,096 KB (about 4 MB).\nDouble Indirect Pointer\nThis pointer points to a block that contains pointers to other single indirect blocks, each of which points to data blocks.\nExample: The double indirect block might hold 1024 pointers to single indirect blocks. Each of those holds 1024 pointers to data blocks, so that’s 1024 × 1024 = 1,048,576 data blocks, or about 4 GB with 4 KB blocks.\nTotal with Direct and Single: (12 + 1024 + 1,048,576) × 4 KB ≈ 4 GB.\nThis structure acts like an imbalanced tree: small files use only direct pointers, while larger files use additional levels of indirection as needed.\nWhy Use Multi-Level Indexing? The multi-level indexing structure is designed to balance efficiency and scalability:\nSmall Files: Most files are small, so direct pointers handle them quickly without extra overhead. Large Files: Indirect pointers allow the inode to scale up to support massive files by adding more layers of pointers. How It Works in Practice When the file system needs to find a specific block in a file:\nIt checks the inode’s direct pointers first. If the block number is beyond the direct pointers’ range, it follows the single indirect pointer to the indirect block and looks up the address there. For even larger block numbers, it traverses the double indirect or triple indirect pointers, following the hierarchy until it finds the right disk address. This process ensures the file system can efficiently locate any block, no matter how big the file is.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-implementation.pdf https://grok.com/chat/d429de0e-556f-426c-84bd-21ff1b0c4002 (Contains reference to actual inode structure implementation along with locks it uses on Linux) https://grok.com/chat/c86e3040-2f86-4aa9-a317-1c0a464564a3?referrer=website ","permalink":"http://localhost:1313/blogs/file-system-implementation/","summary":"\u003ch2 id=\"overall-organization-of-data-in-disks\"\u003eOverall Organization Of Data In Disks\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eAssuming we have a 256KB disk\u003c/em\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDisk Blocks\u003c/strong\u003e: The basic units of storage on the disk, \u003cem\u003eeach 4 KB in size.\u003c/em\u003e The disk is divided into these blocks, numbered from 0 to N-1 (where N is the total number of blocks).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInode Bitmap (i)\u003c/strong\u003e: Block 1; a bitmap tracking which inodes are free (0) or in-use (1).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Bitmap (d)\u003c/strong\u003e: Block 2; a bitmap tracking which data blocks are free (0) or allocated (1).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInode Table (I)\u003c/strong\u003e: Blocks 3-7; an array of inodes, where each inode (256 bytes) holds metadata about a file, like size, permissions, and pointers to data blocks.\n5 blocks of 4KB will contain 80 256 byte inode strutures.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Region (D)\u003c/strong\u003e: Blocks 8-63; the largest section, storing the actual contents of files and directories.\n\u003cimg alt=\"Pasted image 20250301204506.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20250301204506.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"inode\"\u003eInode\u003c/h2\u003e\n\u003cp\u003eEvery inode has a unique identifier called an \u003cstrong\u003einode number\u003c/strong\u003e (or \u003cstrong\u003ei-number\u003c/strong\u003e). This number acts like a file’s address in the file system, allowing the operating system to quickly locate its inode. For example:\u003c/p\u003e","title":"Understanding Inodes and Disk Layout"},{"content":"Files and directories File systems virtualize persistent storage (e.g., hard drives, SSDs) into user-friendly files and directories, adding a third pillar to OS abstractions (processes for CPU, address spaces for memory).\nFile Paths and System Calls Files are organized in a tree-like directory structure, starting from the root (/). A file’s location is identified by its pathname (e.g., /home/user/file.txt). To interact with files, processes use system calls:\nopen(path, flags): Opens a file and returns a file descriptor (fd). read(fd, buffer, size): Reads data from the file into a buffer using the fd. write(fd, buffer, size): Writes data to the file via the fd. close(fd): Closes the file, freeing the fd. File Descriptors A file descriptor is a small integer, unique to each process, that identifies an open file. When a process calls open(), the operating system assigns it the next available fd (e.g., 3, 4, etc.). Every process starts with three default fds:\n0: Standard input (stdin) 1: Standard output (stdout) 2: Standard error (stderr) Quick Note: File Descriptors \u0026amp; Terminals Every process starts with three standard file descriptors:\n0 (stdin), 1 (stdout), 2 (stderr). Where They Point By default they link to a terminal device (e.g., /dev/pts/0 for Terminal 1, /dev/pts/1 for Terminal 2). These are character devices with a major number (e.g., 136) for the tty driver and a unique minor number (e.g., 0 or 1) for each instance.\nCommand Output (Terminal 1):\n1 2 3 4 ls -l /proc/self/fd lrwx------ 1 runner runner 64 Feb 23 11:47 0 -\u0026gt; /dev/pts/0 lrwx------ 1 runner runner 64 Feb 23 11:47 1 -\u0026gt; /dev/pts/0 lrwx------ 1 runner runner 64 Feb 23 11:47 2 -\u0026gt; /dev/pts/0 Device Details:\n1 2 3 4 5 6 7 8 9 10 11 12 ls -l /dev/pts/0 crw--w---- 1 runner tty 136, 0 Feb 23 11:53 /dev/pts/0 ~/workspace$ stat /dev/pts/0 File: /dev/pts/0 Size: 0 Blocks: 0 IO Block: 1024 character special file Device: 0,1350 Inode: 3 Links: 1 Device type: 136,0 Access: (0620/crw--w----) Uid: ( 1000/ runner) Gid: ( 5/ tty) Access: 2025-02-23 12:13:52.419852946 +0000 Modify: 2025-02-23 12:13:52.419852946 +0000 Change: 2025-02-23 11:36:28.419852946 +0000 Birth: - Versus a File A regular file descriptor (e.g., for test.txt) points to a disk inode with data blocks, tied to a filesystem device (e.g., /dev/sda1), not a driver.\nExample:\n1 2 3 4 5 6 7 8 9 10 11 12 ls -li test.txt 12345 -rw-r--r-- 1 runner runner 5 Feb 23 12:00 test.txt ~/workspace$ stat test.txt File: test.txt Size: 288 Blocks: 8 IO Block: 4096 regular file Device: 0,375 Inode: 1111 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 1000/ runner) Gid: ( 1000/ runner) Access: 2025-02-23 11:40:57.014322027 +0000 Modify: 2025-02-23 11:41:41.800233516 +0000 Change: 2025-02-23 11:41:41.800233516 +0000 Birth: 2025-02-23 11:40:57.014322027 +0000 Fun Test Redirected Terminal 1’s output to Terminal 2:\nCommand: echo \u0026quot;Hello\u0026quot; \u0026gt; /dev/pts/1 Result: \u0026ldquo;Hello\u0026rdquo; appeared in Terminal 2 (/dev/pts/1, minor 1)! Open File Table The Open File Table (OFT) is a system-wide structure in kernel memory that tracks all open files. Each entry in the OFT includes:\nThe current offset (position for the next read/write). Permissions (e.g., read, write). A reference count (if multiple processes share the file). Each process has its own array of file descriptors, where each fd maps to an entry in the OFT. For example, process A’s fd 3 and process B’s fd 4 might point to the same OFT entry if they’ve opened the same file.\nExample: Showing entries of a PID in open file table\n1 2 3 4 5 6 7 8 ~/workspace$ lsof -p 113 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME bash 113 runner cwd DIR 0,375 394 256 /home/runner/workspace bash 113 runner rtd DIR 0,1324 4096 1562007 / bash 113 runner 0u CHR 136,0 0t0 3 /dev/pts/0 bash 113 runner 1u CHR 136,0 0t0 3 /dev/pts/0 bash 113 runner 2u CHR 136,0 0t0 3 /dev/pts/0 bash 113 runner 255u CHR 136,0 0t0 3 /dev/pts/0 As you see above that the process had 0,1 and 2 FDs as well as FD for a directory as well.\nReference counting in OFT Reference counting is a technique used in operating systems to manage entries in the Open File Table (OFT), which stores information about open files shared by processes or file descriptors.\nHow It Works Each OFT entry has a reference count that tracks how many file descriptors (from one or more processes) refer to it. When a file is opened, the reference count increases by 1. If the same file is reused (e.g., via fork() or dup()), the count increments without creating a new entry. When a file descriptor is closed (e.g., with close(fd)), the count decreases by 1. The OFT entry is removed only when the reference count reaches 0, meaning no process is using it. How It Helps Remove Entries Reference counting ensures an OFT entry is deleted only when it’s no longer needed. For example, after a fork(), both parent and child processes share the same OFT entry (reference count = 2). Closing the file in one process lowers the count to 1, but the entry persists until the second process closes it, bringing the count to 0.\nWhy It’s Useful This method efficiently manages shared file resources, preventing premature removal of file metadata (like the current offset) while any process still needs it.\nINode What is an Inode? An inode (short for \u0026ldquo;index node\u0026rdquo;) is a fundamental data structure in UNIX-based file systems. It stores essential metadata about a file, enabling the system to manage and locate files efficiently. Each file in the system is uniquely identified by its inode number (also called i-number). The metadata stored in an inode includes:\nFile size: The total size of the file in bytes.\nPermissions: Access rights defining who can read, write, or execute the file.\nOwnership: User ID (UID) and group ID (GID) of the file\u0026rsquo;s owner.\nTimestamps:\nLast access time (when the file was last read). Last modification time (when the file\u0026rsquo;s content was last changed). Last status change time (when the file\u0026rsquo;s metadata, like permissions, was last modified). Pointers to data blocks: Locations on disk where the file\u0026rsquo;s actual content is stored.\nThe inode does not store the file\u0026rsquo;s name or its content; these are managed separately. The inode\u0026rsquo;s role is to provide a compact and efficient way to access a file\u0026rsquo;s metadata. Each inode is stored in inode block in disk.\nHow the stat System Call Works The stat system call is used to retrieve metadata about a file without accessing its actual content. It provides a way for programs to query information like file size, permissions, and timestamps. Here\u0026rsquo;s how it works:\nInput: The stat system call takes a file path as input. Locate the inode: The file system uses the file path to find the corresponding inode number. (Insert link to FS implemetation note here explaining how FS searches inode and data block from given file path) Retrieve metadata: The inode is fetched from disk (or from a cache, if available, for faster access). Populate the struct stat buffer: The metadata stored in the inode is copied into a struct stat buffer, which contains fields for file size, permissions, ownership, timestamps, and more. Return to the user: The struct stat buffer is returned to the calling program, providing all the metadata for the file. Because the stat system call only accesses the inode and not the file\u0026rsquo;s content, it is a fast and efficient operation. This separation of metadata (stored in the inode) and content (stored in data blocks) allows the system to quickly retrieve file information without unnecessary disk I/O.\nDirectories ls -al Output for Directories\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 total 152 drwxr-xr-x 1 runner runner 394 Feb 23 11:40 . drwxrwxrwx 1 runner runner 46 Feb 23 11:36 .. -rw-r--r-- 1 runner runner 174 Feb 15 11:31 .breakpoints drwxr-xr-x 1 runner runner 34 Feb 15 11:32 .cache drwxr-x--- 1 runner runner 260 Feb 15 11:31 .ccls-cache -rw-r--r-- 1 runner runner 1564 Dec 21 11:44 common_threads.h -rwxr-xr-x 1 runner runner 16128 Dec 21 11:54 deadlock -rw-r--r-- 1 runner runner 647 Dec 21 11:51 deadlock.c -rwxr-xr-x 1 runner runner 16160 Dec 21 11:57 deadlock-global -rw-r--r-- 1 runner runner 748 Dec 21 11:57 deadlock-global.c -rw-r--r-- 1 runner runner 6320 Feb 23 11:36 generated-icon.png -rw-r--r-- 1 runner runner 429 Mar 8 2024 .gitignore -rwxr-xr-x 1 runner runner 15984 Dec 21 11:51 main -rw-r--r-- 1 runner runner 415 Dec 21 11:51 main.c -rwxr-xr-x 1 runner runner 16816 Aug 16 2024 main-debug -rw-r--r-- 1 runner runner 411 Dec 12 2023 Makefile -rwxr-xr-x 1 runner runner 16136 Dec 14 11:04 mem -rw-r--r-- 1 runner runner 1437 Aug 16 2024 .replit -rw-r--r-- 1 runner runner 134 Feb 23 13:54 replit.nix -rwxr-xr-x 1 runner runner 15936 Dec 21 11:59 signal -rw-r--r-- 1 runner runner 329 Dec 21 11:59 signal.c -rw-r--r-- 1 runner runner 288 Feb 23 11:41 test.txt drwxr-xr-x 1 runner runner 42 Feb 15 11:32 wcat Size of a directory only means storage needed to store each directory entry which basically comprise entry name and its inode number along with some other metdata\nDirectory Data Block Contents The data blocks of a directory contain:\nDirectory entries: Mapping of file/subdirectory names to their inode numbers. Special entries: . (current directory) and .. (parent directory). Optimization for Small Directories For small directories, some file systems store directory entries directly in the inode itself, avoiding separate data blocks. This optimization saves space and speeds up access.\nReading Directory Entries with System Calls To programmatically read directory contents following sys calls are used:\nopendir(path): Opens the directory. readdir(dir): Reads one entry at a time (returns a struct dirent with name and inode number. closedir(dir): Closes the directory. Permission bits and their octal representation Format: ls -l displays permissions as rwxr-xr-x:\nFirst character: d (directory), - (file), or l (symlink). Next 9 bits: Three groups of rwx for owner, group, and others: r = read, w = write, x = execute (run for files, enter for directories). Converting a Full 9-Bit Permission to Numeric (Octal) Representation Example: rwxr-xr-x\nBreakdown: Split into three groups (owner, group, others): Owner: rwx. Group: r-x. Others: r-x. Step-by-Step Conversion\nOwner: rwx: r = 1, w = 1, x = 1 → Binary: 111. Decimal: 1×4 + 1×2 + 1×1 = 4 + 2 + 1 = 7. Group: r-x: r = 1, w = 0, x = 1 → Binary: 101. Decimal: 1×4 + 0×2 + 1×1 = 4 + 1 = 5. Others: r-x: r = 1, w = 0, x = 1 → Binary: 101. Decimal: 1×4 + 0×2 + 1×1 = 4 + 1 = 5. Final Octal Representation\nCombine the three digits: 7 5 5. Result: rwxr-xr-x = 755. Quick Recall: Split rwx into 3 groups, convert each to binary (1s/0s), sum (4, 2, 1), get octal (e.g., 755). Hard Links, Symbolic Links (Including Size), and File Deletion (Based on OSTEP PDF) Hard Links and unlink Definition: A hard link (PDF p. 18-19) is an extra directory entry pointing to the same inode, created with link(old, new).. When a directory reference its parent directory (with ..) it increases hard link count by 1. For a new empty directory its hard count is always 2 since it is referred by itself (with .) and also referred by its parent\u0026rsquo;s directory entry (with ..) With unlink: Removes a name-to-inode link, decrements the inode’s link count. When count hits 0 (and no processes use it), the inode and data blocks are freed from disk. Symbolic Links and Dangling References Definition: A symbolic (soft) link (PDF p. 20-21) is a distinct file storing a pathname to another file, created with ln -s old new. Size: Its size equals the length of the stored pathname (e.g., 4 bytes for \u0026ldquo;file\u0026rdquo;, 15 bytes for \u0026ldquo;alongerfilename\u0026rdquo;; PDF p. 21). How It Works: References the path, not the inode directly; accessing it resolves the path. Dangling Reference: If the target is deleted (e.g., unlink file), the symbolic link persists but points to nothing, causing errors (e.g., \u0026ldquo;No such file or directory\u0026rdquo;). Disk, Partition, Volume, File System Hierarchy, and Mounting Distinction and Hierarchy Disk: Physical storage device (e.g., hard drive, SSD) holding raw data blocks. Partition: A subdivided section of a disk (e.g., /dev/sda1), treated as a separate unit. Volume: A logical abstraction, often a partition or group of partitions, formatted with a file system (FS). File System: Software structure (e.g., ext3, AFS) organizing data into files and directories on a volume. Hierarchy: Disk → Partitions → Volumes → File Systems.\nMounting Process Creation: Use mkfs to format a partition with a file system (e.g., mkfs -t ext3 /dev/sda1 creates an empty FS). Mounting: The mount command (PDF p. 24) attaches a file system to the directory tree at a mount point (e.g., mount -t ext3 /dev/sda1 /home/users), making its contents accessible under that path. Multiple File Systems on One Machine A single machine can host multiple file systems by mounting them at different points in the tree (PDF p. 26). Example Output (from mount): 1 2 3 /dev/sda1 on / type ext3 (rw) # Root FS /dev/sda5 on /tmp type ext3 (rw) # Separate tmp FS AFS on /afs type afs (rw) # Distributed FS How: Each partition or volume gets its own FS type and mount point, unified under one tree (e.g., /). Quick Recall: Disk splits into partitions; volumes get FS; mount glues them into a tree; multiple FS coexist at different paths.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-intro.pdf https://grok.com/share/bGVnYWN5_47ab49d6-aa1d-4de1-9bbe-0d47332e12fe https://grok.com/share/bGVnYWN5_7db77c97-d33d-4543-9993-d5aa362c8b2b ","permalink":"http://localhost:1313/blogs/files-and-directories/","summary":"\u003ch1 id=\"files-and-directories\"\u003eFiles and directories\u003c/h1\u003e\n\u003cp\u003eFile systems virtualize persistent storage (e.g., hard drives, SSDs) into user-friendly files and directories, adding a third pillar to OS abstractions (processes for CPU, address spaces for memory).\u003c/p\u003e\n\u003ch2 id=\"file-paths-and-system-calls\"\u003eFile Paths and System Calls\u003c/h2\u003e\n\u003cp\u003eFiles are organized in a \u003cstrong\u003etree-like directory structure\u003c/strong\u003e, starting from the root (/). A file’s location is identified by its \u003cstrong\u003epathname\u003c/strong\u003e (e.g., /home/user/file.txt). To interact with files, processes use \u003cstrong\u003esystem calls\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eopen(path, flags)\u003c/strong\u003e: Opens a file and returns a \u003cstrong\u003efile descriptor\u003c/strong\u003e (fd).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eread(fd, buffer, size)\u003c/strong\u003e: Reads data from the file into a buffer using the fd.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ewrite(fd, buffer, size)\u003c/strong\u003e: Writes data to the file via the fd.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eclose(fd)\u003c/strong\u003e: Closes the file, freeing the fd.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"file-descriptors\"\u003eFile Descriptors\u003c/h2\u003e\n\u003cp\u003eA \u003cstrong\u003efile descriptor\u003c/strong\u003e is a small integer, unique to each process, that identifies an open file. When a process calls open(), the operating system assigns it the next available fd (e.g., 3, 4, etc.). Every process starts with three default fds:\u003c/p\u003e","title":"Files And Directories"},{"content":"RAID Disks Three axes on which disks are analysed\nCapacity - How much capacity is needed to store X bytes of data Reliability - How much fault-tolerant is the disk Performance - Read and write speeds (Sequential and random) To make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification) When we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database In a way this set of disks also have challenges similar to distributes databases.\nThere are different types of data arrangement in set of physical disks which results in different types/levels of RAID\nRAID Level 0 - Striping Disk 0 Disk 1 Disk 2 Disk 3 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Here each cell is a disk block which will be of fixed size (for example 4KB) A vertical column shows blocks stored by a single disk\nStriping: Writing out chunks (in multiple of disk block size) to each disk, one at a time so that we have the data spread uniformly across the disk. When read or write requests comes up to disk it comes in the form of stripe number (row number in above illustration) and based on RAID level disk controller knows which block it has to access and which disk to read it from.\nTradeoffs:\nThis level has no reliability as a disk failure always means loss of some data. This arrangement is good for capacity as we are utilizing all disks for storage. RAID Level 1 - Mirroring Disk 0 Disk 1 Disk 2 Disk 3 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 Copies of blocks made to two disks, tradeoff of reliability over capacity. Consistent update problem Imagine the write is issued to the RAID, and then the RAID decides that it must be written to two disks, disk 0 and disk 1. The RAID then issues the write to disk 0, but just before the RAID can issue the request to disk 1, a power loss (or system crash) occurs. In this unfortunate case, let us assume that the request to disk 0 completed (but clearly the request to disk 1 did not, as it was never issued). The result of this untimely power loss is that the two copies of the block are now inconsistent; the copy on disk 0 is the new version, and the copy on disk 1 is the old. What we would like to happen is for the state of both disks to change atomically, i.e., either both should end up as the new version or neither. The general way to solve this problem is to use a write-ahead log of some kind to first record what the RAID is about to do (i.e., update two disks with a certain piece of data) before doing it. By taking this approach, we can ensure that in the presence of a crash, the right thing will happen; by running a recovery procedure that replays all pending transactions to the RAID, we can ensure that no two mirrored copies (in the RAID-1 case) are out of sync. One last note: because logging to disk on every write is prohibitively expensive, most RAID hardware includes a small amount of non-volatile RAM (e.g., battery-backed) where it performs this type of logging. Thus, consistent update is provided without the high cost of logging to disk.\nTradeoffs:\nThis level can tolerate up to N/2 disk failures. This arrangement is good for reliability over cost of capacity being half Even though updates for a block needs to happen at two separate disks, The write would be parallel but still slower than updating a single disk (If we consider different seek and rotational time for both disks) RAID Level 4 - Parity Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 4 5 6 7 P1 8 9 10 11 P2 12 13 14 15 P3 N - 1 Disks follow striping with the Nth disk containing parity block for each strip row Concept: RAID 4 adds redundancy to a disk array using parity, which consumes less storage space than mirroring. How it works:\nA single parity block is added to each stripe of data blocks. The parity block stores redundant information calculated from the data blocks in its stripe. The XOR function is used to calculate parity. XOR returns 0 if there are an even number of 1s in the bits, and 1 if there are an odd number of 1s. The parity bit ensures that the number of 1s in any row, including the parity bit, is always even. Example of Parity Calculation: Imagine a RAID 4 system with 4-bit data blocks. Let\u0026rsquo;s say we have the following data in one stripe:\nBlock 0: 0010 Block 1: 1001 Block 2: 0110 Block 3: 1011 To calculate the parity block, we perform a bitwise XOR across the corresponding bits of each data block:\nBit 1 (from left): 0 XOR 1 XOR 0 XOR 1 = 0 Bit 2: 0 XOR 0 XOR 1 XOR 0 = 1 Bit 3: 1 XOR 0 XOR 1 XOR 1 = 1 Bit 4: 0 XOR 1 XOR 0 XOR 1 = 0 Therefore, the parity block would be 0110.\nData recovery:\nIf a data block is lost, the remaining blocks in the stripe, including the parity block, are read. The XOR function is applied to these blocks to reconstruct the missing data. For example, if Block 2 (0110) was lost, we would XOR Block 0, Block 1, Block 3, and the parity block: 0010 XOR 1001 XOR 1011 XOR 0110 = 0110. Performance:\nRAID 4 has a performance cost due to the overhead of parity calculation. Crucially, all write operations must update the parity disk. Write Bottleneck: If multiple random write requests comes for various blocks at the same time then they all will all require to update different parity blocks but all parity blocks are in one disk so multiple updates to different parity block will be done one after the other because all are in the same disk which eventually makes concurrent random write requests sequential in nature this is also known as small-write problem RAID Level 5 - Rotating Parity Instead of having one dedicated disk for parity blocks for each stripe, distribute the parity blocks in rotating manner to all disks for each stripe.\nDisk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 4 5 6 P1 7 8 9 P2 10 11 12 P3 13 14 15 Small-write: Concurrent random write requests to blocks of different blocks of different stripes can now be done parallelly since parity block for each stripe will be in different disk. It is still possible that blocks of different stripes need to update parity blocks which are lying in same disk (due to rotating nature of parity block) write requests to blocks of same stripes will still be sequential since parity block will be on same disk for all the blocks of same stripe. In summary: While RAID 5 significantly improves random write performance compared to RAID 4, it doesn\u0026rsquo;t completely eliminate the possibility of parity-related bottlenecks. The rotated parity distribution reduces the likelihood of contention, but it doesn\u0026rsquo;t guarantee that parity updates will always be fully parallel. The chance of multiple stripes\u0026rsquo; parity residing on the same disk is still there, leading to potential performance degradation.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf How Data recovery happens with parity drive in RAID: https://blogs.oracle.com/solaris/post/understanding-raid-5-recovery-with-elementary-school-math#:~:text=We%20need%20to%20read%20all%20date%20from%20other%20drives%20to%20recovery%20parity.\n","permalink":"http://localhost:1313/blogs/raid-redundant-array-of-inexpensive-disk/","summary":"\u003ch1 id=\"raid-disks\"\u003eRAID Disks\u003c/h1\u003e\n\u003cp\u003eThree axes on which disks are analysed\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCapacity - How much capacity is needed to store X bytes of data\u003c/li\u003e\n\u003cli\u003eReliability - How much fault-tolerant is the disk\u003c/li\u003e\n\u003cli\u003ePerformance - Read and write speeds (Sequential and random)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification)\nWhen we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database\nIn a way this set of disks also have challenges similar to distributes databases.\u003c/p\u003e","title":"RAID (Redundant array of inexpensive disk)"},{"content":"Segmented Page Table Page table can grow large for a 32-bit address space and 4 KB page size we will be using 20 bits for virtual page number resulting in 2^20 bytes (i.e. 4MB of page table) for a single page table and each process will have its own page table so it is possible that we will be storing ~100sMB for page table alone which is not good. For above page table with 4 bits for VPN (Virtual page number) we can see that only VPN 0,4,14 and 15 are valid i.e. pointing to a PFN (Physical Frame Number) other PTEs (Page table entry) are just taking up space which is not used. We can use segmentation here with base and bound registers for each page table to only store valid PTE in the table. This will again split the virtual address to also contain the segment bits to identify which segment the address belongs to (code, heap or stack). Instead of using Base Page Table Register to query page table we will now be using Base Page Table Register [Segment] to get page table physical address for a given segment.\nSN = (VirtualAddress \u0026amp; SEG_MASK) \u0026gt;\u0026gt; SN_SHIFT VPN = (VirtualAddress \u0026amp; VPN_MASK) \u0026gt;\u0026gt; VPN_SHIFT AddressOfPTE = Base[SN] + (VPN * sizeof(PTE)) ^3c2fde\nThis way we can place contents of a process\u0026rsquo;s page table at different locations in memory for different segments and avoiding storing of invalid PTEs.\nMultilevel Page Table Segmented page table still can suffer from space wastage if we have sparse usage of heap and can cause external fragmentation since now size of a page table segment can be different (multiple of PTE size) and finding free space for variable sized page table can be difficult.\nIdea of Multilevel page table is to group page table entries into size of a page and ignore those group of PTE where each entry is invalid Like in above figure PFN 202 and 203 contain all entries as invalid and with multilevel page table we do not require to store PTE inside such pages. Now we would have an indirection where we will first refer page directory and then the actual page of the page table to get physical frame number of a given virtual address. So in a way we now have page table for the actual page table called page directory Lets assume we have 14 bit address space with 4 byte PTE with 64 Byte page size. VPN will be of 8 bits, which means a linear page table will contain 256 PTE each of size 4 byte resulting in 1KB page table (4 * 256). A 1KB page table requires 16 pages of size of 32 bytes so we can group this page table intro 16 different segments which will be now addressed by page table directory. To address 16 segments we need 4 bits so now we will be using first 4 bits of virtual address for page directory index.\nYou can see the similarity with segmentation here with the only difference being we are now creating segments of page size, so everything will be in multiple of page sizes so we won\u0026rsquo;t be facing external fragmentation + We do not need to know the number of segments beforehand like in segmentation with code, heap and stack. Similar to how we were calculating AddressOfPTE [[#^3c2fde]] in linear page table now we will first calculate AddressOfPDE (Page Directory Entry) as\nPDEAddr = PageDirBase + (PDIndex * sizeof(PDE)). Now we are storing page directory base address in register\nOnce we get PDE Address from there we can get the page address of required PTE. Similar to concatenating page offset in linear page table now we will be concatenating rest of the bits after page directory index on virtual address (Page Table Index or Page offset of page-table\u0026rsquo;s page) with PDEAddress to get the physical address of the page table entry.\nPTEAddr = (PDE.PFN \u0026lt;\u0026lt; SHIFT) + (PTIndex * sizeof(PTE)) Once we get the PTE address we can concatenate the physical address inside the entry with the page offset in virtual address to finally get the physical address of the given virtual address.\nSummary As you can see the tradeoff here is indirection to save memory space. Indirection basically means more number of memory access. For a given virtual address now we need following memory accesses:\nAccess memory to fetch page directory address Access memory to fetch page table entry address from page directory index Finally access memory for given overall virtual address As you can see with a TLB miss [[2- Source Materials/Books/OSTEP/TLB#^3bbded]] now we have 2 extra memory access overhead but after this miss we will have cache hit for this virtual address next time, and we will translate virtual address directly into physical address without any memory access. Overall A bigger table such as linear page table means faster service in case of TLB miss (lesser memory access) and reducing page table with indirection means slower TLB miss service. References https://pages.cs.wisc.edu/~remzi/OSTEP/vm-smalltables.pdf\n","permalink":"http://localhost:1313/blogs/multilevel-page-table/","summary":"\u003ch1 id=\"segmented-page-table\"\u003eSegmented Page Table\u003c/h1\u003e\n\u003cp\u003ePage table can grow large for a 32-bit address space and 4 KB page size we will be using 20 bits for virtual page number resulting in 2^20 bytes (i.e. 4MB of page table) for a single page table and each process will have its own page table so it is possible that we will be storing ~100sMB for page table alone which is not good.\n\u003cimg alt=\"Pasted image 20241127093849.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241127093849.png\"\u003e\nFor above page table with 4 bits for VPN (Virtual page number) we can see that only VPN 0,4,14 and 15 are valid i.e. pointing to a PFN (Physical Frame Number) other PTEs (Page table entry) are just taking up space which is not used.\nWe can use segmentation here with base and bound registers for each page table to only store valid PTE in the table.\n\u003cimg alt=\"Pasted image 20241127094506.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241127094506.png\"\u003e\nThis will again split the virtual address to also contain the segment bits to identify which segment the address belongs to (code, heap or stack). Instead of using \u003cem\u003eBase Page Table Register\u003c/em\u003e to query page table we will now be using \u003cem\u003eBase Page Table Register [Segment]\u003c/em\u003e to get page table physical address for a given segment.\u003c/p\u003e","title":"Multilevel Page table"},{"content":"TLB Translation look-aside buffer is a CPU cache which is generally small but since it is closer to CPU a TLB hit results in address translation to happen in 1-5 CPU cycles.\nCPU Cycle Time taken by CPU to fully execute an instruction, while CPU frequency refers to the number of these cycles that occur per second\nA TLB hit means for given virtual address the physical frame number was found in the TLB cache. A TLB hit will benefit all the address that lie on the same page. In the above given image page size is 16 bytes, so 4 INT variables can be saved in a single page, so a TLB hit of VPN 07 will serve address translation for VPN = 07 + page of offset of 0, 4,8 and 12 byte. This type of caching is benefitted from spatial locality of data where a cache hit results in cache hits for surrounding data as well. If we cache data and other data points which are more probable to get accessed in the same time frame (like loop variables etc) then such caching is benefitted from Temporal locality.\nSoftware (OS) handled TLB miss ^3bbded\nWhen a TLB miss happens\nGenerate a trap Trap handler for this trap is OS code. This trap handler will find the translation of virtual address from page table stored in memory A privileged operation to update the TLB Return to trap with PC updated to same instruction which generated the trap. (Usually return to trap updates PC to next instruction address) Context Switch and TLB A naive approach is to flush the TLB (by setting valid bit of the page table entry to 0) so that the next yet to run process has a clean TLB, but this can slow things down since after every context switch new process will have few TLB misses, and we have high frequency of context switches then the performance may degrade. Another approach is to use same TLB for multiple processes with a Address space identifier (ASID, similar to PID but small) in the page table entry signifying the process to which a page table entry belongs Using cache replacement policies like LRU for page table entries References https://pages.cs.wisc.edu/~remzi/OSTEP/vm-tlbs.pdf\n","permalink":"http://localhost:1313/blogs/tlb/","summary":"\u003ch1 id=\"tlb\"\u003eTLB\u003c/h1\u003e\n\u003cp\u003eTranslation look-aside buffer is a CPU cache which is generally small but since it is closer to CPU a TLB hit results in address translation to happen in 1-5 CPU cycles.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCPU Cycle\nTime taken by CPU to fully execute an instruction, while CPU frequency refers to the number of these cycles that occur per second\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eA TLB hit means for given virtual address the physical frame number was found in the TLB cache. A TLB hit will benefit all the address that lie on the same page.\n\u003cimg alt=\"Pasted image 20241120223520.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241120223520.png\"\u003e\nIn the above given image page size is 16 bytes, so 4 INT variables can be saved in a single page, so a TLB hit of VPN 07 will serve address translation for VPN = 07 + page of offset of 0, 4,8 and 12 byte.\nThis type of caching is benefitted from \u003cem\u003e\u003cstrong\u003espatial locality\u003c/strong\u003e\u003c/em\u003e of data where a cache hit results in cache hits for surrounding data as well.\nIf we cache data and other data points which are more probable to get accessed in the same time frame (like loop variables etc) then such caching is benefitted from \u003cem\u003e\u003cstrong\u003eTemporal locality.\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e","title":"TLB"},{"content":"Page Tables Page table contains the translation information of virtual page number to physical frame number. For an address space of 32 bits and page size of 4 KB (i.e. memory of 2^32 is divided into segments of 4 KB where each segment is called a memory page) , The virtual address will be of size 32 bits of which 12 bits (2^12 = 4 KB) will be used as offset inside a single page whereas remaining 20 bits will be used as virtual page number\nExample Memory Access As it can be seen from the above memory access flow, translating and accessing a virtual page address to actual physical frame address requires 2 memory access which can be slow when the process assumed that it is making only one memory access to fetch data or instruction from memory.\nPage table size is directly proportional to address space size Page table size is inversely proportional to a page size References OSTEP\n","permalink":"http://localhost:1313/blogs/page-tables/","summary":"\u003ch1 id=\"page-tables\"\u003ePage Tables\u003c/h1\u003e\n\u003cp\u003ePage table contains the translation information of virtual page number to physical frame number.\nFor an address space of 32 bits and page size of 4 KB \u003cem\u003e(i.e. memory of 2^32 is divided into segments of 4 KB where each segment is called a memory page)\u003c/em\u003e , The virtual address will be of size 32 bits of which 12 bits (2^12 = 4 KB) will be used as offset inside a single page whereas remaining 20 bits will be used as virtual page number\u003c/p\u003e","title":"Page Tables"},{"content":"References 5.6 Problem Generally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:\nIf the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading If the operation is a write operation then there are again two possibilities: Optimistic Locking If the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it\u0026rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify ^ab3c53 Pessimistic Locking But if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking Pessimistic locking involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf\u0026rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff\nNow there is a glaring problem with pessimistic locking, even if the restructuring is limited to the leaf node and its direct parent or neighbours only then also we are taking a write latch on the root restricting potential read operations resulting in slow reads\n8.0 Solution Introduction to SX Lock (Write lock is called X lock - Exclusive lock | Read lock is called S lock - Shared lock)\n- SX LOCK does not conflict with S LOCK but does conflict with X LOCK. SX Locks also conflict with each other. - The purpose of an SX LOCK is to indicate the intention to modify the protected area, but the modification has not yet started. Therefore, the resource is still accessible, but once the modification begins, access will no longer be allowed. Since an intention to modify exists, no other modifications can occur, so it conflicts with X LOCKs.\nSX locks are kind of like X lock among themselves but are like S locks when used with S locks, Now with SX locks are held in following manner.\nREAD OPERATION: We take S lock as before on the root node, but we also take S locks on the internal nodes till the leaf nodes WRITE OPERATION: If the write operation is just going to modify the leaf page we still use Optimistic Locking [[#^ab3c53]] But if the write operation involves restructuring of the tree then instead of taking a write lock on the root as discussed in Pessimistic locking [[#^17a3ff]] We take SX lock on the root and same X locks on leaf node and its direct parent and neighbour, This still allows S locks to be taken in root node to allow other read operation which are not having the same path as the ongoing write operation But still prevents another SX lock on root node by some other write operation. So we can see that SX lock\u0026rsquo;s introduction helps in increasing the read throughput but still has the problem of global contention on write operation even if the writes are not going to happen in the ongoing another write operation. I think a root level latch is under the pessimistic guess of modification of write bubbling up to root which can conflict with another write operation even if not in the same path as bubbling up to the root will impact all the branches, butt the question is do all write operations bubble up to root and if not is it wise to take a root level of SX lock also to prevent other write operations. The answer lies in another type of lock mechanism called Latch Coupling or Latch Crabbing.\nReferences Inno DB B-Tree Latch Optimisation\n","permalink":"http://localhost:1313/blogs/b-tree-latch-optimisation/","summary":"\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\u003ch2 id=\"56-problem\"\u003e5.6 Problem\u003c/h2\u003e\n\u003cp\u003eGenerally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading\u003c/li\u003e\n\u003cli\u003eIf the operation is a write operation then there are again two possibilities:\n\u003cul\u003e\n\u003cli\u003e\n\u003ch3 id=\"optimistic-locking\"\u003eOptimistic Locking\u003c/h3\u003e\nIf the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it\u0026rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify\n\u003cimg alt=\"Pasted image 20241117123300.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241117123300.png\"\u003e ^ab3c53\u003c/li\u003e\n\u003cli\u003e\n\u003ch3 id=\"pessimistic-locking\"\u003ePessimistic Locking\u003c/h3\u003e\nBut if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking\n\u003cimg alt=\"Pasted image 20241117123407.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241117123407.png\"\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003ePessimistic locking\u003c/strong\u003e involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf\u0026rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff\u003c/p\u003e","title":"B-Tree Latch Optimisation"}]