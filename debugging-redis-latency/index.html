<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Debugging Redis Latency | Learning Loop</title><meta name=keywords content="Networking,Database,Observability"><meta name=description content="This article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.
Background
After introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as ioredis for interacting with our Redis-compatible database.
Once we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand why this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions – options ranged from open-source SDKs (OpenTelemetry Node.js SDK) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted Grafana Tempo instance to test the setup and feasibility. (So far, the setup is working great, and I&rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution."><meta name=author content><link rel=canonical href=https://harshrai654.github.io/blogs/debugging-redis-latency/><link crossorigin=anonymous href=/blogs/assets/css/stylesheet.4a2d4f6ce40ed4a68aae2b504ff6913b1f64bf1b352beea0c885d29c6d987670.css integrity="sha256-Si1PbOQO1KaKritQT/aROx9kvxs1K+6gyIXSnG2YdnA=" rel="preload stylesheet" as=style><link rel=icon href=https://harshrai654.github.io/blogs/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://harshrai654.github.io/blogs/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://harshrai654.github.io/blogs/favicon-32x32.png><link rel=apple-touch-icon href=https://harshrai654.github.io/blogs/apple-touch-icon.png><link rel=mask-icon href=https://harshrai654.github.io/blogs/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://harshrai654.github.io/blogs/debugging-redis-latency/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script data-goatcounter=https://ttharsh.goatcounter.com/count async src=//gc.zgo.at/count.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://harshrai654.github.io/blogs/ accesskey=h title="Learning Loop (Alt + H)">Learning Loop</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://harshrai654.github.io/blogs/>Home</a></div><h1 class="post-title entry-hint-parent">Debugging Redis Latency</h1><div class=post-meta><span title='2025-04-06 14:21:54 +0530 +0530'>April 6, 2025</span>&nbsp;·&nbsp;11 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#background aria-label=Background>Background</a></li><li><a href=#finding-the-culprit aria-label="Finding the culprit">Finding the culprit</a><ul><li><a href=#the-scan-command-and-frequent-use aria-label="The SCAN Command and Frequent Use">The SCAN Command and Frequent Use</a></li><li><a href=#the-restart-clue-and-initial-hypotheses aria-label="The Restart Clue and Initial Hypotheses">The Restart Clue and Initial Hypotheses</a></li><li><a href=#investigating-connection-pooling aria-label="Investigating Connection Pooling">Investigating Connection Pooling</a></li><li><a href=#investigating-tcp-connection-state aria-label="Investigating TCP Connection State">Investigating TCP Connection State</a></li><li><a href=#a-perfect-correlation-and-the-root-cause aria-label="A Perfect Correlation and the Root Cause">A Perfect Correlation and the Root Cause</a></li><li><a href=#the-fix-replacing-scan-with-sets aria-label="The Fix: Replacing SCAN with SETs">The Fix: Replacing SCAN with SETs</a></li><li><a href=#the-results-latency-tamed aria-label="The Results: Latency Tamed">The Results: Latency Tamed</a></li></ul></li><li><a href=#conclusion-trust-the-clues aria-label="Conclusion: Trust the Clues">Conclusion: Trust the Clues</a></li></ul></div></details></div><div class=post-content><p>This article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.</p><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>After introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as <code>ioredis</code> for interacting with our Redis-compatible database.
Once we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand <em>why</em> this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions – options ranged from open-source SDKs (<a href=https://opentelemetry.io/docs/languages/js/>OpenTelemetry Node.js SDK</a>) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted <a href=https://grafana.com/oss/tempo/>Grafana Tempo</a> instance to test the setup and feasibility. (So far, the setup is working great, and I&rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.</p><p><img alt="Pasted image 20250409091602.png" loading=lazy src=/blogs/media/pasted-image-20250409091602.png>
An example trace showing Redis commands executed while processing an HTTP request.</p><p>Okay, back to the problem. After setting up tracing, we could visually inspect the Redis command spans, like in the example above. Correlating these trace timings with our earlier metrics confirmed the high latency numbers. Indeed, something wasn&rsquo;t right with how our service was connecting to the cache server and executing commands.</p><h2 id=finding-the-culprit>Finding the culprit<a hidden class=anchor aria-hidden=true href=#finding-the-culprit>#</a></h2><p>Dragonfly is a Redis-compatible key-value database but with support for multithreading; Redis, on the other hand, follows a single-threaded, event-based model similar to Node.js.</p><p>Our first step was to check if anything was wrong with the cache server deployment itself. We enabled Dragonfly&rsquo;s slow query logs to check for commands taking longer than 100ms. Interestingly, we only saw <code>SCAN</code> commands in these logs. This didn&rsquo;t immediately make sense because our high latency metrics were observed for commands like <code>GET</code>, <code>SET</code>, <code>DELETE</code>, and <code>UNLINK</code>. These are typically O(1) commands and should not take more than a few milliseconds, so we ruled out the possibility of these specific commands taking significant time <em>to process</em> on the cache server itself.</p><p>To further monitor command execution time directly on the Dragonfly server, we enabled its Prometheus metrics exporter. We looked at two metrics: &ldquo;Pipeline Latency&rdquo; and &ldquo;Average Pipeline Length&rdquo;. The &ldquo;Average Pipeline Length&rdquo; was always close to 0, and the &ldquo;Pipeline Latency&rdquo; was consistently under 10ms. While there wasn&rsquo;t clear documentation from Dragonfly detailing these metrics precisely, going by the names, we assumed they represented the actual command execution time on the cache server.</p><p>So, the evidence suggested commands were executing quickly <em>on</em> the cache server (confirmed by both low Prometheus pipeline latency and the absence of <code>GET</code>/<code>SET</code> etc., in the slow query logs). But wait – the slow query logs <em>did</em> show the <code>SCAN</code> command with execution times in the range of 50ms to 200ms. So, what exactly is the <code>SCAN</code> command, and why were we using it?</p><h3 id=the-scan-command-and-frequent-use>The <code>SCAN</code> Command and Frequent Use<a hidden class=anchor aria-hidden=true href=#the-scan-command-and-frequent-use>#</a></h3><p>First, what is the <code>SCAN</code> command? <code>SCAN</code> is a cursor-based command in Redis used to iterate over the keyspace. It takes a cursor position and a glob pattern, matching the pattern against keys in the database without blocking the server for long periods (unlike its predecessor, <code>KEYS</code>).</p><p>In our system, we primarily use <code>SCAN</code> to invalidate cache entries for specific users. We publish cache invalidation events from various parts of our application. Depending on the event type, a process triggers that uses <code>SCAN</code> to find and delete cache keys matching a user-specific pattern. Since these invalidation events are very frequent in our workload, the <code>SCAN</code> command was executed much more often than we initially realized.</p><h3 id=the-restart-clue-and-initial-hypotheses>The Restart Clue and Initial Hypotheses<a hidden class=anchor aria-hidden=true href=#the-restart-clue-and-initial-hypotheses>#</a></h3><p>During our investigation, we stumbled upon a curious behavior: if we restarted the DragonflyDB instance, the high command latency would drop back to normal levels for a few hours before inevitably climbing back up to the problematic 200ms range. This provided a temporary, albeit disruptive, fix during peak hours (the cost being the loss of cached data, although we later mitigated this by enabling snapshotting for restores).</p><p>This temporary &ldquo;fix&rdquo; from restarting was a significant clue. It strongly suggested the problem wasn&rsquo;t necessarily the <code>SCAN</code> command&rsquo;s execution <em>on</em> the server (which slow logs and metrics already indicated was fast <em>most</em> of the time, except for <code>SCAN</code> itself sometimes), but perhaps something related to the <em>state</em> of the connection or interaction <em>between</em> our Node.js services and DragonflyDB over time.</p><p>This led us to two main hypotheses related to connection handling:</p><ol><li><strong>Connection Pooling:</strong> <code>ioredis</code>, the driver we were using, maintains a single connection to the Redis server. This is standard for single-threaded Redis, where multiple connections offer little benefit. However, DragonflyDB is multi-threaded. Could our single connection be a bottleneck when dealing with frequent commands, especially potentially long-running <code>SCAN</code> operations, under Dragonfly&rsquo;s multi-threaded architecture? Perhaps connection pooling would allow better parallel execution.</li><li><strong>Long-Running TCP Connections:</strong> Could the TCP connections themselves, after being open for extended periods, degrade in performance or enter a state that caused delays in sending commands or receiving responses?</li></ol><h3 id=investigating-connection-pooling>Investigating Connection Pooling<a hidden class=anchor aria-hidden=true href=#investigating-connection-pooling>#</a></h3><p>To test the connection pooling hypothesis, we considered adding a pooling library like <code>generic-pool</code> on top of <code>ioredis</code>. However, we noticed that <code>node-redis</code>, the official Redis client for Node.js, already included built-in connection pooling capabilities and had an API largely compatible with <code>ioredis</code>. So, as a direct way to test the effect of pooling, we replaced <code>ioredis</code> with <code>node-redis</code> in our service.</p><p>Unfortunately, even with <code>node-redis</code> and its connection pooling configured, the behavior remained the same: high latencies persisted, only dropping temporarily after a DragonflyDB restart. This seemed to rule out simple connection pooling as the solution.</p><h3 id=investigating-tcp-connection-state>Investigating TCP Connection State<a hidden class=anchor aria-hidden=true href=#investigating-tcp-connection-state>#</a></h3><p>With the pooling hypothesis proving unfruitful, we turned to the idea of issues with long-running TCP connections. We tried several approaches to detect problems here:</p><ul><li><strong>Code Profiling:</strong> We profiled the Node.js service during periods of high latency, generating flame graphs to see if significant time was being spent within the Redis driver&rsquo;s internal methods, specifically looking for delays in writing to or reading from the underlying network socket.</li><li><strong>Packet Tracing:</strong> We used <code>tcpdump</code> on the service instances to capture network traffic between the Node.js service and DragonflyDB, looking for signs of network-level latency, packet loss, or retransmissions that could explain the delays.</li></ul><p>Both of these efforts came up empty. The profiling data showed no unusual delays within the driver&rsquo;s socket operations, and the <code>tcpdump</code> analysis indicated normal network communication without significant latency.</p><p>We had confirmed the high frequency of <code>SCAN</code>, observed the strange restart behavior, and ruled out both simple connection pooling and obvious TCP-level network issues as the root cause. We needed a new hypothesis.</p><h3 id=a-perfect-correlation-and-the-root-cause>A Perfect Correlation and the Root Cause<a hidden class=anchor aria-hidden=true href=#a-perfect-correlation-and-the-root-cause>#</a></h3><p>We refocused on the most reliable clue: why did restarting the cache server temporarily fix the latency? We had ruled out connection management issues. The other major effect of a restart was clearing the in-memory key-value store <em>(remember, at this stage, we weren&rsquo;t restoring snapshots immediately after restarts)</em>. Plotting the number of keys in DragonflyDB over time confirmed our suspicion. We saw the key count drop to zero after each restart and then steadily climb until the next restart.
<img alt="Pasted image 20250410230134.png" loading=lazy src=/blogs/media/pasted-image-20250410230134.png>
Correlating this key count graph with our latency metrics revealed a clear pattern: as the number of keys rose, so did the p99 latency for our Redis commands. Although Redis/DragonflyDB can handle millions of keys, we started seeing significant latency increases once the key count grew into the 100,000–200,000 range in our specific setup. Now, which of our commands would be most affected by the <em>number</em> of keys? Looking at our common operations (<code>GET</code>, <code>SET</code>, <code>DEL</code>, <code>UNLINK</code>, <code>SCAN</code>, <code>EXISTS</code>), <code>SCAN</code> stood out. While most Redis commands have O(1) complexity, <code>SCAN</code>&rsquo;s performance is inherently tied to the number of keys it needs to iterate over. (More details on <code>SCAN</code>&rsquo;s behavior can be found in the <a href=https://redis.io/docs/latest/commands/scan/>official Redis documentation</a>). We were using <code>SCAN</code> extensively for cache invalidation, employing code similar to this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-js data-lang=js><span class=line><span class=ln>1</span><span class=cl><span class=kr>const</span> <span class=nx>keysToDelete</span> <span class=o>=</span> <span class=p>[];</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>
</span></span><span class=line><span class=ln>3</span><span class=cl><span class=k>for</span> <span class=kr>await</span> <span class=p>(</span><span class=kr>const</span> <span class=nx>key</span> <span class=k>of</span> <span class=k>this</span><span class=p>.</span><span class=nx>client</span><span class=p>.</span><span class=nx>scanIterator</span><span class=p>({</span>
</span></span><span class=line><span class=ln>4</span><span class=cl>	<span class=nx>MATCH</span><span class=o>:</span> <span class=nx>pattern</span><span class=p>,</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>	<span class=nx>COUNT</span><span class=o>:</span> <span class=nx>count</span><span class=p>,</span>
</span></span><span class=line><span class=ln>6</span><span class=cl><span class=p>}))</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>7</span><span class=cl>	<span class=nx>keysToDelete</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=nx>key</span><span class=p>);</span>
</span></span><span class=line><span class=ln>8</span><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Critically, for each cache invalidation event (which were frequent), we potentially ran multiple <code>SCAN</code> operations, and each <code>scanIterator</code> loop continued until the <em>entire relevant portion of the keyspace</em> was traversed to find all keys matching the pattern.</p><p>But how could <code>SCAN</code>, even if sometimes slow itself (as seen in the slow logs), cause delays for fast O(1) commands like <code>GET</code> or <code>SET</code>? Our server-side metrics (like Dragonfly&rsquo;s Pipeline Latency) showed quick <em>execution</em> times for those O(1) commands. This led to a new hypothesis: the server metrics likely measured only the actual CPU time for command execution, <em>not</em> the total turnaround time experienced by the client, which includes any <em>wait time</em> before the command gets processed.</p><p>Even though <code>SCAN</code> is non-blocking, issuing a large number of <code>SCAN</code> commands concurrently, especially when each needs to iterate over a growing keyspace (100k-200k+ keys), could potentially overwhelm the server&rsquo;s connection-handling threads (even Dragonfly&rsquo;s multiple threads). If threads were busy processing numerous, longer-running <code>SCAN</code> iterations, incoming <code>GET</code>, <code>SET</code>, etc., commands would have to wait longer before being picked up for execution, increasing their <em>total observed latency</em> from the client&rsquo;s perspective. The performance degradation of <code>SCAN</code> with more keys, combined with its high frequency, created a bottleneck that impacted all other operations.</p><h3 id=the-fix-replacing-scan-with-sets>The Fix: Replacing SCAN with SETs<a hidden class=anchor aria-hidden=true href=#the-fix-replacing-scan-with-sets>#</a></h3><p>Armed with this hypothesis, the fix became clear: we needed to drastically reduce or eliminate our reliance on <code>SCAN</code> for finding keys to invalidate.</p><p>We implemented an alternative approach:</p><ol><li>For each user (or entity needing cache invalidation), maintain a Redis SET containing the keys associated with that user.</li><li>When an invalidation event occurs for a user, instead of scanning the keyspace, retrieve the list of keys directly from the user&rsquo;s specific SET using the <code>SMEMBERS</code> command (which is much faster for this purpose).</li><li>Delete the keys obtained from the SET.</li></ol><p>This required some additional logic (housekeeping) to keep these SETs up-to-date as new keys were cached, but the performance benefits far outweighed the complexity.</p><p>So we opted for a more optimal way to invalidate cache where we also stored key names related to a user in a redis SET, so the use of SCAN was not moot because we do not need to scan the namespace every time to first prepare list of keys to delete now we can get that with just a <code>SMEMBERS</code> command which gives list of set elements. A little housekeeping was needed to maintain this set for each user, but it still outweighs the benefits.</p><h3 id=the-results-latency-tamed>The Results: Latency Tamed<a hidden class=anchor aria-hidden=true href=#the-results-latency-tamed>#</a></h3><p>This change dramatically solved the high latency issue.</p><p>First, the frequency of the <code>SCAN</code> command dropped to nearly zero, as expected.
<img alt="Pasted image 20250411171004.png" loading=lazy src=/blogs/media/pasted-image-20250411171004.png></p><p>Consequently, the latency spikes across <em>all</em> commands disappeared. The overall p99 latency stabilized at a much healthier level.
<img alt="Pasted image 20250411171153.png" loading=lazy src=/blogs/media/pasted-image-20250411171153.png></p><p>Interestingly, even the server-side execution time reported by Dragonfly showed improvement, suggesting the reduced load from <code>SCAN</code> allowed other commands to be processed more efficiently internally as well.
<img alt="Pasted image 20250411171413.png" loading=lazy src=/blogs/media/pasted-image-20250411171413.png></p><p>The final result was a significant drop in p99 latency, bringing it down from peaks often exceeding 200ms (and sometimes reaching ~500ms as shown below) to consistently around ~40ms.
<img alt="Pasted image 20250411171517.png" loading=lazy src=/blogs/media/pasted-image-20250411171517.png>
<img alt="Pasted image 20250411171527.png" loading=lazy src=/blogs/media/pasted-image-20250411171527.png>
p99 latency comparison showing reduction from peaks around ~500ms down to ~40ms after the fix</p><h2 id=conclusion-trust-the-clues>Conclusion: Trust the Clues<a hidden class=anchor aria-hidden=true href=#conclusion-trust-the-clues>#</a></h2><p>Looking back at how we tackled our high Redis command latency (~200ms+ p99), the journey involved ramping up the setup for observability, and exploring several potential culprits. While investigating connection pooling, profiling code execution, and even analyzing network packets with <code>tcpdump</code> were valuable exercises, they ultimately didn&rsquo;t lead us to the root cause in this case.</p><p>The most significant clue, in hindsight, was the temporary fix we got from restarting the DragonflyDB instance. If we had focused more intently from the start on <em>why</em> that restart helped – realizing it pointed directly to the state accumulated <em>within</em> the database (specifically, the key count) – and correlated that with our application&rsquo;s command usage patterns, we likely would have identified the high-frequency, full-keyspace <code>SCAN</code> operations as the bottleneck much sooner.</p><p>The real issue wasn&rsquo;t low-level network glitches or basic connection handling, but an application-level pattern: frequent <code>SCAN</code>s over a growing keyspace were overwhelming the server, increasing wait times for all commands. Switching our invalidation logic to use Redis SETs (<code>SMEMBERS</code>) eliminated this problematic pattern, finally bringing our p99 latency down to a stable ~40ms. Although optimizing the <code>SCAN</code> operation itself using [[https://redis.io/docs/latest/operate/oss_and_stack/reference/cluster-spec/#hash-tags]] was another interesting possibility (ensuring keys with the same tag land in the same hash slot to potentially limit scan scope), we didn&rsquo;t opt for this solution since this required a rethink of our cache key nomenclature and would have involved substantial changes.
Ultimately, the most direct path to a solution often lies in understanding the application&rsquo;s behavior and trusting the most obvious clues, rather than immediately reaching for the deepest diagnostic tools.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://harshrai654.github.io/blogs/tags/networking/>Networking</a></li><li><a href=https://harshrai654.github.io/blogs/tags/database/>Database</a></li><li><a href=https://harshrai654.github.io/blogs/tags/observability/>Observability</a></li></ul><nav class=paginav><a class=prev href=https://harshrai654.github.io/blogs/map-reduce/><span class=title>« Prev</span><br><span>Map Reduce</span>
</a><a class=next href=https://harshrai654.github.io/blogs/socket-file-descriptor-and-tcp-connections/><span class=title>Next »</span><br><span>Socket File Descriptor and TCP connections</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Debugging Redis Latency on x" href="https://x.com/intent/tweet/?text=Debugging%20Redis%20Latency&amp;url=https%3a%2f%2fharshrai654.github.io%2fblogs%2fdebugging-redis-latency%2f&amp;hashtags=Networking%2cDatabase%2cObservability"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Debugging Redis Latency on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fharshrai654.github.io%2fblogs%2fdebugging-redis-latency%2f&amp;title=Debugging%20Redis%20Latency&amp;summary=Debugging%20Redis%20Latency&amp;source=https%3a%2f%2fharshrai654.github.io%2fblogs%2fdebugging-redis-latency%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Debugging Redis Latency on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fharshrai654.github.io%2fblogs%2fdebugging-redis-latency%2f&title=Debugging%20Redis%20Latency"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Debugging Redis Latency on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fharshrai654.github.io%2fblogs%2fdebugging-redis-latency%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Debugging Redis Latency on whatsapp" href="https://api.whatsapp.com/send?text=Debugging%20Redis%20Latency%20-%20https%3a%2f%2fharshrai654.github.io%2fblogs%2fdebugging-redis-latency%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Debugging Redis Latency on telegram" href="https://telegram.me/share/url?text=Debugging%20Redis%20Latency&amp;url=https%3a%2f%2fharshrai654.github.io%2fblogs%2fdebugging-redis-latency%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Debugging Redis Latency on ycombinator" href="https://news.ycombinator.com/submitlink?t=Debugging%20Redis%20Latency&u=https%3a%2f%2fharshrai654.github.io%2fblogs%2fdebugging-redis-latency%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://harshrai654.github.io/blogs/>Learning Loop</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script type=module>
  import mermaid from "https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs";

  
  function isDarkMode() {
    return (
      document.body.classList.contains("dark") ||
      localStorage.getItem("pref-theme") === "dark" ||
      (!localStorage.getItem("pref-theme") &&
        window.matchMedia("(prefers-color-scheme: dark)").matches)
    );
  }

  
  mermaid.initialize({
    startOnLoad: true,
    theme: isDarkMode() ? "dark" : "default",
    themeVariables: {
      dark: {
        primaryColor: "#ff6b6b",
        primaryTextColor: "#ffffff",
        primaryBorderColor: "#ff6b6b",
        lineColor: "#ffffff",
        secondaryColor: "#4ecdc4",
        tertiaryColor: "#45b7d1",
        background: "#1a1a1a",
        mainBkg: "#2d2d2d",
        secondBkg: "#3d3d3d",
        tertiaryBkg: "#4d4d4d",
      },
      default: {
        primaryColor: "#ff6b6b",
        primaryTextColor: "#000000",
        primaryBorderColor: "#ff6b6b",
        lineColor: "#000000",
        secondaryColor: "#4ecdc4",
        tertiaryColor: "#45b7d1",
      },
    },
  });

  
  const themeToggle = document.getElementById("theme-toggle");
  if (themeToggle) {
    themeToggle.addEventListener("click", () => {
      setTimeout(() => {
        mermaid.initialize({
          startOnLoad: false,
          theme: isDarkMode() ? "dark" : "default",
          themeVariables: {
            dark: {
              primaryColor: "#ff6b6b",
              primaryTextColor: "#ffffff",
              primaryBorderColor: "#ff6b6b",
              lineColor: "#ffffff",
              secondaryColor: "#4ecdc4",
              tertiaryColor: "#45b7d1",
              background: "#1a1a1a",
              mainBkg: "#2d2d2d",
              secondBkg: "#3d3d3d",
              tertiaryBkg: "#4d4d4d",
            },
            default: {
              primaryColor: "#ff6b6b",
              primaryTextColor: "#000000",
              primaryBorderColor: "#ff6b6b",
              lineColor: "#000000",
              secondaryColor: "#4ecdc4",
              tertiaryColor: "#45b7d1",
            },
          },
        });

        
        document.querySelectorAll(".mermaid").forEach((element) => {
          element.removeAttribute("data-processed");
          mermaid.init(undefined, element);
        });
      }, 100);
    });
  }
</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>