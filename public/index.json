[{"content":"This article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.\nBackground After introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as ioredis for interacting with our Redis-compatible database. Once we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand why this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions – options ranged from open-source SDKs (OpenTelemetry Node.js SDK) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted Grafana Tempo instance to test the setup and feasibility. (So far, the setup is working great, and I\u0026rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.\nAn example trace showing Redis commands executed while processing an HTTP request.\nOkay, back to the problem. After setting up tracing, we could visually inspect the Redis command spans, like in the example above. Correlating these trace timings with our earlier metrics confirmed the high latency numbers. Indeed, something wasn\u0026rsquo;t right with how our service was connecting to the cache server and executing commands.\nFinding the culprit Dragonfly is a Redis-compatible key-value database but with support for multithreading; Redis, on the other hand, follows a single-threaded, event-based model similar to Node.js.\nOur first step was to check if anything was wrong with the cache server deployment itself. We enabled Dragonfly\u0026rsquo;s slow query logs to check for commands taking longer than 100ms. Interestingly, we only saw SCAN commands in these logs. This didn\u0026rsquo;t immediately make sense because our high latency metrics were observed for commands like GET, SET, DELETE, and UNLINK. These are typically O(1) commands and should not take more than a few milliseconds, so we ruled out the possibility of these specific commands taking significant time to process on the cache server itself.\nTo further monitor command execution time directly on the Dragonfly server, we enabled its Prometheus metrics exporter. We looked at two metrics: \u0026ldquo;Pipeline Latency\u0026rdquo; and \u0026ldquo;Average Pipeline Length\u0026rdquo;. The \u0026ldquo;Average Pipeline Length\u0026rdquo; was always close to 0, and the \u0026ldquo;Pipeline Latency\u0026rdquo; was consistently under 10ms. While there wasn\u0026rsquo;t clear documentation from Dragonfly detailing these metrics precisely, going by the names, we assumed they represented the actual command execution time on the cache server.\nSo, the evidence suggested commands were executing quickly on the cache server (confirmed by both low Prometheus pipeline latency and the absence of GET/SET etc., in the slow query logs). But wait – the slow query logs did show the SCAN command with execution times in the range of 50ms to 200ms. So, what exactly is the SCAN command, and why were we using it?\nThe SCAN Command and Frequent Use First, what is the SCAN command? SCAN is a cursor-based command in Redis used to iterate over the keyspace. It takes a cursor position and a glob pattern, matching the pattern against keys in the database without blocking the server for long periods (unlike its predecessor, KEYS).\nIn our system, we primarily use SCAN to invalidate cache entries for specific users. We publish cache invalidation events from various parts of our application. Depending on the event type, a process triggers that uses SCAN to find and delete cache keys matching a user-specific pattern. Since these invalidation events are very frequent in our workload, the SCAN command was executed much more often than we initially realized.\nThe Restart Clue and Initial Hypotheses During our investigation, we stumbled upon a curious behavior: if we restarted the DragonflyDB instance, the high command latency would drop back to normal levels for a few hours before inevitably climbing back up to the problematic 200ms range. This provided a temporary, albeit disruptive, fix during peak hours (the cost being the loss of cached data, although we later mitigated this by enabling snapshotting for restores).\nThis temporary \u0026ldquo;fix\u0026rdquo; from restarting was a significant clue. It strongly suggested the problem wasn\u0026rsquo;t necessarily the SCAN command\u0026rsquo;s execution on the server (which slow logs and metrics already indicated was fast most of the time, except for SCAN itself sometimes), but perhaps something related to the state of the connection or interaction between our Node.js services and DragonflyDB over time.\nThis led us to two main hypotheses related to connection handling:\nConnection Pooling: ioredis, the driver we were using, maintains a single connection to the Redis server. This is standard for single-threaded Redis, where multiple connections offer little benefit. However, DragonflyDB is multi-threaded. Could our single connection be a bottleneck when dealing with frequent commands, especially potentially long-running SCAN operations, under Dragonfly\u0026rsquo;s multi-threaded architecture? Perhaps connection pooling would allow better parallel execution. Long-Running TCP Connections: Could the TCP connections themselves, after being open for extended periods, degrade in performance or enter a state that caused delays in sending commands or receiving responses? Investigating Connection Pooling To test the connection pooling hypothesis, we considered adding a pooling library like generic-pool on top of ioredis. However, we noticed that node-redis, the official Redis client for Node.js, already included built-in connection pooling capabilities and had an API largely compatible with ioredis. So, as a direct way to test the effect of pooling, we replaced ioredis with node-redis in our service.\nUnfortunately, even with node-redis and its connection pooling configured, the behavior remained the same: high latencies persisted, only dropping temporarily after a DragonflyDB restart. This seemed to rule out simple connection pooling as the solution.\nInvestigating TCP Connection State With the pooling hypothesis proving unfruitful, we turned to the idea of issues with long-running TCP connections. We tried several approaches to detect problems here:\nCode Profiling: We profiled the Node.js service during periods of high latency, generating flame graphs to see if significant time was being spent within the Redis driver\u0026rsquo;s internal methods, specifically looking for delays in writing to or reading from the underlying network socket. Packet Tracing: We used tcpdump on the service instances to capture network traffic between the Node.js service and DragonflyDB, looking for signs of network-level latency, packet loss, or retransmissions that could explain the delays. Both of these efforts came up empty. The profiling data showed no unusual delays within the driver\u0026rsquo;s socket operations, and the tcpdump analysis indicated normal network communication without significant latency.\nWe had confirmed the high frequency of SCAN, observed the strange restart behavior, and ruled out both simple connection pooling and obvious TCP-level network issues as the root cause. We needed a new hypothesis.\nA Perfect Correlation and the Root Cause We refocused on the most reliable clue: why did restarting the cache server temporarily fix the latency? We had ruled out connection management issues. The other major effect of a restart was clearing the in-memory key-value store (remember, at this stage, we weren\u0026rsquo;t restoring snapshots immediately after restarts). Plotting the number of keys in DragonflyDB over time confirmed our suspicion. We saw the key count drop to zero after each restart and then steadily climb until the next restart. Correlating this key count graph with our latency metrics revealed a clear pattern: as the number of keys rose, so did the p99 latency for our Redis commands. Although Redis/DragonflyDB can handle millions of keys, we started seeing significant latency increases once the key count grew into the 100,000–200,000 range in our specific setup. Now, which of our commands would be most affected by the number of keys? Looking at our common operations (GET, SET, DEL, UNLINK, SCAN, EXISTS), SCAN stood out. While most Redis commands have O(1) complexity, SCAN\u0026rsquo;s performance is inherently tied to the number of keys it needs to iterate over. (More details on SCAN\u0026rsquo;s behavior can be found in the official Redis documentation). We were using SCAN extensively for cache invalidation, employing code similar to this:\n1 2 3 4 5 6 7 8 const keysToDelete = []; for await (const key of this.client.scanIterator({ MATCH: pattern, COUNT: count, })) { keysToDelete.push(key); } Critically, for each cache invalidation event (which were frequent), we potentially ran multiple SCAN operations, and each scanIterator loop continued until the entire relevant portion of the keyspace was traversed to find all keys matching the pattern.\nBut how could SCAN, even if sometimes slow itself (as seen in the slow logs), cause delays for fast O(1) commands like GET or SET? Our server-side metrics (like Dragonfly\u0026rsquo;s Pipeline Latency) showed quick execution times for those O(1) commands. This led to a new hypothesis: the server metrics likely measured only the actual CPU time for command execution, not the total turnaround time experienced by the client, which includes any wait time before the command gets processed.\nEven though SCAN is non-blocking, issuing a large number of SCAN commands concurrently, especially when each needs to iterate over a growing keyspace (100k-200k+ keys), could potentially overwhelm the server\u0026rsquo;s connection-handling threads (even Dragonfly\u0026rsquo;s multiple threads). If threads were busy processing numerous, longer-running SCAN iterations, incoming GET, SET, etc., commands would have to wait longer before being picked up for execution, increasing their total observed latency from the client\u0026rsquo;s perspective. The performance degradation of SCAN with more keys, combined with its high frequency, created a bottleneck that impacted all other operations.\nThe Fix: Replacing SCAN with SETs Armed with this hypothesis, the fix became clear: we needed to drastically reduce or eliminate our reliance on SCAN for finding keys to invalidate.\nWe implemented an alternative approach:\nFor each user (or entity needing cache invalidation), maintain a Redis SET containing the keys associated with that user. When an invalidation event occurs for a user, instead of scanning the keyspace, retrieve the list of keys directly from the user\u0026rsquo;s specific SET using the SMEMBERS command (which is much faster for this purpose). Delete the keys obtained from the SET. This required some additional logic (housekeeping) to keep these SETs up-to-date as new keys were cached, but the performance benefits far outweighed the complexity.\nSo we opted for a more optimal way to invalidate cache where we also stored key names related to a user in a redis SET, so the use of SCAN was not moot because we do not need to scan the namespace every time to first prepare list of keys to delete now we can get that with just a SMEMBERS command which gives list of set elements. A little housekeeping was needed to maintain this set for each user, but it still outweighs the benefits.\nThe Results: Latency Tamed This change dramatically solved the high latency issue.\nFirst, the frequency of the SCAN command dropped to nearly zero, as expected. Consequently, the latency spikes across all commands disappeared. The overall p99 latency stabilized at a much healthier level. Interestingly, even the server-side execution time reported by Dragonfly showed improvement, suggesting the reduced load from SCAN allowed other commands to be processed more efficiently internally as well. The final result was a significant drop in p99 latency, bringing it down from peaks often exceeding 200ms (and sometimes reaching ~500ms as shown below) to consistently around ~40ms. p99 latency comparison showing reduction from peaks around ~500ms down to ~40ms after the fix\nConclusion: Trust the Clues Looking back at how we tackled our high Redis command latency (~200ms+ p99), the journey involved ramping up the setup for observability, and exploring several potential culprits. While investigating connection pooling, profiling code execution, and even analyzing network packets with tcpdump were valuable exercises, they ultimately didn\u0026rsquo;t lead us to the root cause in this case.\nThe most significant clue, in hindsight, was the temporary fix we got from restarting the DragonflyDB instance. If we had focused more intently from the start on why that restart helped – realizing it pointed directly to the state accumulated within the database (specifically, the key count) – and correlated that with our application\u0026rsquo;s command usage patterns, we likely would have identified the high-frequency, full-keyspace SCAN operations as the bottleneck much sooner.\nThe real issue wasn\u0026rsquo;t low-level network glitches or basic connection handling, but an application-level pattern: frequent SCANs over a growing keyspace were overwhelming the server, increasing wait times for all commands. Switching our invalidation logic to use Redis SETs (SMEMBERS) eliminated this problematic pattern, finally bringing our p99 latency down to a stable ~40ms. Although optimizing the SCAN operation itself using [[https://redis.io/docs/latest/operate/oss_and_stack/reference/cluster-spec/#hash-tags]] was another interesting possibility (ensuring keys with the same tag land in the same hash slot to potentially limit scan scope), we didn\u0026rsquo;t opt for this solution since this required a rethink of our cache key nomenclature and would have involved substantial changes. Ultimately, the most direct path to a solution often lies in understanding the application\u0026rsquo;s behavior and trusting the most obvious clues, rather than immediately reaching for the deepest diagnostic tools.\n","permalink":"https://harshrai654.github.io/blogs/debugging-redis-latency/","summary":"\u003cp\u003eThis article is about how at work we solved the issue of high response time while executing Redis commands from Node.js server to a Redis compatible database known as dragonfly.\u003c/p\u003e\n\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eAfter introducing metrics to our Node.js service, we started recording the overall response time whenever a Redis command was executed. We had a wrapper service around a Redis driver known as \u003ccode\u003eioredis\u003c/code\u003e for interacting with our Redis-compatible database.\nOnce we set up Grafana dashboards for metrics like cache latency, we saw unusually high p99 latency numbers, close to 200ms. This is a very large number, especially considering the underlying database query itself typically takes less than 10ms to complete. To understand \u003cem\u003ewhy\u003c/em\u003e this latency was so high, we needed more detailed insight than metrics alone could provide. As part of a broader effort to set up our observability stack, I had been exploring various tracing solutions – options ranged from open-source SDKs (\u003ca href=\"https://opentelemetry.io/docs/languages/js/\"\u003eOpenTelemetry Node.js SDK\u003c/a\u003e) with a self-deployed trace backend, to third-party managed solutions (Datadog, Middleware, etc.). For this investigation, we decided to proceed with a self-hosted \u003ca href=\"https://grafana.com/oss/tempo/\"\u003eGrafana Tempo\u003c/a\u003e instance to test the setup and feasibility. (So far, the setup is working great, and I\u0026rsquo;m planning a detailed blog post on our observability architecture soon). With tracing set up, we could get a waterfall view of the path taken by the service while responding to things like HTTP requests or event processing, which we hoped would pinpoint the source of the delay in our Redis command execution.\u003c/p\u003e","title":"Debugging Redis Latency"},{"content":"Socket File Descriptors and Their Kernel Structures A socket is a special type of file descriptor (FD) in Linux, represented as socket:[inode]. Unlike regular file FDs, socket FDs point to in-memory kernel structures, not disk inodes. The /proc/\u0026lt;pid\u0026gt;/fd directory lists all FDs for a process, including sockets. The inode number of a socket can be used to inspect its details via tools like ss and /proc/net/tcp. Example: Checking Open FDs for Process 216 ls -l /proc/216/fd Output:\nlrwx------. 1 root root 64 Mar 2 09:01 0 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 1 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 2 -\u0026gt; /dev/pts/5 lrwx------. 1 root root 64 Mar 2 09:01 3 -\u0026gt; \u0026#39;socket:[35587]\u0026#39; Here, FD 3 is a socket pointing to inode 35587. Checking FD Details cat /proc/216/fdinfo/3 Output: pos: 0 flags: 02 mnt_id: 10 ino: 35587 How Data Flows Through a Socket (User Space to Kernel Space) When a process writes data to a socket, it is copied from user-space memory to kernel-space buffers (using syscall write()). The kernel then processes and forwards the data to the network interface card (NIC). This copying introduces overhead, which can be mitigated using zero-copy techniques like sendfile() and io_uring. (A tweet which might recall this) TCP 3-Way Handshake (How a Connection is Established) A TCP connection is established through a 3-way handshake between the client and server:\nClient → SYN (Initial sequence number) Server → SYN-ACK (Acknowledges client’s SYN, sends its own) Client → ACK (Acknowledges server’s SYN-ACK) Checking a Listening TCP Port ss -aep | grep 35587 Output:\ntcp LISTEN 0 0 0.0.0.0:41555 0.0.0.0:* users:((\u0026#34;nc\u0026#34;,pid=216,fd=3)) ino:35587 sk:53f53fa7 Port 41555 is in the LISTEN state, bound to nc (netcat). It corresponds to socket inode 35587. TCP Connection Queues in the Kernel Once a TCP connection request arrives, it goes through two queues managed by the kernel:\n1️] SYN Queue (Incomplete Connection Queue) Holds half-open connections (received SYN but not yet fully established). If this queue overflows, new SYN requests may be dropped (SYN flood attack risk). 2]Accept Queue (Backlog Queue, Fully Established Connections) Holds connections that have completed the handshake and are waiting for accept(). Controlled by listen(sockfd, backlog), where backlog defines max queue size If full, new connections are dropped. Checking Connection Queues ss -ltni Output:\nState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 5 0.0.0.0:8080 0.0.0.0:* Recv-Q (Accept Queue, Backlog Queue) → Number of connections waiting in the backlog. Send-Q (Not relevant here) → Usually for outbound data. Checking Kernel TCP Queues via **/proc/net/tcp** cat /proc/net/tcp Output:\nsl local_address rem_address st tx_queue rx_queue tr tm-\u0026gt;when retrnsmt uid timeout inode 0: 00000000:A253 00000000:0000 0A 00000000:00000000 00:00000000 00000000 0 0 35587 1 0000000053f53fa7 100 0 0 10 0 tx_queue → Data waiting to be sent. rx_queue → Data waiting to be read. The Role of the Kernel in TCP Connections The Linux kernel manages the entire TCP stack:\nHandshaking, sequencing, retransmissions, timeouts. Maintaining connection queues \u0026amp; buffering. Interacting with the NIC for packet transmission. Applications don’t deal with raw packets directly—they only read/write to sockets, while the kernel handles the rest.\nFlow Diagram: TCP Connection Journey with Kernel Involvement Client (User Space) Kernel (Server) Application (User Space) | | | | 1. SYN | | |---------------------------\u0026gt;| | | | | | 2. SYN-ACK | | |\u0026lt;---------------------------| | | | | | 3. ACK | | |---------------------------\u0026gt;| | | | Connection Added to SYN Queue | | |-----------------------------\u0026gt;| | | | | | Connection Moved to Accept Queue | | |-----------------------------\u0026gt;| | | | | | Application Calls `accept()` | | |-----------------------------\u0026gt;| | | | | | Data Transfer Begins | Why Each Connection Needs a Separate FD When a server listens on a port, it creates a listening socket FD. When a client initiates a connection: The kernel accepts the connection using the 3-way handshake. The kernel creates a new socket structure for this connection. The server application calls accept(), which returns a new FD. Why is a New FD Required? Each TCP connection requires its own state:\nSequence numbers (to track packets in order) Receive and send buffers Connection state (e.g., established, closed) Does the Communication Happen on the Same Port?\nYes, all connections still use the same local port (the port used for listening for connection on the server side). But, each accepted connection is a unique socket with a different remote IP/port pair. The kernel distinguishes connections by:\n(Local IP, Local Port) \u0026lt;\u0026ndash;\u0026gt; (Remote IP, Remote Port). Think of it like this:\nThe listening socket is like a front desk at a hotel. Every guest (client) gets their own room (new socket), but the front desk (listening socket) stays the same. Multiple Sockets on the Same Port (SO_REUSEPORT) Allows multiple FDs bound to the same port. Kernel load-balances connections across them. Used in: Nginx, HAProxy. Example: Multi-Threaded Server with SO_REUSEPORT int sock1 = socket(AF_INET, SOCK_STREAM, 0); int sock2 = socket(AF_INET, SOCK_STREAM, 0); setsockopt(sock1, SOL_SOCKET, SO_REUSEPORT, \u0026amp;opt, sizeof(opt)); setsockopt(sock2, SOL_SOCKET, SO_REUSEPORT, \u0026amp;opt, sizeof(opt)); bind(sock1, ...); bind(sock2, ...); listen(sock1, BACKLOG); listen(sock2, BACKLOG); References https://chatgpt.com/c/67c414f3-4830-8013-a058-0fd2596e3c07 ","permalink":"https://harshrai654.github.io/blogs/socket-file-descriptor-and-tcp-connections/","summary":"\u003ch2 id=\"socket-file-descriptors-and-their-kernel-structures\"\u003eSocket File Descriptors and Their Kernel Structures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003esocket\u003c/strong\u003e is a special type of file descriptor (FD) in Linux, represented as \u003ccode\u003esocket:[inode]\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eUnlike regular file FDs, socket FDs point to \u003cstrong\u003ein-memory kernel structures\u003c/strong\u003e, not disk inodes.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003e/proc/\u0026lt;pid\u0026gt;/fd\u003c/code\u003e directory lists all FDs for a process, including sockets.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003einode number\u003c/strong\u003e of a socket can be used to inspect its details via tools like \u003ccode\u003ess\u003c/code\u003e and \u003ccode\u003e/proc/net/tcp\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"example-checking-open-fds-for-process-216\"\u003eExample: Checking Open FDs for Process \u003ccode\u003e216\u003c/code\u003e\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003els -l /proc/216/fd\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eOutput:\u003c/strong\u003e\u003c/p\u003e","title":"Socket File Descriptor and TCP connections"},{"content":"Overall Organization Of Data In Disks Assuming we have a 256KB disk.\nDisk Blocks: The basic units of storage on the disk, each 4 KB in size. The disk is divided into these blocks, numbered from 0 to N-1 (where N is the total number of blocks). Inode Bitmap (i): Block 1; a bitmap tracking which inodes are free (0) or in-use (1). Data Bitmap (d): Block 2; a bitmap tracking which data blocks are free (0) or allocated (1). Inode Table (I): Blocks 3-7; an array of inodes, where each inode (256 bytes) holds metadata about a file, like size, permissions, and pointers to data blocks. 5 blocks of 4KB will contain 80 256 byte inode strutures. Data Region (D): Blocks 8-63; the largest section, storing the actual contents of files and directories. Inode Every inode has a unique identifier called an inode number (or i-number). This number acts like a file’s address in the file system, allowing the operating system to quickly locate its inode. For example:\nIn a system with 80 inodes, numbers might range from 0 to 79. Conventionally, the root directory is assigned inode number 2 (numbers 0 and 1 are often reserved or used for special purposes). The inode number is the key to finding a file’s metadata on the disk, and it’s stored in directory entries alongside the file’s name.\nHow Do We Jump to the Disk Block for a Specific Inode Number? In a file system like vsfs, inodes are stored consecutively in an inode table, a reserved area of the disk (e.g., spanning blocks 3 to 7). Each inode has a fixed size—let’s say 256 bytes—and the disk is divided into blocks (e.g., 4096 bytes each). To locate a specific inode given its i-number, we calculate its exact position on the disk. Here’s how:\nIdentify the Inode Table’s Starting Block: Suppose the inode table starts at block 3. Calculate the Block Containing the Inode: Formula: block = (i-number * inode_size) / block_size + start_block Example: For inode 10, inode_size = 256 bytes, block_size = 4096 bytes, start_block = 3 (10 * 256) / 4096 = 2560 / 4096 = 0.625 → integer part is 0. block = 0 + 3 = 3. So, inode 10 is in block 3. Calculate the Offset Within the Block: Formula: offset = (i-number * inode_size) % block_size Example: (10 * 256) % 4096 = 2560 % 4096 = 2560 bytes. Inode 10 starts 2560 bytes into block 3. Result: The operating system reads block 3 from the disk and jumps to offset 2560 bytes to access inode 10’s metadata. This process allows the file system to efficiently retrieve an inode’s information and, from there, its data blocks. Since multiple processes may have file descriptors for the same file opened with their own data of offset to read from, multiple processes will be accessing the same inode structure to read about file and modify its data (Here inodes data would mean modifying things like last accessed time or adding another entry for list of data blocks etc), So i-node needs some sort of concurrency control in place. (More on this)\nHow does inode know the data it owns An inode is a data structure in a file system that stores information about a file, including where its data is located on the disk. Instead of holding the file’s data itself, the inode contains pointers that reference the disk blocks where the data is stored.\nDisk Blocks: These are fixed-size chunks of storage on the disk (e.g., 4 KB each) that hold the actual file content. Pointers: These are entries in the inode that specify the locations of these disk blocks. The inode uses these pointers to keep track of all the blocks that make up a file, allowing the file system to retrieve the data when needed.\nWhat Are Disk Addresses? Disk addresses are the identifiers that tell the file system the exact physical locations of data blocks on the disk. Think of them as a map: each address corresponds to a specific block, such as block number 100, which might map to a particular sector and track on a hard drive.\nFor example, if a file is 8 KB and the block size is 4 KB, the inode might have two pointers with disk addresses like \u0026ldquo;block 50\u0026rdquo; and \u0026ldquo;block 51,\u0026rdquo; pointing to the two blocks that hold the file’s data. How Does an Inode Manage Disk Blocks? The inode organizes its pointers in a way that can handle files of different sizes efficiently. It uses a combination of direct pointers and indirect pointers, forming a multi-level indexing structure.\n1. Direct Pointers The inode starts with a set of direct pointers, which point straight to the disk addresses of data blocks. Example: If the block size is 4 KB and the inode has 12 direct pointers, it can directly address 12 × 4 KB = 48 KB of data. 2. Indirect Pointers (Multi-Level Indexing) For files too big for direct pointers alone, the inode uses indirect pointers, which point to special blocks that themselves contain more pointers. This creates a hierarchical, or multi-level, structure.\nSingle Indirect Pointer\nThis pointer points to a block (called an indirect block) that contains a list of disk addresses to data blocks.\nExample: If a block is 4 KB and each pointer is 4 bytes, the indirect block can hold 4 KB / 4 bytes = 1024 pointers. That’s 1024 × 4 KB = 4 MB of additional data.\nTotal with Direct: With 12 direct pointers and 1 single indirect pointer, the file can reach (12 + 1024) × 4 KB = 4,096 KB (about 4 MB).\nDouble Indirect Pointer\nThis pointer points to a block that contains pointers to other single indirect blocks, each of which points to data blocks.\nExample: The double indirect block might hold 1024 pointers to single indirect blocks. Each of those holds 1024 pointers to data blocks, so that’s 1024 × 1024 = 1,048,576 data blocks, or about 4 GB with 4 KB blocks.\nTotal with Direct and Single: (12 + 1024 + 1,048,576) × 4 KB ≈ 4 GB.\nThis structure acts like an imbalanced tree: small files use only direct pointers, while larger files use additional levels of indirection as needed.\nWhy Use Multi-Level Indexing? The multi-level indexing structure is designed to balance efficiency and scalability:\nSmall Files: Most files are small, so direct pointers handle them quickly without extra overhead. Large Files: Indirect pointers allow the inode to scale up to support massive files by adding more layers of pointers. How It Works in Practice When the file system needs to find a specific block in a file:\nIt checks the inode’s direct pointers first. If the block number is beyond the direct pointers’ range, it follows the single indirect pointer to the indirect block and looks up the address there. For even larger block numbers, it traverses the double indirect or triple indirect pointers, following the hierarchy until it finds the right disk address. This process ensures the file system can efficiently locate any block, no matter how big the file is.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-implementation.pdf https://grok.com/chat/d429de0e-556f-426c-84bd-21ff1b0c4002 (Contains reference to actual inode structure implementation along with locks it uses on Linux) https://grok.com/chat/c86e3040-2f86-4aa9-a317-1c0a464564a3?referrer=website ","permalink":"https://harshrai654.github.io/blogs/file-system-implementation/","summary":"\u003ch2 id=\"overall-organization-of-data-in-disks\"\u003eOverall Organization Of Data In Disks\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eAssuming we have a 256KB disk\u003c/em\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDisk Blocks\u003c/strong\u003e: The basic units of storage on the disk, \u003cem\u003eeach 4 KB in size.\u003c/em\u003e The disk is divided into these blocks, numbered from 0 to N-1 (where N is the total number of blocks).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInode Bitmap (i)\u003c/strong\u003e: Block 1; a bitmap tracking which inodes are free (0) or in-use (1).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Bitmap (d)\u003c/strong\u003e: Block 2; a bitmap tracking which data blocks are free (0) or allocated (1).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInode Table (I)\u003c/strong\u003e: Blocks 3-7; an array of inodes, where each inode (256 bytes) holds metadata about a file, like size, permissions, and pointers to data blocks.\n5 blocks of 4KB will contain 80 256 byte inode strutures.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Region (D)\u003c/strong\u003e: Blocks 8-63; the largest section, storing the actual contents of files and directories.\n\u003cimg alt=\"Pasted image 20250301204506.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20250301204506.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"inode\"\u003eInode\u003c/h2\u003e\n\u003cp\u003eEvery inode has a unique identifier called an \u003cstrong\u003einode number\u003c/strong\u003e (or \u003cstrong\u003ei-number\u003c/strong\u003e). This number acts like a file’s address in the file system, allowing the operating system to quickly locate its inode. For example:\u003c/p\u003e","title":"Understanding Inodes and Disk Layout"},{"content":"Files and directories File systems virtualize persistent storage (e.g., hard drives, SSDs) into user-friendly files and directories, adding a third pillar to OS abstractions (processes for CPU, address spaces for memory).\nFile Paths and System Calls Files are organized in a tree-like directory structure, starting from the root (/). A file’s location is identified by its pathname (e.g., /home/user/file.txt). To interact with files, processes use system calls:\nopen(path, flags): Opens a file and returns a file descriptor (fd). read(fd, buffer, size): Reads data from the file into a buffer using the fd. write(fd, buffer, size): Writes data to the file via the fd. close(fd): Closes the file, freeing the fd. File Descriptors A file descriptor is a small integer, unique to each process, that identifies an open file. When a process calls open(), the operating system assigns it the next available fd (e.g., 3, 4, etc.). Every process starts with three default fds:\n0: Standard input (stdin) 1: Standard output (stdout) 2: Standard error (stderr) Quick Note: File Descriptors \u0026amp; Terminals Every process starts with three standard file descriptors:\n0 (stdin), 1 (stdout), 2 (stderr). Where They Point By default they link to a terminal device (e.g., /dev/pts/0 for Terminal 1, /dev/pts/1 for Terminal 2). These are character devices with a major number (e.g., 136) for the tty driver and a unique minor number (e.g., 0 or 1) for each instance.\nCommand Output (Terminal 1):\n1 2 3 4 ls -l /proc/self/fd lrwx------ 1 runner runner 64 Feb 23 11:47 0 -\u0026gt; /dev/pts/0 lrwx------ 1 runner runner 64 Feb 23 11:47 1 -\u0026gt; /dev/pts/0 lrwx------ 1 runner runner 64 Feb 23 11:47 2 -\u0026gt; /dev/pts/0 Device Details:\n1 2 3 4 5 6 7 8 9 10 11 12 ls -l /dev/pts/0 crw--w---- 1 runner tty 136, 0 Feb 23 11:53 /dev/pts/0 ~/workspace$ stat /dev/pts/0 File: /dev/pts/0 Size: 0 Blocks: 0 IO Block: 1024 character special file Device: 0,1350 Inode: 3 Links: 1 Device type: 136,0 Access: (0620/crw--w----) Uid: ( 1000/ runner) Gid: ( 5/ tty) Access: 2025-02-23 12:13:52.419852946 +0000 Modify: 2025-02-23 12:13:52.419852946 +0000 Change: 2025-02-23 11:36:28.419852946 +0000 Birth: - Versus a File A regular file descriptor (e.g., for test.txt) points to a disk inode with data blocks, tied to a filesystem device (e.g., /dev/sda1), not a driver.\nExample:\n1 2 3 4 5 6 7 8 9 10 11 12 ls -li test.txt 12345 -rw-r--r-- 1 runner runner 5 Feb 23 12:00 test.txt ~/workspace$ stat test.txt File: test.txt Size: 288 Blocks: 8 IO Block: 4096 regular file Device: 0,375 Inode: 1111 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 1000/ runner) Gid: ( 1000/ runner) Access: 2025-02-23 11:40:57.014322027 +0000 Modify: 2025-02-23 11:41:41.800233516 +0000 Change: 2025-02-23 11:41:41.800233516 +0000 Birth: 2025-02-23 11:40:57.014322027 +0000 Fun Test Redirected Terminal 1’s output to Terminal 2:\nCommand: echo \u0026quot;Hello\u0026quot; \u0026gt; /dev/pts/1 Result: \u0026ldquo;Hello\u0026rdquo; appeared in Terminal 2 (/dev/pts/1, minor 1)! Open File Table The Open File Table (OFT) is a system-wide structure in kernel memory that tracks all open files. Each entry in the OFT includes:\nThe current offset (position for the next read/write). Permissions (e.g., read, write). A reference count (if multiple processes share the file). Each process has its own array of file descriptors, where each fd maps to an entry in the OFT. For example, process A’s fd 3 and process B’s fd 4 might point to the same OFT entry if they’ve opened the same file.\nExample: Showing entries of a PID in open file table\n1 2 3 4 5 6 7 8 ~/workspace$ lsof -p 113 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME bash 113 runner cwd DIR 0,375 394 256 /home/runner/workspace bash 113 runner rtd DIR 0,1324 4096 1562007 / bash 113 runner 0u CHR 136,0 0t0 3 /dev/pts/0 bash 113 runner 1u CHR 136,0 0t0 3 /dev/pts/0 bash 113 runner 2u CHR 136,0 0t0 3 /dev/pts/0 bash 113 runner 255u CHR 136,0 0t0 3 /dev/pts/0 As you see above that the process had 0,1 and 2 FDs as well as FD for a directory as well.\nReference counting in OFT Reference counting is a technique used in operating systems to manage entries in the Open File Table (OFT), which stores information about open files shared by processes or file descriptors.\nHow It Works Each OFT entry has a reference count that tracks how many file descriptors (from one or more processes) refer to it. When a file is opened, the reference count increases by 1. If the same file is reused (e.g., via fork() or dup()), the count increments without creating a new entry. When a file descriptor is closed (e.g., with close(fd)), the count decreases by 1. The OFT entry is removed only when the reference count reaches 0, meaning no process is using it. How It Helps Remove Entries Reference counting ensures an OFT entry is deleted only when it’s no longer needed. For example, after a fork(), both parent and child processes share the same OFT entry (reference count = 2). Closing the file in one process lowers the count to 1, but the entry persists until the second process closes it, bringing the count to 0.\nWhy It’s Useful This method efficiently manages shared file resources, preventing premature removal of file metadata (like the current offset) while any process still needs it.\nINode What is an Inode? An inode (short for \u0026ldquo;index node\u0026rdquo;) is a fundamental data structure in UNIX-based file systems. It stores essential metadata about a file, enabling the system to manage and locate files efficiently. Each file in the system is uniquely identified by its inode number (also called i-number). The metadata stored in an inode includes:\nFile size: The total size of the file in bytes.\nPermissions: Access rights defining who can read, write, or execute the file.\nOwnership: User ID (UID) and group ID (GID) of the file\u0026rsquo;s owner.\nTimestamps:\nLast access time (when the file was last read). Last modification time (when the file\u0026rsquo;s content was last changed). Last status change time (when the file\u0026rsquo;s metadata, like permissions, was last modified). Pointers to data blocks: Locations on disk where the file\u0026rsquo;s actual content is stored.\nThe inode does not store the file\u0026rsquo;s name or its content; these are managed separately. The inode\u0026rsquo;s role is to provide a compact and efficient way to access a file\u0026rsquo;s metadata. Each inode is stored in inode block in disk.\nHow the stat System Call Works The stat system call is used to retrieve metadata about a file without accessing its actual content. It provides a way for programs to query information like file size, permissions, and timestamps. Here\u0026rsquo;s how it works:\nInput: The stat system call takes a file path as input. Locate the inode: The file system uses the file path to find the corresponding inode number. (Insert link to FS implemetation note here explaining how FS searches inode and data block from given file path) Retrieve metadata: The inode is fetched from disk (or from a cache, if available, for faster access). Populate the struct stat buffer: The metadata stored in the inode is copied into a struct stat buffer, which contains fields for file size, permissions, ownership, timestamps, and more. Return to the user: The struct stat buffer is returned to the calling program, providing all the metadata for the file. Because the stat system call only accesses the inode and not the file\u0026rsquo;s content, it is a fast and efficient operation. This separation of metadata (stored in the inode) and content (stored in data blocks) allows the system to quickly retrieve file information without unnecessary disk I/O.\nDirectories ls -al Output for Directories\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 total 152 drwxr-xr-x 1 runner runner 394 Feb 23 11:40 . drwxrwxrwx 1 runner runner 46 Feb 23 11:36 .. -rw-r--r-- 1 runner runner 174 Feb 15 11:31 .breakpoints drwxr-xr-x 1 runner runner 34 Feb 15 11:32 .cache drwxr-x--- 1 runner runner 260 Feb 15 11:31 .ccls-cache -rw-r--r-- 1 runner runner 1564 Dec 21 11:44 common_threads.h -rwxr-xr-x 1 runner runner 16128 Dec 21 11:54 deadlock -rw-r--r-- 1 runner runner 647 Dec 21 11:51 deadlock.c -rwxr-xr-x 1 runner runner 16160 Dec 21 11:57 deadlock-global -rw-r--r-- 1 runner runner 748 Dec 21 11:57 deadlock-global.c -rw-r--r-- 1 runner runner 6320 Feb 23 11:36 generated-icon.png -rw-r--r-- 1 runner runner 429 Mar 8 2024 .gitignore -rwxr-xr-x 1 runner runner 15984 Dec 21 11:51 main -rw-r--r-- 1 runner runner 415 Dec 21 11:51 main.c -rwxr-xr-x 1 runner runner 16816 Aug 16 2024 main-debug -rw-r--r-- 1 runner runner 411 Dec 12 2023 Makefile -rwxr-xr-x 1 runner runner 16136 Dec 14 11:04 mem -rw-r--r-- 1 runner runner 1437 Aug 16 2024 .replit -rw-r--r-- 1 runner runner 134 Feb 23 13:54 replit.nix -rwxr-xr-x 1 runner runner 15936 Dec 21 11:59 signal -rw-r--r-- 1 runner runner 329 Dec 21 11:59 signal.c -rw-r--r-- 1 runner runner 288 Feb 23 11:41 test.txt drwxr-xr-x 1 runner runner 42 Feb 15 11:32 wcat Size of a directory only means storage needed to store each directory entry which basically comprise entry name and its inode number along with some other metdata\nDirectory Data Block Contents The data blocks of a directory contain:\nDirectory entries: Mapping of file/subdirectory names to their inode numbers. Special entries: . (current directory) and .. (parent directory). Optimization for Small Directories For small directories, some file systems store directory entries directly in the inode itself, avoiding separate data blocks. This optimization saves space and speeds up access.\nReading Directory Entries with System Calls To programmatically read directory contents following sys calls are used:\nopendir(path): Opens the directory. readdir(dir): Reads one entry at a time (returns a struct dirent with name and inode number. closedir(dir): Closes the directory. Permission bits and their octal representation Format: ls -l displays permissions as rwxr-xr-x:\nFirst character: d (directory), - (file), or l (symlink). Next 9 bits: Three groups of rwx for owner, group, and others: r = read, w = write, x = execute (run for files, enter for directories). Converting a Full 9-Bit Permission to Numeric (Octal) Representation Example: rwxr-xr-x\nBreakdown: Split into three groups (owner, group, others): Owner: rwx. Group: r-x. Others: r-x. Step-by-Step Conversion\nOwner: rwx: r = 1, w = 1, x = 1 → Binary: 111. Decimal: 1×4 + 1×2 + 1×1 = 4 + 2 + 1 = 7. Group: r-x: r = 1, w = 0, x = 1 → Binary: 101. Decimal: 1×4 + 0×2 + 1×1 = 4 + 1 = 5. Others: r-x: r = 1, w = 0, x = 1 → Binary: 101. Decimal: 1×4 + 0×2 + 1×1 = 4 + 1 = 5. Final Octal Representation\nCombine the three digits: 7 5 5. Result: rwxr-xr-x = 755. Quick Recall: Split rwx into 3 groups, convert each to binary (1s/0s), sum (4, 2, 1), get octal (e.g., 755). Hard Links, Symbolic Links (Including Size), and File Deletion (Based on OSTEP PDF) Hard Links and unlink Definition: A hard link (PDF p. 18-19) is an extra directory entry pointing to the same inode, created with link(old, new).. When a directory reference its parent directory (with ..) it increases hard link count by 1. For a new empty directory its hard count is always 2 since it is referred by itself (with .) and also referred by its parent\u0026rsquo;s directory entry (with ..) With unlink: Removes a name-to-inode link, decrements the inode’s link count. When count hits 0 (and no processes use it), the inode and data blocks are freed from disk. Symbolic Links and Dangling References Definition: A symbolic (soft) link (PDF p. 20-21) is a distinct file storing a pathname to another file, created with ln -s old new. Size: Its size equals the length of the stored pathname (e.g., 4 bytes for \u0026ldquo;file\u0026rdquo;, 15 bytes for \u0026ldquo;alongerfilename\u0026rdquo;; PDF p. 21). How It Works: References the path, not the inode directly; accessing it resolves the path. Dangling Reference: If the target is deleted (e.g., unlink file), the symbolic link persists but points to nothing, causing errors (e.g., \u0026ldquo;No such file or directory\u0026rdquo;). Disk, Partition, Volume, File System Hierarchy, and Mounting Distinction and Hierarchy Disk: Physical storage device (e.g., hard drive, SSD) holding raw data blocks. Partition: A subdivided section of a disk (e.g., /dev/sda1), treated as a separate unit. Volume: A logical abstraction, often a partition or group of partitions, formatted with a file system (FS). File System: Software structure (e.g., ext3, AFS) organizing data into files and directories on a volume. Hierarchy: Disk → Partitions → Volumes → File Systems.\nMounting Process Creation: Use mkfs to format a partition with a file system (e.g., mkfs -t ext3 /dev/sda1 creates an empty FS). Mounting: The mount command (PDF p. 24) attaches a file system to the directory tree at a mount point (e.g., mount -t ext3 /dev/sda1 /home/users), making its contents accessible under that path. Multiple File Systems on One Machine A single machine can host multiple file systems by mounting them at different points in the tree (PDF p. 26). Example Output (from mount): 1 2 3 /dev/sda1 on / type ext3 (rw) # Root FS /dev/sda5 on /tmp type ext3 (rw) # Separate tmp FS AFS on /afs type afs (rw) # Distributed FS How: Each partition or volume gets its own FS type and mount point, unified under one tree (e.g., /). Quick Recall: Disk splits into partitions; volumes get FS; mount glues them into a tree; multiple FS coexist at different paths.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-intro.pdf https://grok.com/share/bGVnYWN5_47ab49d6-aa1d-4de1-9bbe-0d47332e12fe https://grok.com/share/bGVnYWN5_7db77c97-d33d-4543-9993-d5aa362c8b2b ","permalink":"https://harshrai654.github.io/blogs/files-and-directories/","summary":"\u003ch1 id=\"files-and-directories\"\u003eFiles and directories\u003c/h1\u003e\n\u003cp\u003eFile systems virtualize persistent storage (e.g., hard drives, SSDs) into user-friendly files and directories, adding a third pillar to OS abstractions (processes for CPU, address spaces for memory).\u003c/p\u003e\n\u003ch2 id=\"file-paths-and-system-calls\"\u003eFile Paths and System Calls\u003c/h2\u003e\n\u003cp\u003eFiles are organized in a \u003cstrong\u003etree-like directory structure\u003c/strong\u003e, starting from the root (/). A file’s location is identified by its \u003cstrong\u003epathname\u003c/strong\u003e (e.g., /home/user/file.txt). To interact with files, processes use \u003cstrong\u003esystem calls\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eopen(path, flags)\u003c/strong\u003e: Opens a file and returns a \u003cstrong\u003efile descriptor\u003c/strong\u003e (fd).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eread(fd, buffer, size)\u003c/strong\u003e: Reads data from the file into a buffer using the fd.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ewrite(fd, buffer, size)\u003c/strong\u003e: Writes data to the file via the fd.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eclose(fd)\u003c/strong\u003e: Closes the file, freeing the fd.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"file-descriptors\"\u003eFile Descriptors\u003c/h2\u003e\n\u003cp\u003eA \u003cstrong\u003efile descriptor\u003c/strong\u003e is a small integer, unique to each process, that identifies an open file. When a process calls open(), the operating system assigns it the next available fd (e.g., 3, 4, etc.). Every process starts with three default fds:\u003c/p\u003e","title":"Files And Directories"},{"content":"RAID Disks Three axes on which disks are analysed\nCapacity - How much capacity is needed to store X bytes of data Reliability - How much fault-tolerant is the disk Performance - Read and write speeds (Sequential and random) To make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification) When we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database In a way this set of disks also have challenges similar to distributes databases.\nThere are different types of data arrangement in set of physical disks which results in different types/levels of RAID\nRAID Level 0 - Striping Disk 0 Disk 1 Disk 2 Disk 3 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Here each cell is a disk block which will be of fixed size (for example 4KB) A vertical column shows blocks stored by a single disk\nStriping: Writing out chunks (in multiple of disk block size) to each disk, one at a time so that we have the data spread uniformly across the disk. When read or write requests comes up to disk it comes in the form of stripe number (row number in above illustration) and based on RAID level disk controller knows which block it has to access and which disk to read it from.\nTradeoffs:\nThis level has no reliability as a disk failure always means loss of some data. This arrangement is good for capacity as we are utilizing all disks for storage. RAID Level 1 - Mirroring Disk 0 Disk 1 Disk 2 Disk 3 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 Copies of blocks made to two disks, tradeoff of reliability over capacity. Consistent update problem Imagine the write is issued to the RAID, and then the RAID decides that it must be written to two disks, disk 0 and disk 1. The RAID then issues the write to disk 0, but just before the RAID can issue the request to disk 1, a power loss (or system crash) occurs. In this unfortunate case, let us assume that the request to disk 0 completed (but clearly the request to disk 1 did not, as it was never issued). The result of this untimely power loss is that the two copies of the block are now inconsistent; the copy on disk 0 is the new version, and the copy on disk 1 is the old. What we would like to happen is for the state of both disks to change atomically, i.e., either both should end up as the new version or neither. The general way to solve this problem is to use a write-ahead log of some kind to first record what the RAID is about to do (i.e., update two disks with a certain piece of data) before doing it. By taking this approach, we can ensure that in the presence of a crash, the right thing will happen; by running a recovery procedure that replays all pending transactions to the RAID, we can ensure that no two mirrored copies (in the RAID-1 case) are out of sync. One last note: because logging to disk on every write is prohibitively expensive, most RAID hardware includes a small amount of non-volatile RAM (e.g., battery-backed) where it performs this type of logging. Thus, consistent update is provided without the high cost of logging to disk.\nTradeoffs:\nThis level can tolerate up to N/2 disk failures. This arrangement is good for reliability over cost of capacity being half Even though updates for a block needs to happen at two separate disks, The write would be parallel but still slower than updating a single disk (If we consider different seek and rotational time for both disks) RAID Level 4 - Parity Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 4 5 6 7 P1 8 9 10 11 P2 12 13 14 15 P3 N - 1 Disks follow striping with the Nth disk containing parity block for each strip row Concept: RAID 4 adds redundancy to a disk array using parity, which consumes less storage space than mirroring. How it works:\nA single parity block is added to each stripe of data blocks. The parity block stores redundant information calculated from the data blocks in its stripe. The XOR function is used to calculate parity. XOR returns 0 if there are an even number of 1s in the bits, and 1 if there are an odd number of 1s. The parity bit ensures that the number of 1s in any row, including the parity bit, is always even. Example of Parity Calculation: Imagine a RAID 4 system with 4-bit data blocks. Let\u0026rsquo;s say we have the following data in one stripe:\nBlock 0: 0010 Block 1: 1001 Block 2: 0110 Block 3: 1011 To calculate the parity block, we perform a bitwise XOR across the corresponding bits of each data block:\nBit 1 (from left): 0 XOR 1 XOR 0 XOR 1 = 0 Bit 2: 0 XOR 0 XOR 1 XOR 0 = 1 Bit 3: 1 XOR 0 XOR 1 XOR 1 = 1 Bit 4: 0 XOR 1 XOR 0 XOR 1 = 0 Therefore, the parity block would be 0110.\nData recovery:\nIf a data block is lost, the remaining blocks in the stripe, including the parity block, are read. The XOR function is applied to these blocks to reconstruct the missing data. For example, if Block 2 (0110) was lost, we would XOR Block 0, Block 1, Block 3, and the parity block: 0010 XOR 1001 XOR 1011 XOR 0110 = 0110. Performance:\nRAID 4 has a performance cost due to the overhead of parity calculation. Crucially, all write operations must update the parity disk. Write Bottleneck: If multiple random write requests comes for various blocks at the same time then they all will all require to update different parity blocks but all parity blocks are in one disk so multiple updates to different parity block will be done one after the other because all are in the same disk which eventually makes concurrent random write requests sequential in nature this is also known as small-write problem RAID Level 5 - Rotating Parity Instead of having one dedicated disk for parity blocks for each stripe, distribute the parity blocks in rotating manner to all disks for each stripe.\nDisk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 4 5 6 P1 7 8 9 P2 10 11 12 P3 13 14 15 Small-write: Concurrent random write requests to blocks of different blocks of different stripes can now be done parallelly since parity block for each stripe will be in different disk. It is still possible that blocks of different stripes need to update parity blocks which are lying in same disk (due to rotating nature of parity block) write requests to blocks of same stripes will still be sequential since parity block will be on same disk for all the blocks of same stripe. In summary: While RAID 5 significantly improves random write performance compared to RAID 4, it doesn\u0026rsquo;t completely eliminate the possibility of parity-related bottlenecks. The rotated parity distribution reduces the likelihood of contention, but it doesn\u0026rsquo;t guarantee that parity updates will always be fully parallel. The chance of multiple stripes\u0026rsquo; parity residing on the same disk is still there, leading to potential performance degradation.\nReferences https://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf How Data recovery happens with parity drive in RAID: https://blogs.oracle.com/solaris/post/understanding-raid-5-recovery-with-elementary-school-math#:~:text=We%20need%20to%20read%20all%20date%20from%20other%20drives%20to%20recovery%20parity.\n","permalink":"https://harshrai654.github.io/blogs/raid-redundant-array-of-inexpensive-disk/","summary":"\u003ch1 id=\"raid-disks\"\u003eRAID Disks\u003c/h1\u003e\n\u003cp\u003eThree axes on which disks are analysed\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCapacity - How much capacity is needed to store X bytes of data\u003c/li\u003e\n\u003cli\u003eReliability - How much fault-tolerant is the disk\u003c/li\u003e\n\u003cli\u003ePerformance - Read and write speeds (Sequential and random)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo make a logical disk (comprising set of physical disks) reliable we need replication, so there is tradeoff with capacity and performance (write amplification)\nWhen we talk about collection of physical disks representing one single logical disk we should know that there would be small compute and some non-volatile RAM also included to fully complete the disk controller component. This RAM is also used for WAL for faster writes similar to #Database\nIn a way this set of disks also have challenges similar to distributes databases.\u003c/p\u003e","title":"RAID (Redundant array of inexpensive disk)"},{"content":"Segmented Page Table Page table can grow large for a 32-bit address space and 4 KB page size we will be using 20 bits for virtual page number resulting in 2^20 bytes (i.e. 4MB of page table) for a single page table and each process will have its own page table so it is possible that we will be storing ~100sMB for page table alone which is not good. For above page table with 4 bits for VPN (Virtual page number) we can see that only VPN 0,4,14 and 15 are valid i.e. pointing to a PFN (Physical Frame Number) other PTEs (Page table entry) are just taking up space which is not used. We can use segmentation here with base and bound registers for each page table to only store valid PTE in the table. This will again split the virtual address to also contain the segment bits to identify which segment the address belongs to (code, heap or stack). Instead of using Base Page Table Register to query page table we will now be using Base Page Table Register [Segment] to get page table physical address for a given segment.\nSN = (VirtualAddress \u0026amp; SEG_MASK) \u0026gt;\u0026gt; SN_SHIFT VPN = (VirtualAddress \u0026amp; VPN_MASK) \u0026gt;\u0026gt; VPN_SHIFT AddressOfPTE = Base[SN] + (VPN * sizeof(PTE)) ^3c2fde\nThis way we can place contents of a process\u0026rsquo;s page table at different locations in memory for different segments and avoiding storing of invalid PTEs.\nMultilevel Page Table Segmented page table still can suffer from space wastage if we have sparse usage of heap and can cause external fragmentation since now size of a page table segment can be different (multiple of PTE size) and finding free space for variable sized page table can be difficult.\nIdea of Multilevel page table is to group page table entries into size of a page and ignore those group of PTE where each entry is invalid Like in above figure PFN 202 and 203 contain all entries as invalid and with multilevel page table we do not require to store PTE inside such pages. Now we would have an indirection where we will first refer page directory and then the actual page of the page table to get physical frame number of a given virtual address. So in a way we now have page table for the actual page table called page directory Lets assume we have 14 bit address space with 4 byte PTE with 64 Byte page size. VPN will be of 8 bits, which means a linear page table will contain 256 PTE each of size 4 byte resulting in 1KB page table (4 * 256). A 1KB page table requires 16 pages of size of 32 bytes so we can group this page table intro 16 different segments which will be now addressed by page table directory. To address 16 segments we need 4 bits so now we will be using first 4 bits of virtual address for page directory index.\nYou can see the similarity with segmentation here with the only difference being we are now creating segments of page size, so everything will be in multiple of page sizes so we won\u0026rsquo;t be facing external fragmentation + We do not need to know the number of segments beforehand like in segmentation with code, heap and stack. Similar to how we were calculating AddressOfPTE [[#^3c2fde]] in linear page table now we will first calculate AddressOfPDE (Page Directory Entry) as\nPDEAddr = PageDirBase + (PDIndex * sizeof(PDE)). Now we are storing page directory base address in register\nOnce we get PDE Address from there we can get the page address of required PTE. Similar to concatenating page offset in linear page table now we will be concatenating rest of the bits after page directory index on virtual address (Page Table Index or Page offset of page-table\u0026rsquo;s page) with PDEAddress to get the physical address of the page table entry.\nPTEAddr = (PDE.PFN \u0026lt;\u0026lt; SHIFT) + (PTIndex * sizeof(PTE)) Once we get the PTE address we can concatenate the physical address inside the entry with the page offset in virtual address to finally get the physical address of the given virtual address.\nSummary As you can see the tradeoff here is indirection to save memory space. Indirection basically means more number of memory access. For a given virtual address now we need following memory accesses:\nAccess memory to fetch page directory address Access memory to fetch page table entry address from page directory index Finally access memory for given overall virtual address As you can see with a TLB miss [[2- Source Materials/Books/OSTEP/TLB#^3bbded]] now we have 2 extra memory access overhead but after this miss we will have cache hit for this virtual address next time, and we will translate virtual address directly into physical address without any memory access. Overall A bigger table such as linear page table means faster service in case of TLB miss (lesser memory access) and reducing page table with indirection means slower TLB miss service. References https://pages.cs.wisc.edu/~remzi/OSTEP/vm-smalltables.pdf\n","permalink":"https://harshrai654.github.io/blogs/multilevel-page-table/","summary":"\u003ch1 id=\"segmented-page-table\"\u003eSegmented Page Table\u003c/h1\u003e\n\u003cp\u003ePage table can grow large for a 32-bit address space and 4 KB page size we will be using 20 bits for virtual page number resulting in 2^20 bytes (i.e. 4MB of page table) for a single page table and each process will have its own page table so it is possible that we will be storing ~100sMB for page table alone which is not good.\n\u003cimg alt=\"Pasted image 20241127093849.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241127093849.png\"\u003e\nFor above page table with 4 bits for VPN (Virtual page number) we can see that only VPN 0,4,14 and 15 are valid i.e. pointing to a PFN (Physical Frame Number) other PTEs (Page table entry) are just taking up space which is not used.\nWe can use segmentation here with base and bound registers for each page table to only store valid PTE in the table.\n\u003cimg alt=\"Pasted image 20241127094506.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241127094506.png\"\u003e\nThis will again split the virtual address to also contain the segment bits to identify which segment the address belongs to (code, heap or stack). Instead of using \u003cem\u003eBase Page Table Register\u003c/em\u003e to query page table we will now be using \u003cem\u003eBase Page Table Register [Segment]\u003c/em\u003e to get page table physical address for a given segment.\u003c/p\u003e","title":"Multilevel Page table"},{"content":"TLB Translation look-aside buffer is a CPU cache which is generally small but since it is closer to CPU a TLB hit results in address translation to happen in 1-5 CPU cycles.\nCPU Cycle Time taken by CPU to fully execute an instruction, while CPU frequency refers to the number of these cycles that occur per second\nA TLB hit means for given virtual address the physical frame number was found in the TLB cache. A TLB hit will benefit all the address that lie on the same page. In the above given image page size is 16 bytes, so 4 INT variables can be saved in a single page, so a TLB hit of VPN 07 will serve address translation for VPN = 07 + page of offset of 0, 4,8 and 12 byte. This type of caching is benefitted from spatial locality of data where a cache hit results in cache hits for surrounding data as well. If we cache data and other data points which are more probable to get accessed in the same time frame (like loop variables etc) then such caching is benefitted from Temporal locality.\nSoftware (OS) handled TLB miss ^3bbded\nWhen a TLB miss happens\nGenerate a trap Trap handler for this trap is OS code. This trap handler will find the translation of virtual address from page table stored in memory A privileged operation to update the TLB Return to trap with PC updated to same instruction which generated the trap. (Usually return to trap updates PC to next instruction address) Context Switch and TLB A naive approach is to flush the TLB (by setting valid bit of the page table entry to 0) so that the next yet to run process has a clean TLB, but this can slow things down since after every context switch new process will have few TLB misses, and we have high frequency of context switches then the performance may degrade. Another approach is to use same TLB for multiple processes with a Address space identifier (ASID, similar to PID but small) in the page table entry signifying the process to which a page table entry belongs Using cache replacement policies like LRU for page table entries References https://pages.cs.wisc.edu/~remzi/OSTEP/vm-tlbs.pdf\n","permalink":"https://harshrai654.github.io/blogs/tlb/","summary":"\u003ch1 id=\"tlb\"\u003eTLB\u003c/h1\u003e\n\u003cp\u003eTranslation look-aside buffer is a CPU cache which is generally small but since it is closer to CPU a TLB hit results in address translation to happen in 1-5 CPU cycles.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCPU Cycle\nTime taken by CPU to fully execute an instruction, while CPU frequency refers to the number of these cycles that occur per second\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eA TLB hit means for given virtual address the physical frame number was found in the TLB cache. A TLB hit will benefit all the address that lie on the same page.\n\u003cimg alt=\"Pasted image 20241120223520.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241120223520.png\"\u003e\nIn the above given image page size is 16 bytes, so 4 INT variables can be saved in a single page, so a TLB hit of VPN 07 will serve address translation for VPN = 07 + page of offset of 0, 4,8 and 12 byte.\nThis type of caching is benefitted from \u003cem\u003e\u003cstrong\u003espatial locality\u003c/strong\u003e\u003c/em\u003e of data where a cache hit results in cache hits for surrounding data as well.\nIf we cache data and other data points which are more probable to get accessed in the same time frame (like loop variables etc) then such caching is benefitted from \u003cem\u003e\u003cstrong\u003eTemporal locality.\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e","title":"TLB"},{"content":"Page Tables Page table contains the translation information of virtual page number to physical frame number. For an address space of 32 bits and page size of 4 KB (i.e. memory of 2^32 is divided into segments of 4 KB where each segment is called a memory page) , The virtual address will be of size 32 bits of which 12 bits (2^12 = 4 KB) will be used as offset inside a single page whereas remaining 20 bits will be used as virtual page number\nExample Memory Access As it can be seen from the above memory access flow, translating and accessing a virtual page address to actual physical frame address requires 2 memory access which can be slow when the process assumed that it is making only one memory access to fetch data or instruction from memory.\nPage table size is directly proportional to address space size Page table size is inversely proportional to a page size References OSTEP\n","permalink":"https://harshrai654.github.io/blogs/page-tables/","summary":"\u003ch1 id=\"page-tables\"\u003ePage Tables\u003c/h1\u003e\n\u003cp\u003ePage table contains the translation information of virtual page number to physical frame number.\nFor an address space of 32 bits and page size of 4 KB \u003cem\u003e(i.e. memory of 2^32 is divided into segments of 4 KB where each segment is called a memory page)\u003c/em\u003e , The virtual address will be of size 32 bits of which 12 bits (2^12 = 4 KB) will be used as offset inside a single page whereas remaining 20 bits will be used as virtual page number\u003c/p\u003e","title":"Page Tables"},{"content":"References 5.6 Problem Generally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:\nIf the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading If the operation is a write operation then there are again two possibilities: Optimistic Locking If the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it\u0026rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify ^ab3c53 Pessimistic Locking But if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking Pessimistic locking involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf\u0026rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff\nNow there is a glaring problem with pessimistic locking, even if the restructuring is limited to the leaf node and its direct parent or neighbours only then also we are taking a write latch on the root restricting potential read operations resulting in slow reads\n8.0 Solution Introduction to SX Lock (Write lock is called X lock - Exclusive lock | Read lock is called S lock - Shared lock)\n- SX LOCK does not conflict with S LOCK but does conflict with X LOCK. SX Locks also conflict with each other. - The purpose of an SX LOCK is to indicate the intention to modify the protected area, but the modification has not yet started. Therefore, the resource is still accessible, but once the modification begins, access will no longer be allowed. Since an intention to modify exists, no other modifications can occur, so it conflicts with X LOCKs.\nSX locks are kind of like X lock among themselves but are like S locks when used with S locks, Now with SX locks are held in following manner.\nREAD OPERATION: We take S lock as before on the root node, but we also take S locks on the internal nodes till the leaf nodes WRITE OPERATION: If the write operation is just going to modify the leaf page we still use Optimistic Locking [[#^ab3c53]] But if the write operation involves restructuring of the tree then instead of taking a write lock on the root as discussed in Pessimistic locking [[#^17a3ff]] We take SX lock on the root and same X locks on leaf node and its direct parent and neighbour, This still allows S locks to be taken in root node to allow other read operation which are not having the same path as the ongoing write operation But still prevents another SX lock on root node by some other write operation. So we can see that SX lock\u0026rsquo;s introduction helps in increasing the read throughput but still has the problem of global contention on write operation even if the writes are not going to happen in the ongoing another write operation. I think a root level latch is under the pessimistic guess of modification of write bubbling up to root which can conflict with another write operation even if not in the same path as bubbling up to the root will impact all the branches, butt the question is do all write operations bubble up to root and if not is it wise to take a root level of SX lock also to prevent other write operations. The answer lies in another type of lock mechanism called Latch Coupling or Latch Crabbing.\nReferences Inno DB B-Tree Latch Optimisation\n","permalink":"https://harshrai654.github.io/blogs/b-tree-latch-optimisation/","summary":"\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\u003ch2 id=\"56-problem\"\u003e5.6 Problem\u003c/h2\u003e\n\u003cp\u003eGenerally when traversing the index made up of btree we have to take latch on it. In MySQL 5.6 the approach of taking latch depends on the possible operation we are doing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf the operation is a read operation then taking a read lock is sufficient to prevent any writes to happen to the pages we are accessing in Btree while reading\u003c/li\u003e\n\u003cli\u003eIf the operation is a write operation then there are again two possibilities:\n\u003cul\u003e\n\u003cli\u003e\n\u003ch3 id=\"optimistic-locking\"\u003eOptimistic Locking\u003c/h3\u003e\nIf the write is limited to modifying the leaf page only without modifying the structure of the tree (Merging OR Splitting) then it\u0026rsquo;s an optimistic locking approach where we take read latch on root of the tree and write latch only on the leaf node to modify\n\u003cimg alt=\"Pasted image 20241117123300.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241117123300.png\"\u003e ^ab3c53\u003c/li\u003e\n\u003cli\u003e\n\u003ch3 id=\"pessimistic-locking\"\u003ePessimistic Locking\u003c/h3\u003e\nBut if the operation result is in any type of restructuring of the tree itself then that will be known to us only after reaching the target leaf node and knowing its neighbours and parents. So the approach is first to try with optimistic locking defined above and then go for pessimistic locking\n\u003cimg alt=\"Pasted image 20241117123407.png\" loading=\"lazy\" src=\"/blogs/media/pasted-image-20241117123407.png\"\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003ePessimistic locking\u003c/strong\u003e involves taking a write latch on the root resulting in full ownership of the tree by the current operation (until the operation is complete no other operation can take a read or write latch, so all the other operations has to wait even if they are read operations and involve only optimistic locking). When the leaf node is found we take write latch on the leaf\u0026rsquo;s neighbours as well as its parent and do the restructuring and if the same restructuring needs to happen at parent level then we will take similar write locks recursively up the tree. ^17a3ff\u003c/p\u003e","title":"B-Tree Latch Optimisation"}]